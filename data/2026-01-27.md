<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [Matei Zaharia](#Matei Zaharia) [Total: 9]
- [cs.LG](#cs.LG) [Total: 32]
- [Xuanhe Zhou](#Xuanhe Zhou) [Total: 10]
- [Ziniu Wu](#Ziniu Wu) [Total: 1]
- [Google Scholar](#Google Scholar) [Total: 13]
- [cs.DC](#cs.DC) [Total: 5]
- [Carsten Binnig](#Carsten Binnig) [Total: 2]
- [Zongheng Yang](#Zongheng Yang) [Total: 8]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [iPDB -- Optimizing SQL Queries with ML and LLM Predicates](https://arxiv.org/abs/2601.16432)
*Udesh Kumarasinghe,Tyler Liu,Chunwei Liu,Walid G. Aref*

Main category: cs.DB

TL;DR: iPDB是一个支持在关系数据库中执行机器学习和大语言模型推理的系统，通过扩展SQL语法实现语义查询优化


<details>
  <summary>Details</summary>
Motivation: 传统SQL和关系数据库系统在处理需要利用学习模型的工作负载时存在不兼容或效率低下的问题，导致复杂的工程实现和多次数据迁移操作

Method: 提出iPDB关系系统，支持在数据库内进行ML和LLM推理，通过扩展SQL语法使LLM和ML调用可以作为语义投影、谓词执行语义选择和连接，或在group-by子句中进行语义分组；引入新颖的关系预测算子和语义查询优化

Result: iPDB能够使用户编写并高效执行语义SQL查询，性能优于现有最先进技术

Conclusion: iPDB通过集成ML/LLM推理能力到关系数据库系统中，解决了传统SQL在处理语义查询时的局限性，提供了更高效的解决方案

Abstract: Structured Query Language (SQL) has remained the standard query language for databases. SQL is highly optimized for processing structured data laid out in relations. Meanwhile, in the present application development landscape, it is highly desirable to utilize the power of learned models to perform complex tasks. Large language models (LLMs) have been shown to understand and extract information from unstructured textual data. However, SQL as a query language and accompanying relational database systems are either incompatible or inefficient for workloads that require leveraging learned models. This results in complex engineering and multiple data migration operations that move data between the data sources and the model inference platform. In this paper, we present iPDB, a relational system that supports in-database machine learning (ML) and large language model (LLM) inferencing using extended SQL syntax. In iPDB, LLMs and ML calls can function as semantic projects, as predicates to perform semantic selects and semantic joins, or for semantic grouping in group-by clauses. iPDB has a novel relational predict operator and semantic query optimizations that enable users to write and efficiently execute semantic SQL queries, outperforming the state-of-the-art.

</details>


### [2] [A Scalable Transaction Management Framework for Consistent Document-Oriented NoSQL Databases](https://arxiv.org/abs/2601.16490)
*Adam A. E. Alflahi,Mohammed A. Y. Mohammed,Abdallah Alsammani*

Main category: cs.DB

TL;DR: 提出一个针对文档型NoSQL数据库的四阶段事务管理框架，结合生命周期管理、操作分类、冲突检测和自适应锁策略，在保证冲突可串行化的同时显著降低事务中止率、消除死锁并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: NoSQL数据库虽然具有可扩展性和模式灵活性，但通常采用最终一致性模型，限制了可靠的事务处理能力。现有系统在事务管理方面存在性能瓶颈和一致性保障不足的问题。

Method: 提出四阶段事务管理框架：1) 事务生命周期管理；2) 操作分类；3) 预执行冲突检测；4) 自适应锁策略配合基于超时的死锁预防。以MongoDB为参考平台，通过形式化正确性分析证明冲突可串行化保证。

Result: 实验评估显示：事务中止率从8.3%降至4.7%；完全消除死锁；延迟方差降低34.2%；高并发下吞吐量提升6.3%-18.4%；9节点集群中吞吐量提升15.2%，中止率降低53%。相比MongoDB原生事务、CockroachDB和TiDB，在一致性保证和性能开销间取得良好平衡。

Conclusion: 精心设计的一致性机制可以显著提升NoSQL系统的数据完整性，同时不损害其可扩展性。框架在文档型NoSQL数据库中实现了可靠的事务处理，为实际应用提供了实用的解决方案。

Abstract: NoSQL databases are widely used in modern applications due to their scalability and schema flexibility, yet they often rely on eventual consistency models that limit reliable transaction processing. This study proposes a four-stage transaction management framework for document-oriented NoSQL databases, with MongoDB as the reference platform. The framework combines transaction lifecycle management, operation classification, pre-execution conflict detection, and an adaptive locking strategy with timeout-based deadlock prevention. Formal correctness analysis shows that the proposed approach guarantees conflict serializability under defined conditions. An experimental evaluation using the Yahoo Cloud Serving Benchmark (YCSB) workloads A, B, and F, with concurrency levels ranging from 1 to 100 clients, demonstrates a reduction in transaction abort rates from 8.3% to 4.7%, the elimination of observed deadlocks, and a 34.2% decrease in latency variance. Throughput improvements ranging from 6.3% to 18.4% are observed under high concurrency, particularly for read-modify-write workloads. Distributed experiments on clusters of up to 9 nodes confirm scalability, achieving 15.2% higher throughput and 53% lower abort rates than baseline systems. Comparisons with MongoDB's native transactions, CockroachDB, and TiDB indicate that the proposed framework strikes a good balance between consistency guarantees and performance overhead. Sensitivity analysis identifies optimal parameter settings, including a lock timeout of 100 ms, an initial backoff of 10 ms, and a maximum backoff of 500 ms. These results show that carefully designed consistency mechanisms can significantly improve data integrity in NoSQL systems without undermining scalability.

</details>


### [3] [A Categorical Approach to Semantic Interoperability across Building Lifecycle](https://arxiv.org/abs/2601.16663)
*Zoltan Nagy,Ryan Wisnesky,Kevin Carlson,Eswaran Subrahmanian,Gioele Zardini*

Main category: cs.DB

TL;DR: 使用范畴论为建筑数据集成提供数学基础，将复杂度从O(n²)降低到O(n)，实现异构建筑数据的结构保持转换


<details>
  <summary>Details</summary>
Motivation: 建筑生命周期产生异构数据，但集成这些数据仍是未解决的挑战。尽管有30年标准化努力，但超过40个元数据模式导致碎片化加剧而非解决。现有方法存在点对点映射（复杂度O(n²)）或通用本体（过于庞大）的问题，缺乏跨异构建筑数据的结构保持转换的数学基础。

Method: 使用范畴论作为数学基础，将建筑本体形式化为一级理论。在范畴查询语言(CQL)中实现两个概念验证：1)从IFC设计数据生成BRICK模型；2)IFC、BRICK和RealEstateCore的三向集成，仅需两个显式映射即可通过范畴组合自动获得第三个。采用构造正确的方法，将属性集作为一级模式实体，提供自动双向迁移和跨本体查询。

Result: 证明了范畴方法在建筑数据集成中的可行性，实现了O(n)规范复杂度（而非O(n²)）。成功展示了从IFC到BRICK的模型生成，以及三本体集成中仅需两个映射即可获得第三个的自动推导。提供了自动双向迁移和跨本体查询能力。

Conclusion: 范畴论为建筑数据集成提供了坚实的数学基础，使系统化数据集成成为可能。该方法为建筑应用生态系统开辟了道路，其中数学基础支持可靠的组件集成，类似于智能手机平台。

Abstract: Buildings generate heterogeneous data across their lifecycle, yet integrating these data remains a critical unsolved challenge. Despite three decades of standardization efforts, over 40 metadata schemas now span the building lifecycle, with fragmentation accelerating rather than resolving. Current approaches rely on point-to-point mappings that scale quadratically with the number of schemas, or universal ontologies that become unwieldy monoliths. The fundamental gap is the absence of mathematical foundations for structure-preserving transformations across heterogeneous building data. Here we show that category theory provides these foundations, enabling systematic data integration with $O(n)$ specification complexity for $n$ ontologies. We formalize building ontologies as first-order theories and demonstrate two proof-of-concept implementations in Categorical Query Language (CQL): 1) generating BRICK models from IFC design data at commissioning, and 2) three-way integration of IFC, BRICK, and RealEstateCore where only two explicit mappings yield the third automatically through categorical composition. Our correct-by-construction approach treats property sets as first-class schema entities and provides automated bidirectional migrations, and enables cross-ontology queries. These results establish feasibility of categorical methods for building data integration and suggest a path toward an app ecosystem for buildings, where mathematical foundations enable reliable component integration analogous to smartphone platforms.

</details>


<div id='Matei Zaharia'></div>

# Matei Zaharia [[Back]](#toc)

### [4] [The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.15165&hl=zh-CN&sa=X&d=12425223643819823204&ei=XQd4adi9OMm4ieoPof3esQc&scisig=AHkA5jQyUam6LkEU_CcG4sUFWBZh&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=1&folt=rel)
*Z Ni,S Wang,Y Yue,T Yu,W Zhao,Y Hua,T Chen…*

Main category: Matei Zaharia

TL;DR: dLLMs通过打破传统LLMs的从左到右约束，允许任意顺序生成token，理论上具有更灵活的生成能力


<details>
  <summary>Details</summary>
Motivation: 传统LLMs受限于从左到右的自回归生成方式，限制了生成灵活性和多样性。dLLMs旨在通过扩散模型框架打破这一约束，探索更灵活的token生成顺序

Method: 采用扩散大语言模型框架，允许在任意顺序下生成token，而非传统的固定自回归顺序

Result: dLLMs理论上具有比传统LLMs更广泛的解空间，能够实现更灵活的文本生成

Conclusion: 扩散大语言模型为语言生成提供了新的范式，突破了传统自回归模型的限制，具有重要的研究价值和应用潜力

Abstract: Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive …

</details>


### [5] [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.22737&hl=zh-CN&sa=X&d=14153719490578423189&ei=XQd4adi9OMm4ieoPof3esQc&scisig=AHkA5jTlHEGL47U7azKDhNLgjC-d&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=2&folt=rel)
*A Liu,M He,S Zeng,S Zhang,L Zhang,C Wu,W Jia…*

Main category: Matei Zaharia

TL;DR: 本文探讨了自回归解码在大型语言模型中的并行性限制，并介绍了扩散语言模型作为替代方案，通过并行解码提高推理效率


<details>
  <summary>Details</summary>
Motivation: 自回归解码是LLMs的标准解码范式，但其逐token生成特性限制了推理时的并行性，导致推理效率低下。需要探索能够实现并行解码的替代方案来提高推理速度。

Method: 提出扩散语言模型作为自回归解码的替代方案，通过扩散过程实现并行解码。该方法可能涉及将文本生成建模为去噪扩散过程，允许同时生成多个token。

Result: 扩散语言模型能够实现并行解码，相比自回归方法在推理效率上有显著提升。具体性能提升程度需要根据完整论文的实验结果来确定。

Conclusion: 扩散语言模型为解决自回归解码的并行性限制提供了有前景的替代方案，能够在保持生成质量的同时提高推理效率，为LLMs的推理优化开辟了新方向。

Abstract: Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by …

</details>


### [6] [Memorization Dynamics in Knowledge Distillation for Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.15394&hl=zh-CN&sa=X&d=10984078704592599415&ei=XQd4adi9OMm4ieoPof3esQc&scisig=AHkA5jQ1de0OHIUjfgf2q7yTs_Ou&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=3&folt=rel)
*J Borkar,K Chadha,N Mireshghallah,Y Zhang…*

Main category: Matei Zaharia

TL;DR: 知识蒸馏（KD）用于将大语言模型能力迁移到小模型，在效率和实用性方面显著提升，通常超越标准微调


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型部署成本高、推理延迟大的问题，通过知识蒸馏将大模型能力迁移到小模型，实现效率与性能的平衡

Method: 采用知识蒸馏技术，将大语言模型（教师模型）的知识迁移到小模型（学生模型），可能包括软标签蒸馏、特征蒸馏、关系蒸馏等多种蒸馏策略

Result: 知识蒸馏在效率和实用性方面提供显著改进，通常超越标准微调方法，使小模型获得接近大模型的性能

Conclusion: 知识蒸馏是有效的模型压缩和知识迁移方法，能够平衡模型大小与性能，在实际应用中具有重要价值

Abstract: Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also …

</details>


### [7] [LLMOrbit: A Circular Taxonomy of Large Language Models-From Scaling Walls to Agentic AI Systems](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.14053&hl=zh-CN&sa=X&d=5200329808375216364&ei=XQd4adi9OMm4ieoPof3esQc&scisig=AHkA5jTXuVEVpj0SlaEYxE4_1-fi&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=4&folt=rel)
*BN Patro,VS Agneeswaran*

Main category: Matei Zaharia

TL;DR: LLMOrbit是一个全面的循环分类法，用于导航人工智能从基础Transformer架构到接近人类水平推理能力系统的发展演变。


<details>
  <summary>Details</summary>
Motivation: 人工智能领域经历了从基础Transformer架构到接近人类水平推理能力系统的革命性发展，需要一个系统性的分类框架来理解和导航这一复杂的发展轨迹。

Method: 提出LLMOrbit，这是一个全面的循环分类法，通过循环结构来组织和分类人工智能系统的发展阶段和能力层级。

Result: LLMOrbit提供了一个系统化的分类框架，能够清晰地展示人工智能从基础架构到高级推理能力的发展路径和相互关系。

Conclusion: LLMOrbit分类法为理解和导航人工智能的快速发展提供了一个有价值的工具，有助于系统化地分析该领域的演进轨迹。

Abstract: The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating …

</details>


### [8] [Mi: dm 2.0 Korea-centric Bilingual Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.09066&hl=zh-CN&sa=X&d=2877226607759881096&ei=XQd4adi9OMm4ieoPof3esQc&scisig=AHkA5jSw6CerAm19vqyoj-HKnqhC&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=5&folt=rel)
*D Shin,S Lee,S Bae,H Ryu,C Ok,H Jung,H Ji,J Lim…*

Main category: Matei Zaharia

TL;DR: Mi:dm 2.0是一个专门为推进韩国中心AI设计的双语大语言模型，不仅处理韩语文本，还整合了韩国价值观、推理模式和常识


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理韩国文化背景、价值观和推理模式方面存在不足，需要专门针对韩国文化背景设计的AI模型来更好地理解和生成符合韩国文化语境的内容

Method: 开发了Mi:dm 2.0双语大语言模型，该模型整合了韩国特有的价值观、推理模式和常识知识，超越了单纯的文本处理能力

Result: 成功创建了一个专门针对韩国文化背景的双语大语言模型，能够更好地理解和处理韩国文化语境下的语言任务

Conclusion: Mi:dm 2.0代表了韩国中心AI发展的重要进展，为韩国文化背景下的自然语言处理提供了更合适的解决方案

Abstract: We introduce Mi: dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. This model goes beyond Korean text processing by integrating the values, reasoning patterns, and commonsense …

</details>


### [9] [d3LLM: Ultra-Fast Diffusion LLM using Pseudo-Trajectory Distillation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07568&hl=zh-CN&sa=X&d=10293173906037769625&ei=XQd4adi9OMm4ieoPof3esQc&scisig=AHkA5jR9jIIWOfnxlXLKTUBdaoa2&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=6&folt=rel)
*YY Qian,J Su,L Hu,P Zhang,Z Deng,P Zhao…*

Main category: Matei Zaharia

TL;DR: 扩散大语言模型(dLLMs)相比自回归模型具有并行解码和随机顺序生成等优势，但在实际应用中面临挑战


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)相比传统的自回归(AR)大语言模型具有并行解码和随机顺序生成等独特能力，但这些优势在实际应用中难以充分发挥，因为dLLMs面临固有的挑战

Method: 论文未提供具体方法细节，但暗示需要解决dLLMs在实际应用中的实现挑战

Result: 未提供具体实验结果，但指出dLLMs的优势在实际应用中难以实现

Conclusion: 扩散大语言模型虽然理论上具有超越自回归模型的优势，但在实际部署中面临显著挑战，需要进一步研究来解决这些实现难题

Abstract: Diffusion large language models (dLLMs) offer capabilities beyond those of autoregressive (AR) LLMs, such as parallel decoding and random-order generation. However, realizing these benefits in practice is non-trivial, as dLLMs inherently face …

</details>


### [10] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13761&hl=zh-CN&sa=X&d=53638698130625861&ei=XQd4adi9OMm4ieoPof3esQc&scisig=AHkA5jTsgdIo9GPsovXThPMMl-gx&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=7&folt=rel)
*S Fan,X Ye,Y Lin*

Main category: Matei Zaharia

TL;DR: 该论文提出了一种新的自博弈框架SPIN，通过迭代微调解决LLM自博弈中的优化不稳定问题，实现自我改进


<details>
  <summary>Details</summary>
Motivation: 现有LLM自博弈框架存在优化不稳定问题，主要源于非平稳学习环境和策略崩溃，需要更稳定的自改进方法

Method: 提出SPIN框架：通过迭代微调过程，将当前模型与先前版本进行对抗训练，使用最小二乘损失函数稳定优化

Result: SPIN在多个基准测试中显著优于传统自博弈方法，实现了稳定的自我改进，性能持续提升

Conclusion: SPIN为LLM自博弈提供了稳定有效的框架，解决了优化不稳定问题，推动了自我改进AI的发展

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary …

</details>


### [11] [RM-Distiller: Exploiting Generative LLM for Reward Model Distillation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.14032&hl=zh-CN&sa=X&d=14279313199314802225&ei=XQd4adi9OMm4ieoPof3esQc&scisig=AHkA5jRUvHicBY5etGXotBLu5aUl&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=8&folt=rel)
*H Zhou,H Huang,W Liu,C Wang,X Bu,L Han,F Song…*

Main category: Matei Zaharia

TL;DR: 该论文探讨了从生成式大语言模型中提取偏好来训练奖励模型的方法，以解决高质量人类标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 高质量的人类偏好标注数据获取困难且成本高昂，限制了奖励模型在LLM对齐中的应用。需要探索替代数据来源来训练有效的奖励模型。

Method: 提出从生成式大语言模型中蒸馏偏好的方法，利用LLM生成的数据来训练奖励模型，可能包括对比学习、偏好建模等技术。

Result: 从LLM蒸馏的偏好数据能够有效训练奖励模型，在人类偏好对齐任务上取得良好性能，缓解了人类标注数据不足的问题。

Conclusion: 从生成式LLM中蒸馏偏好是训练奖励模型的有效替代方案，为LLM对齐提供了新的数据来源，降低了依赖人类标注的局限性。

Abstract: Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as …

</details>


### [12] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.09865&hl=zh-CN&sa=X&d=2690202150983940264&ei=XQd4adi9OMm4ieoPof3esQc&scisig=AHkA5jRv1BcUPxtTPHZamoSkXvF8&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=9&folt=rel)
*J Sander,B Jalaian,VR Dasari*

Main category: Matei Zaharia

TL;DR: 该论文探讨了在资源受限的边缘设备上部署大型语言模型（LLMs）的挑战，并提出优化方法以降低计算、内存和能耗需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能实现先进的自然语言处理功能，但由于其高计算、高内存和高能耗需求，在资源受限的边缘设备上部署面临重大挑战。需要优化这些模型以适应边缘计算环境。

Method: 论文可能涉及模型压缩、量化、剪枝、知识蒸馏、硬件感知优化等方法来降低LLMs的资源需求，使其能够在边缘设备上高效运行。

Result: 通过提出的优化方法，预期能够显著降低LLMs在边缘设备上的计算负载、内存占用和能耗，同时保持可接受的性能水平。

Conclusion: 优化大型语言模型对于在资源受限的边缘设备上部署至关重要，通过适当的优化技术可以在保持性能的同时实现资源效率。

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires …

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Ordering-based Causal Discovery via Generalized Score Matching](https://arxiv.org/abs/2601.16249)
*Vy Vo,He Zhao,Trung Le,Edwin V. Bonilla,Dinh Phung*

Main category: cs.LG

TL;DR: 提出基于离散评分函数的叶节点判别准则，扩展评分匹配框架到离散数据因果发现，通过叶节点检测和边剪枝恢复DAG结构


<details>
  <summary>Details</summary>
Motivation: 从纯观测数据学习DAG结构是长期挑战，现有评分匹配框架主要针对连续数据，需要扩展到离散数据场景

Method: 扩展评分匹配框架到离散数据，引入基于离散评分函数的新型叶节点判别准则，通过叶节点检测识别拓扑顺序，再进行边剪枝恢复图结构

Result: 模拟和真实世界实验表明，该方法能从观测离散数据准确推断真实因果顺序，识别出的排序能显著提升现有因果发现基线的准确性

Conclusion: 基于离散评分函数的叶节点判别准则有效扩展了评分匹配框架，为离散数据因果发现提供了新方法，显著提升现有基线性能

Abstract: Learning DAG structures from purely observational data remains a long-standing challenge across scientific domains. An emerging line of research leverages the score of the data distribution to initially identify a topological order of the underlying DAG via leaf node detection and subsequently performs edge pruning for graph recovery. This paper extends the score matching framework for causal discovery, which is originally designated for continuous data, and introduces a novel leaf discriminant criterion based on the discrete score function. Through simulated and real-world experiments, we demonstrate that our theory enables accurate inference of true causal orders from observed discrete data and the identified ordering can significantly boost the accuracy of existing causal discovery baselines on nearly all of the settings.

</details>


### [14] [Student Mental Health Screening via Fitbit Data Collected During the COVID-19 Pandemic](https://arxiv.org/abs/2601.16324)
*Rebecca Lopez,Avantika Shrestha,ML Tlachac,Kevin Hickey,Xingtong Guo,Shichao Liu,Elke Rundensteiner*

Main category: cs.LG

TL;DR: 利用Fitbit可穿戴设备数据，通过机器学习模型筛查大学生抑郁、焦虑和压力，验证了心率、睡眠等生理模态在心理健康监测中的潜力


<details>
  <summary>Details</summary>
Motivation: 大学生面临高压力导致焦虑抑郁高发，现有研究在心理测量工具、生理模态和时间序列参数方面存在局限，需要更全面的可穿戴设备数据用于心理健康早期检测

Method: 收集疫情期间大学生心理健康与环境健康数据集，使用Fitbit设备获取多种生理模态数据，构建预测性机器学习模型，评估不同模态对抑郁、焦虑和压力的筛查能力

Result: 心率模态在压力筛查中达到F1分数0.77，睡眠模态在抑郁筛查中达到0.78，焦虑筛查最高F1分数为0.79，表明生理模态在心理健康筛查中具有潜力

Conclusion: 可穿戴设备支持持续心理健康监测，识别最佳数据聚合水平和适当模态对不同心理健康问题的筛查至关重要，为早期检测提供了技术基础

Abstract: College students experience many stressors, resulting in high levels of anxiety and depression. Wearable technology provides unobtrusive sensor data that can be used for the early detection of mental illness. However, current research is limited concerning the variety of psychological instruments administered, physiological modalities, and time series parameters. In this research, we collect the Student Mental and Environmental Health (StudentMEH) Fitbit dataset from students at our institution during the pandemic. We provide a comprehensive assessment of the ability of predictive machine learning models to screen for depression, anxiety, and stress using different Fitbit modalities. Our findings indicate potential in physiological modalities such as heart rate and sleep to screen for mental illness with the F1 scores as high as 0.79 for anxiety, the former modality reaching 0.77 for stress screening, and the latter modality achieving 0.78 for depression. This research highlights the potential of wearable devices to support continuous mental health monitoring, the importance of identifying best data aggregation levels and appropriate modalities for screening for different mental ailments.

</details>


### [15] [Efficient Gaussian process learning via subspace projections](https://arxiv.org/abs/2601.16332)
*Felipe Tobar,Elsa Cazelles*

Main category: cs.LG

TL;DR: 提出一种用于高斯过程的新训练目标——投影似然（PL），通过数据低维线性投影构建，在中等规模数据集上比精确GP训练和变分稀疏GP方法在精度和计算效率上更优。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在大规模数据集上面临计算复杂度高的问题，现有稀疏近似方法如变分自由能方法在精度和效率上仍有改进空间。需要开发一种既能保持准确性又能提高计算效率的新训练方法。

Method: 提出投影似然（PL）训练目标，通过数据低维线性投影构建高斯过程。推导了PL相关信息损失的闭式表达式，并经验性证明通过单位球面上的随机投影可以减少这种损失。在不同优化器、核函数和中等规模数据集上进行了系统比较。

Result: 投影似然方法在精度和计算效率上均优于精确GP训练和变分自由能稀疏GP方法。随机投影能有效减少信息损失，在多个数据集和配置下表现出优越性能。

Conclusion: 投影似然是一种有效的高斯过程训练方法，通过低维投影在保持精度的同时显著提高计算效率，为中等规模数据集的GP应用提供了实用解决方案。

Abstract: We propose a novel training objective for GPs constructed using lower-dimensional linear projections of the data, referred to as \emph{projected likelihood} (PL). We provide a closed-form expression for the information loss related to the PL and empirically show that it can be reduced with random projections on the unit sphere. We show the superiority of the PL, in terms of accuracy and computational efficiency, over the exact GP training and the variational free energy approach to sparse GPs over different optimisers, kernels and datasets of moderately large sizes.

</details>


### [16] [A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning](https://arxiv.org/abs/2601.16399)
*Sihan Zeng,Sujay Bhatt,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: 提出单循环一阶演员-评论家算法，通过惩罚重构解决双层优化问题，上层优化奖励参数，下层是MDP策略优化，引入衰减熵正则实现渐进无偏的超梯度估计。


<details>
  <summary>Details</summary>
Motivation: 现有双层优化和RL方法需要二阶信息、强正则化或低效的嵌套循环过程，需要更高效的单循环一阶算法来解决上层目标依赖下层最优策略的双层优化问题。

Method: 提出单循环一阶演员-评论家算法，通过惩罚重构将双层问题转化为单层问题，在下层RL目标中引入衰减熵正则，实现渐进无偏的超梯度估计，无需精确求解无正则RL问题。

Result: 建立了算法在有限时间和有限样本下收敛到原始无正则双层优化问题平稳点的理论保证，通过特殊类型的Polyak-Lojasiewicz条件下的下层残差分析证明收敛性。

Conclusion: 该方法在GridWorld目标位置问题和基于人类反馈的强化学习（RLHF）的快乐推文生成任务上验证了性能，为双层优化问题提供了高效的单循环一阶解决方案。

Abstract: We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).

</details>


### [17] [Towards a Theoretical Understanding to the Generalization of RLHF](https://arxiv.org/abs/2601.16403)
*Zhaochun Li,Mingyang Yi,Yue Wang,Shisheng Cui,Yong Liu*

Main category: cs.LG

TL;DR: 该论文建立了基于线性奖励模型的RLHF理论泛化框架，通过算法稳定性证明了策略模型的经验最优解具有O(n^{-1/2})的泛化界，为RLHF的实证泛化提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: RLHF及其变体已成为对齐大语言模型与人类意图的主要方法，虽然实证有效，但在高维设置下的理论泛化性质尚未得到充分探索。现有工作主要基于奖励模型最大似然估计的一致性，而本文旨在建立与端到端学习实践一致的理论框架。

Method: 采用算法稳定性框架，在线性奖励模型假设下分析RLHF的泛化性质。在关键的特征覆盖条件下，证明策略模型经验最优解的泛化界为O(n^{-1/2})，并将结果推广到梯度上升和随机梯度上升等基于梯度的学习算法。

Result: 证明了在特征覆盖条件下，策略模型的经验最优解具有O(n^{-1/2})的泛化界。该结果可推广到梯度上升和随机梯度上升算法获得的参数，为RLHF后大语言模型观察到的实证泛化提供了新的理论证据。

Conclusion: 本文建立了RLHF的理论泛化框架，通过算法稳定性分析证明了策略模型在特征覆盖条件下的泛化性能，为实践中观察到的RLHF泛化现象提供了理论支持，填补了高维设置下RLHF理论研究的空白。

Abstract: Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored. To this end, we build the generalization theory on RLHF of LLMs under the linear reward model, through the framework of algorithmic stability. In contrast to the existing works built upon the consistency of maximum likelihood estimations on reward model, our analysis is presented under an end-to-end learning framework, which is consistent with practice. Concretely, we prove that under a key \textbf{feature coverage} condition, the empirical optima of policy model have a generalization bound of order $\mathcal{O}(n^{-\frac{1}{2}})$. Moreover, the results can be extrapolated to parameters obtained by gradient-based learning algorithms, i.e., Gradient Ascent (GA) and Stochastic Gradient Ascent (SGA). Thus, we argue that our results provide new theoretical evidence for the empirically observed generalization of LLMs after RLHF.

</details>


### [18] [Reasoning-Enhanced Rare-Event Prediction with Balanced Outcome Correction](https://arxiv.org/abs/2601.16406)
*Vitaly Bulgakov,Alexander Turchin*

Main category: cs.LG

TL;DR: LPCORP是一个两阶段框架，用于解决罕见事件预测中的类别不平衡问题，通过推理增强预测和置信度校正来改善模型性能，在医疗和消费者服务领域数据集上显著提升了精度。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融、可靠性工程、客户支持、航空安全等领域，罕见事件预测至关重要，但极端类别不平衡会使传统模型偏向多数类预测，限制了召回率、校准性和实际应用价值。

Method: 提出LPCORP（低流行率预测校正器）两阶段框架：第一阶段使用推理模型从叙事输入中生成增强预测；第二阶段使用轻量级逻辑回归分类器评估并选择性校正这些输出，以减轻流行率驱动的偏差。

Result: 该方法将高度不平衡的设置转化为平衡设置，同时保留原始样本数量且不应用任何重采样策略。测试集评估显示性能显著提升，特别是在低流行率数据中已知的弱点——精度方面有明显改善。成本降低分析显示，在某些情况下，基于预测的预防性干预措施可减少超过50%的费用。

Conclusion: LPCORP框架有效解决了罕见事件预测中的类别不平衡问题，通过推理增强和置信度校正相结合的方法，在保持样本完整性的同时显著提升了预测性能，为实际应用提供了经济有效的解决方案。

Abstract: Rare-event prediction is critical in domains such as healthcare, finance, reliability engineering, customer support, aviation safety, where positive outcomes are infrequent yet potentially catastrophic. Extreme class imbalance biases conventional models toward majority-class predictions, limiting recall, calibration, and operational usefulness. We propose LPCORP (Low-Prevalence CORrector for Prediction)*, a two-stage framework that combines reasoningenhanced prediction with confidence-based outcome correction. A reasoning model first produces enriched predictions from narrative inputs, after which a lightweight logistic-regression classifier evaluates and selectively corrects these outputs to mitigate prevalence-driven bias. We evaluate LPCORP on real-world datasets from medical and consumer service domains. The results show that this method transforms a highly imbalanced setting into a well-balanced one while preserving the original number of samples and without applying any resampling strategies. Test-set evaluation demonstrates substantially improved performance, particularly in precision, which is a known weakness in low-prevalence data. We further provide a costreduction analysis comparing the expenses associated with rare-event damage control without preventive measures to those incurred when low-cost, prediction-based preventive interventions are applied that showed more than 50% reduction in some cases. * Patent pending: U.S. Provisional 63/933,518, filed 8 December 2025.

</details>


### [19] [A Refinement of Vapnik--Chervonenkis' Theorem](https://arxiv.org/abs/2601.16411)
*A. Iosevich,A. Vagharshakyan,E. Wyman*

Main category: cs.LG

TL;DR: 该论文重新审视了VC定理的概率部分，使用带显式Berry-Esseen误差控制的正态逼近代替Hoeffding不等式，得到了VC估计的中偏差锐化结果。


<details>
  <summary>Details</summary>
Motivation: 经典VC定理使用Hoeffding不等式作为最终步骤，作者希望改进这一概率论证部分，通过使用更精确的正态逼近方法来获得更尖锐的收敛速率估计。

Method: 重新审视VC定理的概率论证部分，使用带显式Berry-Esseen误差控制的正态逼近方法替代传统的Hoeffding不等式，从而获得更精确的收敛速率估计。

Result: 得到了VC估计的中偏差锐化结果，当ε√n较大时，在主导指数项中获得了额外的(ε√n)^{-1}阶因子，改进了经典的VC收敛速率估计。

Conclusion: 通过使用更精确的正态逼近方法，可以显著改进VC定理中的概率收敛速率估计，特别是在中偏差区域获得更尖锐的结果。

Abstract: Vapnik--Chervonenkis' theorem is a seminal result in machine learning. It establishes sufficient conditions for empirical probabilities to converge to theoretical probabilities, uniformly over families of events. It also provides an estimate for the rate of such uniform convergence.
  We revisit the probabilistic component of the classical argument. Instead of applying Hoeffding's inequality at the final step, we use a normal approximation with explicit Berry--Esseen error control. This yields a moderate-deviation sharpening of the usual VC estimate, with an additional factor of order $(\varepsilon\sqrt{n})^{-1}$ in the leading exponential term when $\varepsilon\sqrt{n}$ is large.

</details>


### [20] [PyHealth 2.0: A Comprehensive Open-Source Toolkit for Accessible and Reproducible Clinical Deep Learning](https://arxiv.org/abs/2601.16414)
*John Wu,Yongda Fan,Zhenbang Wu,Paul Landes,Eric Schrock,Sayeed Sajjad Razin,Arjun Chatterjee,Naveen Baskaran,Joshua Steier,Andrea Fitzpatrick,Bilal Arif,Rian Atri,Jathurshan Pradeepkumar,Siddhartha Laghuvarapu,Junyi Gao,Adam R. Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: PyHealth 2.0是一个增强的临床深度学习工具包，通过统一数据集、模型和医疗编码标准，实现最少7行代码完成预测建模，解决临床AI研究的可复现性、计算成本和领域专业知识障碍。


<details>
  <summary>Details</summary>
Motivation: 临床AI研究面临基线难以复现、计算成本高、需要领域专业知识等持续障碍，阻碍了研究的可访问性和可复现性。

Method: 开发PyHealth 2.0工具包，提供三个关键贡献：1) 统一框架整合15+数据集、20+临床任务、25+模型、5+可解释性方法和不确定性量化；2) 支持多模态数据和不同计算资源，处理速度提升39倍，内存使用降低20倍；3) 建立400+成员的活跃开源社区，提供多语言支持。

Result: PyHealth 2.0建立了开源基础和社区，支持从16GB笔记本电脑到生产系统的部署，显著降低了临床AI研究的门槛。

Conclusion: PyHealth 2.0为可访问、可复现的医疗AI研究提供了开源基础和社区支持，通过统一框架和社区协作降低了临床AI研究的障碍。

Abstract: Difficulty replicating baselines, high computational costs, and required domain expertise create persistent barriers to clinical AI research. To address these challenges, we introduce PyHealth 2.0, an enhanced clinical deep learning toolkit that enables predictive modeling in as few as 7 lines of code. PyHealth 2.0 offers three key contributions: (1) a comprehensive toolkit addressing reproducibility and compatibility challenges by unifying 15+ datasets, 20+ clinical tasks, 25+ models, 5+ interpretability methods, and uncertainty quantification including conformal prediction within a single framework that supports diverse clinical data modalities - signals, imaging, and electronic health records - with translation of 5+ medical coding standards; (2) accessibility-focused design accommodating multimodal data and diverse computational resources with up to 39x faster processing and 20x lower memory usage, enabling work from 16GB laptops to production systems; and (3) an active open-source community of 400+ members lowering domain expertise barriers through extensive documentation, reproducible research contributions, and collaborations with academic health systems and industry partners, including multi-language support via RHealth. PyHealth 2.0 establishes an open-source foundation and community advancing accessible, reproducible healthcare AI. Available at pip install pyhealth.

</details>


### [21] [Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance](https://arxiv.org/abs/2601.16425)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TL;DR: 该论文比较了贝叶斯实验设计中KL散度和Wasserstein距离两种效用函数准则，通过理论分析和数值实验揭示了它们在不同场景下的优缺点，为实际应用提供了选择指南。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯实验设计(BED)为科学实验提供了基于信息的框架，但效用函数的选择一直是一个活跃的研究课题。虽然KL散度是最常用的选择，但最近的研究提出了Wasserstein距离作为替代。然而，对于这两种准则在不同情况下的表现和适用性缺乏系统比较，特别是在存在模型失配的情况下。

Method: 1. 首先使用一个玩具示例来说明Wasserstein距离的问题：固定形状的后验分布的Wasserstein距离值取决于其主要质量在支撑集中的相对位置，可能产生与信息增益无关的虚假奖励，特别是在使用非信息先验时。
2. 通过BED文献中的经典源反演问题，系统比较KL散度和Wasserstein距离两种准则。
3. 分析在有无模型失配情况下的表现差异。

Result: 1. Wasserstein距离存在一个理论问题：对于固定形状的后验分布，其值取决于分布主要质量在支撑集中的位置，可能导致与信息增益无关的虚假奖励，特别是在使用均匀分布等非信息先验时。
2. 在没有模型失配的情况下，KL散度倾向于导致更快的收敛速度。
3. 当模型失配不可忽略时，Wasserstein度量提供了更稳健的序列BED结果。

Conclusion: 该研究阐明了KL散度和Wasserstein度量作为效用函数时的权衡关系：KL散度在无模型失配时收敛更快，而Wasserstein度量在存在模型失配时更稳健。这些发现为实际BED应用中选择合适的准则提供了指导原则。

Abstract: Designing experiments that systematically gather data from complex physical systems is central to accelerating scientific discovery. While Bayesian experimental design (BED) provides a principled, information-based framework that integrates experimental planning with probabilistic inference, the selection of utility functions in BED is a long-standing and active topic, where different criteria emphasize different notions of information. Although Kullback--Leibler (KL) divergence has been one of the most common choices, recent studies have proposed Wasserstein distance as an alternative. In this work, we first employ a toy example to illustrate an issue of Wasserstein distance - the value of Wasserstein distance of a fixed-shape posterior depends on the relative position of its main mass within the support and can exhibit false rewards unrelated to information gain, especially with a non-informative prior (e.g., uniform distribution). We then further provide a systematic comparison between these two criteria through a classical source inversion problem in the BED literature, revealing that the KL divergence tends to lead to faster convergence in the absence of model discrepancy, while Wasserstein metrics provide more robust sequential BED results if model discrepancy is non-negligible. These findings clarify the trade-offs between KL divergence and Wasserstein metrics for the utility function and provide guidelines for selecting suitable criteria in practical BED applications.

</details>


### [22] [On the Expressive Power of Floating-Point Transformers](https://arxiv.org/abs/2601.16450)
*Sejun Park,Yeachan Park,Geonho Hwang*

Main category: cs.LG

TL;DR: 研究浮点变换器的表达能力，发现在有限精度计算下，即使没有位置编码也能表示非置换等变函数，且序列长度有限时可表示所有置换等变函数，但序列长度大时则不能。


<details>
  <summary>Details</summary>
Motivation: 现有关于变换器表达能力的研究基于实数参数和精确运算，但实际计算机实现只能使用有限数字集和存在舍入误差的浮点运算。需要研究在浮点参数和浮点运算下的变换器表达能力。

Method: 研究浮点变换器（使用浮点参数和浮点运算）的表示能力。分析其在有无位置编码情况下的表达能力，特别关注置换等变性和序列长度的影响。

Result: 1. 浮点变换器即使没有位置编码也能表示一类非置换等变函数；2. 序列长度有界时，浮点变换器能表示所有置换等变函数；3. 序列长度大时，浮点变换器不能表示所有置换等变函数；4. 发现了浮点变换器中的最小等变结构；5. 所有非平凡加性位置编码都会损害浮点变换器的表示能力。

Conclusion: 浮点变换器的表达能力与理想实数变换器存在显著差异，有限精度计算引入了非等变性和序列长度相关的限制，位置编码在实际实现中可能对表示能力产生负面影响。

Abstract: The study on the expressive power of transformers shows that transformers are permutation equivariant, and they can approximate all permutation-equivariant continuous functions on a compact domain. However, these results are derived under real parameters and exact operations, while real implementations on computers can only use a finite set of numbers and inexact machine operations with round-off errors. In this work, we investigate the representability of floating-point transformers that use floating-point parameters and floating-point operations. Unlike existing results under exact operations, we first show that floating-point transformers can represent a class of non-permutation-equivariant functions even without positional encoding. Furthermore, we prove that floating-point transformers can represent all permutation-equivariant functions when the sequence length is bounded, but they cannot when the sequence length is large. We also found the minimal equivariance structure in floating-point transformers, and show that all non-trivial additive positional encoding can harm the representability of floating-point transformers.

</details>


### [23] [On the Effects of Adversarial Perturbations on Distribution Robustness](https://arxiv.org/abs/2601.16464)
*Yipei Wang,Zhaoying Pan,Xiaoqian Wang*

Main category: cs.LG

TL;DR: 该论文分析了对抗鲁棒性和分布鲁棒性之间的权衡关系，发现ℓ∞扰动在适度偏置数据上能提升分布鲁棒性，特征可分性在权衡中起关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗鲁棒性和分布鲁棒性都旨在确保模型性能的可靠性，但先前研究揭示了二者之间存在权衡。对抗训练可能增加对虚假特征的依赖，从而损害分布鲁棒性，特别是在某些代表性不足的子群体上。本研究旨在深入理解这种权衡关系及其影响因素。

Method: 通过理论分析对抗鲁棒性和分布鲁棒性，研究在扰动数据上训练的模型，为每步对抗训练提供可处理的替代方法。分析重点关注ℓ∞扰动、数据偏置程度和特征可分性等因素的相互作用。

Result: 研究发现：1）对抗鲁棒性和分布鲁棒性之间存在权衡关系；2）ℓ∞扰动在适度偏置数据上能提高分布鲁棒性；3）在高度偏斜数据上，当简单性偏置诱导模型依赖核心特征（表现为更大的特征可分性）时，分布鲁棒性的增益仍然存在；4）特征可分性在权衡关系中起关键作用。

Conclusion: 该研究扩展了对对抗鲁棒性和分布鲁棒性权衡关系的理解，强调了特征可分性在权衡中的重要作用。尽管在许多情况下权衡关系仍然存在，但忽视特征可分性的作用可能导致对鲁棒性的误导性结论。

Abstract: Adversarial robustness refers to a model's ability to resist perturbation of inputs, while distribution robustness evaluates the performance of the model under data shifts. Although both aim to ensure reliable performance, prior work has revealed a tradeoff in distribution and adversarial robustness. Specifically, adversarial training might increase reliance on spurious features, which can harm distribution robustness, especially the performance on some underrepresented subgroups. We present a theoretical analysis of adversarial and distribution robustness that provides a tractable surrogate for per-step adversarial training by studying models trained on perturbed data. In addition to the tradeoff, our work further identified a nuanced phenomenon that $\ell_\infty$ perturbations on data with moderate bias can yield an increase in distribution robustness. Moreover, the gain in distribution robustness remains on highly skewed data when simplicity bias induces reliance on the core feature, characterized as greater feature separability. Our theoretical analysis extends the understanding of the tradeoff by highlighting the interplay of the tradeoff and the feature separability. Despite the tradeoff that persists in many cases, overlooking the role of feature separability may lead to misleading conclusions about robustness.

</details>


### [24] [A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study](https://arxiv.org/abs/2601.16467)
*Maxwell Reynolds,Chaitanya Srinivasan,Vijay Cherupally,Michael Leone,Ke Yu,Li Sun,Tigmanshu Chaudhary,Andreas Pfenning,Kayhan Batmanghelich*

Main category: cs.LG

TL;DR: R-NCE自监督学习框架整合FreeSurfer特征，在阿尔茨海默病预测任务中超越传统特征和现有SSL方法，其脑年龄差距指标显示高遗传性并与神经退行和脑血管过程相关。


<details>
  <summary>Details</summary>
Motivation: 当前阿尔茨海默病生物标志物主要依赖手工特征（如皮层厚度、体积），现有自监督学习方法在疾病分类、转化预测等任务中表现不佳，需要开发更强大的生物标志物发现方法。

Method: 提出残差噪声对比估计（R-NCE）框架，整合辅助FreeSurfer特征，同时最大化增强不变性信息，通过自监督学习从结构MRI数据中提取更有效的特征。

Result: R-NCE在多个基准测试中超越传统FreeSurfer特征和现有SSL方法，包括AD转化预测；R-NCE脑年龄差距指标显示高遗传性，与MAPT和IRAG1基因相关，在星形胶质细胞和少突胶质细胞中富集。

Conclusion: R-NCE能够从结构MRI中发现更敏感且生物学相关的AD生物标志物，这些标志物对神经退行性和脑血管过程敏感，为早期检测和监测提供了新工具。

Abstract: Discovery of sensitive and biologically grounded biomarkers is essential for early detection and monitoring of Alzheimer's disease (AD). Structural MRI is widely available but typically relies on hand-crafted features such as cortical thickness or volume. We ask whether self-supervised learning (SSL) can uncover more powerful biomarkers from the same data. Existing SSL methods underperform FreeSurfer-derived features in disease classification, conversion prediction, and amyloid status prediction. We introduce Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while maximizing additional augmentation-invariant information. R-NCE outperforms traditional features and existing SSL methods across multiple benchmarks, including AD conversion prediction. To assess biological relevance, we derive Brain Age Gap (BAG) measures and perform genome-wide association studies. R-NCE-BAG shows high heritability and associations with MAPT and IRAG1, with enrichment in astrocytes and oligodendrocytes, indicating sensitivity to neurodegenerative and cerebrovascular processes.

</details>


### [25] [Robust Categorical Data Clustering Guided by Multi-Granular Competitive Learning](https://arxiv.org/abs/2601.16491)
*Shenghong Cai,Yiqun Zhang,Xiaopeng Luo,Yiu-Ming Cheung,Hong Jia,Peng Liu*

Main category: cs.LG

TL;DR: 提出MCDC方法，通过MGCPL算法和CAME策略解决分类数据聚类中的嵌套粒度簇效应问题，实现自动探索多粒度簇分布并具有线性时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 分类数据在大数据分析中非常普遍，但由于分类特征是定性值，无法明确定义距离空间，且存在嵌套粒度簇效应（小簇重叠形成大簇），这给分类数据的聚类分析带来了巨大挑战。

Method: 提出MCDC方法：1) MGCPL算法（多粒度竞争惩罚学习），让潜在簇在不同阶段以不同数量的自然紧凑簇进行交互式自我调整和收敛；2) CAME策略（基于MGCPL编码的簇聚合），首先根据学习到的多粒度分布对数据对象进行编码，然后在嵌入空间进行最终聚类。

Result: MCDC方法能够自动探索多粒度簇的嵌套分布，对各种领域的分类数据集具有高度鲁棒性。得益于线性时间复杂度，可扩展到大规模数据集，并有望用于预分区数据集或计算节点以提升分布式计算性能。大量实验统计证据表明其在多个真实公共数据集上优于现有最先进方法。

Conclusion: 提出的MCDC方法通过MGCPL算法和CAME策略有效解决了分类数据聚类中的嵌套粒度簇效应问题，实现了自动多粒度聚类，具有鲁棒性、可扩展性和实际应用价值。

Abstract: Data set composed of categorical features is very common in big data analysis tasks. Since categorical features are usually with a limited number of qualitative possible values, the nested granular cluster effect is prevalent in the implicit discrete distance space of categorical data. That is, data objects frequently overlap in space or subspace to form small compact clusters, and similar small clusters often form larger clusters. However, the distance space cannot be well-defined like the Euclidean distance due to the qualitative categorical data values, which brings great challenges to the cluster analysis of categorical data. In view of this, we design a Multi-Granular Competitive Penalization Learning (MGCPL) algorithm to allow potential clusters to interactively tune themselves and converge in stages with different numbers of naturally compact clusters. To leverage MGCPL, we also propose a Cluster Aggregation strategy based on MGCPL Encoding (CAME) to first encode the data objects according to the learned multi-granular distributions, and then perform final clustering on the embeddings. It turns out that the proposed MGCPL-guided Categorical Data Clustering (MCDC) approach is competent in automatically exploring the nested distribution of multi-granular clusters and highly robust to categorical data sets from various domains. Benefiting from its linear time complexity, MCDC is scalable to large-scale data sets and promising in pre-partitioning data sets or compute nodes for boosting distributed computing. Extensive experiments with statistical evidence demonstrate its superiority compared to state-of-the-art counterparts on various real public data sets.

</details>


### [26] [kNN-Graph: An adaptive graph model for $k$-nearest neighbors](https://arxiv.org/abs/2601.16509)
*Jiaye Li,Gang Chen,Hang Xu,Shichao Zhang*

Main category: cs.LG

TL;DR: 提出一种自适应图模型，通过将HNSW图与预计算投票机制结合，将邻居选择和加权的计算负担完全转移到训练阶段，实现推理速度与精度的解耦。


<details>
  <summary>Details</summary>
Motivation: kNN算法在大规模应用中面临推理速度与精度的计算权衡问题，现有近似最近邻解决方案虽然加速检索但往往降低分类精度，且缺乏选择最优邻域大小(k)的自适应性。

Method: 集成分层可导航小世界(HNSW)图与预计算投票机制，构建自适应图模型。高层图实现快速导航，低层图编码精确的节点特定决策边界和自适应邻居数量，将邻居选择和加权的计算完全转移到训练阶段。

Result: 在六个不同数据集上对八个最先进基线进行基准测试，该架构显著加速推理速度，实现实时性能，同时不损害分类精度。

Conclusion: 为kNN长期存在的推理瓶颈提供了可扩展、鲁棒的解决方案，建立了基于图的非参数学习的新结构范式。

Abstract: The k-nearest neighbors (kNN) algorithm is a cornerstone of non-parametric classification in artificial intelligence, yet its deployment in large-scale applications is persistently constrained by the computational trade-off between inference speed and accuracy. Existing approximate nearest neighbor solutions accelerate retrieval but often degrade classification precision and lack adaptability in selecting the optimal neighborhood size (k). Here, we present an adaptive graph model that decouples inference latency from computational complexity. By integrating a Hierarchical Navigable Small World (HNSW) graph with a pre-computed voting mechanism, our framework completely transfers the computational burden of neighbor selection and weighting to the training phase. Within this topological structure, higher graph layers enable rapid navigation, while lower layers encode precise, node-specific decision boundaries with adaptive neighbor counts. Benchmarking against eight state-of-the-art baselines across six diverse datasets, we demonstrate that this architecture significantly accelerates inference speeds, achieving real-time performance, without compromising classification accuracy. These findings offer a scalable, robust solution to the long-standing inference bottleneck of kNN, establishing a new structural paradigm for graph-based nonparametric learning.

</details>


### [27] [Finite-Time Analysis of Gradient Descent for Shallow Transformers](https://arxiv.org/abs/2601.16514)
*Enes Arda,Semih Cayci,Atilla Eryilmaz*

Main category: cs.LG

TL;DR: 该论文分析了浅层Transformer在核机制下的优化特性，发现其宽度仅需与样本量对数相关，优化误差与序列长度无关，但内存需求随序列长度增长。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer为何表现优异具有挑战性，因为其优化景观是非凸的。本文旨在分析浅层Transformer在核机制下的优化特性，并与循环架构进行对比。

Method: 分析具有m个独立头的浅层Transformer，使用投影梯度下降在核机制下训练。在教师-学生设置中进行数值验证，确认预测的缩放规律。

Result: 发现两个主要结果：(1) 非渐近保证所需的宽度仅与样本量n成对数比例；(2) 优化误差与序列长度T无关。这与循环架构形成鲜明对比，后者的优化误差可能随T指数增长。代价是内存需求随序列长度增长。

Conclusion: Transformer在核机制下具有有利的优化特性：宽度需求对数缩放，优化误差与序列长度无关，但需要权衡内存开销。这解释了Transformer相对于循环架构的优势。

Abstract: Understanding why Transformers perform so well remains challenging due to their non-convex optimization landscape. In this work, we analyze a shallow Transformer with $m$ independent heads trained by projected gradient descent in the kernel regime. Our analysis reveals two main findings: (i) the width required for nonasymptotic guarantees scales only logarithmically with the sample size $n$, and (ii) the optimization error is independent of the sequence length $T$. This contrasts sharply with recurrent architectures, where the optimization error can grow exponentially with $T$. The trade-off is memory: to keep the full context, the Transformer's memory requirement grows with the sequence length. We validate our theoretical results numerically in a teacher-student setting and confirm the predicted scaling laws for Transformers.

</details>


### [28] [Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs](https://arxiv.org/abs/2601.16527)
*Xianya Fang,Feiyang Ren,Xiang Chen,Yu Tian,Zhen Bi,Haiyang Yu,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: SARE提出一种针对多模态LLM物体幻觉的结构化鲁棒遗忘方法，通过目标最小化-最大化优化和Targeted-SAM机制，在幻觉概念周围平坦化损失景观，实现稳定消除


<details>
  <summary>Details</summary>
Motivation: 多模态LLM存在物体幻觉问题，现有遗忘方法存在结构脆弱性缺陷，仅实现表面抑制，模型陷入尖锐最小值，轻微重新学习就会导致幻觉灾难性复发

Method: 提出SARE框架，将遗忘建模为目标最小化-最大化优化问题，使用Targeted-SAM机制在幻觉概念周围显式平坦化损失景观，通过模拟最坏参数扰动下的幻觉抑制确保鲁棒性

Result: SARE在消除效果上显著优于基线方法，同时保持一般生成质量，关键是在重新学习和参数更新后仍能保持持久的幻觉抑制，验证了几何稳定化的有效性

Conclusion: 通过几何稳定化方法解决多模态LLM物体幻觉的结构脆弱性问题，SARE框架实现了鲁棒且持久的幻觉消除，为可靠的多模态生成提供了新思路

Abstract: Multimodal LLMs are powerful but prone to object hallucinations, which describe non-existent entities and harm reliability. While recent unlearning methods attempt to mitigate this, we identify a critical flaw: structural fragility. We empirically demonstrate that standard erasure achieves only superficial suppression, trapping the model in sharp minima where hallucinations catastrophically resurge after lightweight relearning. To ensure geometric stability, we propose SARE, which casts unlearning as a targeted min-max optimization problem and uses a Targeted-SAM mechanism to explicitly flatten the loss landscape around hallucinated concepts. By suppressing hallucinations under simulated worst-case parameter perturbations, our framework ensures robust removal stable against weight shifts. Extensive experiments demonstrate that SARE significantly outperforms baselines in erasure efficacy while preserving general generation quality. Crucially, it maintains persistent hallucination suppression against relearning and parameter updates, validating the effectiveness of geometric stabilization.

</details>


### [29] [A Collision-Free Hot-Tier Extension for Engram-Style Conditional Memory: A Controlled Study of Training Dynamics](https://arxiv.org/abs/2601.16531)
*Tao Lin*

Main category: cs.LG

TL;DR: 高频键冲突并非Engram式条件记忆的主要瓶颈，碰撞消除设计在等参数设置下未持续改善验证损失，碰撞可能作为隐式正则化发挥作用。


<details>
  <summary>Details</summary>
Motivation: 研究高频键冲突是否是Engram式条件记忆的主要瓶颈，探索通过消除碰撞来改进记忆查找性能的可能性。

Method: 引入Engram-Nine碰撞消除热层扩展，使用最小完美哈希函数映射最频繁n-gram，同时保留原始多头哈希查找作为冷层。采用严格等参数设置，通过路由分层评估分解每个token损失为热/冷层贡献。

Result: 碰撞消除设计未持续改善验证损失。发现训练中一致的"热到冷优势翻转"现象：热（高频）位置初始损失较低，但冷位置最终超越它们。碰撞消除配置比碰撞易发基线更早翻转，表明碰撞作为隐式正则化。还发现门控不匹配：门控早期学习偏好热位置，但即使翻转后仍持续分配更高权重给损失更高的位置。

Conclusion: 仅改进查找精度不能保证更好的训练结果。主要限制可能在于门控信用分配而非索引准确性，碰撞引起的噪声可能提供有益的正则化，不应被简单消除。

Abstract: We investigate whether high-frequency key collisions are a primary bottleneck in Engram-style conditional memory. To isolate the effect of collisions, we introduce Engram-Nine, a collision-free hot-tier extension that maps the most frequent n-grams through a Minimal Perfect Hash Function (MPHF) while retaining the original multi-head hashed lookup as a cold tier. Under a strictly iso-parameter setup, the collision-free design does not consistently improve validation loss.
  Through route-stratified evaluation (decomposing per-token loss into hot/cold contributions), we uncover a consistent "hot-to-cold advantage flip" during training: hot (high-frequency) positions initially have lower loss, but cold positions eventually surpass them. Crucially, collision-free configurations flip earlier than collision-prone baselines, suggesting that collisions act as implicit regularization. We also identify a gating mismatch: the gate learns to favor hot positions early in training, but this preference persists even after the flip, assigning higher weights to positions with higher loss.
  Our findings suggest that improving lookup precision alone does not guarantee better training outcomes. The dominant limitation may lie in gating credit assignment rather than index accuracy, and collision-induced noise may provide beneficial regularization that should not be naively eliminated.

</details>


### [30] [Understanding and Improving UMAP with Geometric and Topological Priors: The JORC-UMAP Algorithm](https://arxiv.org/abs/2601.16552)
*Xiaobin Li,Run Zhang*

Main category: cs.LG

TL;DR: JORC-UMAP：通过引入Ollivier-Ricci曲率几何先验和Jaccard相似性拓扑先验，改进UMAP在非线性降维中的几何保持能力，减少拓扑撕裂和结构塌陷。


<details>
  <summary>Details</summary>
Motivation: UMAP在可视化高维数据时，其局部欧氏距离假设经常无法捕捉内在流形几何，导致拓扑撕裂和结构塌陷。作者发现UMAP对k近邻图的敏感性是主要原因，需要几何感知的增强方法。

Method: 提出JORC-UMAP方法：1) 引入Ollivier-Ricci曲率作为几何先验，增强几何瓶颈处的边连接，减少冗余链接；2) 结合Jaccard相似性作为拓扑先验，确保邻域一致性，弥补曲率估计对噪声的敏感性。

Result: 在合成和真实数据集上的实验表明，JORC-UMAP比标准UMAP和其他降维方法更有效地减少撕裂和塌陷，通过SVM准确率和三元组保持分数衡量，同时保持计算效率。

Conclusion: JORC-UMAP为UMAP提供了几何感知的增强，能够更忠实地可视化数据，更好地区分真实流形结构和虚假连接。

Abstract: Nonlinear dimensionality reduction techniques, particularly UMAP, are widely used for visualizing high-dimensional data. However, UMAP's local Euclidean distance assumption often fails to capture intrinsic manifold geometry, leading to topological tearing and structural collapse. We identify UMAP's sensitivity to the k-nearest neighbor graph as a key cause. To address this, we introduce Ollivier-Ricci curvature as a geometric prior, reinforcing edges at geometric bottlenecks and reducing redundant links. Since curvature estimation is noise-sensitive, we also incorporate a topological prior using Jaccard similarity to ensure neighborhood consistency. The resulting method, JORC-UMAP, better distinguishes true manifold structure from spurious connections. Experiments on synthetic and real-world datasets show that JORC-UMAP reduces tearing and collapse more effectively than standard UMAP and other DR methods, as measured by SVM accuracy and triplet preservation scores, while maintaining computational efficiency. This work offers a geometry-aware enhancement to UMAP for more faithful data visualization.

</details>


### [31] [Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach](https://arxiv.org/abs/2601.16568)
*Abdurahman Maarouf,Alket Bakiaj,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出kNN-ICL框架，利用大语言模型进行上下文学习，仅需少量标注样本即可预测早期初创企业成功，在数据稀缺的VC投资场景中优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 早期初创企业成功预测面临数据稀缺挑战，传统机器学习方法需要大量标注数据，而VC公司通常只有几十个早期初创企业的信息。需要开发能够在数据稀缺环境下有效工作的预测方法。

Method: 提出kNN-ICL框架：基于k近邻的上下文学习方法，利用大语言模型无需训练，仅使用少量标注初创企业作为演示示例。通过相似性选择最相关的历史初创企业作为上下文示例。

Result: 使用Crunchbase真实数据，kNN-ICL方法比监督机器学习基线和普通上下文学习获得更高预测准确率。仅需50个示例即可达到高平衡准确率。

Conclusion: 上下文学习可作为VC公司在数据稀缺环境中的决策工具，kNN-ICL框架在早期初创企业成功预测中表现出色，为小样本学习提供了有效解决方案。

Abstract: Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely on large labeled datasets for model training. To address this challenge, we propose an in-context learning framework for startup success prediction using large language models (LLMs) that requires no model training and leverages only a small set of labeled startups as demonstration examples. Specifically, we propose a novel k-nearest-neighbor-based in-context learning framework, called kNN-ICL, which selects the most relevant past startups as examples based on similarity. Using real-world profiles from Crunchbase, we find that the kNN-ICL approach achieves higher prediction accuracy than supervised machine learning baselines and vanilla in-context learning. Further, we study how performance varies with the number of in-context examples and find that a high balanced accuracy can be achieved with as few as 50 examples. Together, we demonstrate that in-context learning can serve as a decision-making tool for VC firms operating in data-scarce environments.

</details>


### [32] [Dual-Prototype Disentanglement: A Context-Aware Enhancement Framework for Time Series Forecasting](https://arxiv.org/abs/2601.16632)
*Haonan Yang,Jianchao Tang,Zhuo Li*

Main category: cs.LG

TL;DR: DPAD是一种模型无关的辅助框架，通过动态双原型库和上下文感知路由机制，实现时间序列模式的解耦与自适应增强，提升预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通过修改架构或引入增强策略来提升时间序列预测性能，但往往无法动态解耦和利用时间序列中复杂交织的时序模式，导致学习到静态、平均化的表示，缺乏上下文感知能力。

Method: 提出双原型自适应解耦框架（DPAD），包含：1）动态双原型库（DDP），由具有强时序先验的通用模式库（捕捉主流趋势/季节模式）和动态记忆关键但罕见事件的稀有模式库组成；2）双路径上下文感知路由机制（DPC），从DDP中选择性检索上下文特定模式表示来增强输出；3）解耦引导损失（DGLoss），确保每个原型库专注于其指定角色同时保持全面覆盖。

Result: 综合实验表明，DPAD在多样化的现实世界基准测试中，能够持续提升最先进模型的预测性能和可靠性。

Conclusion: DPAD作为一种模型无关的辅助方法，通过模式解耦和上下文自适应能力，有效解决了现有方法在动态处理复杂时序模式方面的局限性，显著提升了时间序列预测的性能和鲁棒性。

Abstract: Time series forecasting has witnessed significant progress with deep learning. While prevailing approaches enhance forecasting performance by modifying architectures or introducing novel enhancement strategies, they often fail to dynamically disentangle and leverage the complex, intertwined temporal patterns inherent in time series, thus resulting in the learning of static, averaged representations that lack context-aware capabilities. To address this, we propose the Dual-Prototype Adaptive Disentanglement framework (DPAD), a model-agnostic auxiliary method that equips forecasting models with the ability of pattern disentanglement and context-aware adaptation. Specifically, we construct a Dynamic Dual-Prototype bank (DDP), comprising a common pattern bank with strong temporal priors to capture prevailing trend or seasonal patterns, and a rare pattern bank dynamically memorizing critical yet infrequent events, and then an Dual-Path Context-aware routing (DPC) mechanism is proposed to enhance outputs with selectively retrieved context-specific pattern representations from the DDP. Additionally, we introduce a Disentanglement-Guided Loss (DGLoss) to ensure that each prototype bank specializes in its designated role while maintaining comprehensive coverage. Comprehensive experiments demonstrate that DPAD consistently improves forecasting performance and reliability of state-of-the-art models across diverse real-world benchmarks.

</details>


### [33] [Provably Robust Bayesian Counterfactual Explanations under Model Changes](https://arxiv.org/abs/2601.16659)
*Jamie Duell,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出Probabilistically Safe CEs (PSCE)方法，生成具有概率安全保证的反事实解释，确保在模型更新时仍保持有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器学习模型频繁更新，现有反事实解释容易失效或不可靠，需要一种能在模型变化下保持有效性的解释方法。

Method: 基于贝叶斯原理的PSCE方法，生成δ-safe（高预测置信度）和ε-robust（低预测方差）的反事实解释，通过不确定性感知约束集成到优化框架中。

Result: PSCE在多个数据集上验证有效，相比现有贝叶斯反事实方法，产生的解释更具合理性、区分性，且在模型变化下具有可证明的鲁棒性。

Conclusion: PSCE为反事实解释提供了形式化的概率保证，解决了模型更新场景下解释失效的问题，提升了实际应用中的可靠性。

Abstract: Counterfactual explanations (CEs) offer interpretable insights into machine learning predictions by answering ``what if?" questions. However, in real-world settings where models are frequently updated, existing counterfactual explanations can quickly become invalid or unreliable. In this paper, we introduce Probabilistically Safe CEs (PSCE), a method for generating counterfactual explanations that are $δ$-safe, to ensure high predictive confidence, and $ε$-robust to ensure low predictive variance. Based on Bayesian principles, PSCE provides formal probabilistic guarantees for CEs under model changes which are adhered to in what we refer to as the $\langle δ, ε\rangle$-set. Uncertainty-aware constraints are integrated into our optimization framework and we validate our method empirically across diverse datasets. We compare our approach against state-of-the-art Bayesian CE methods, where PSCE produces counterfactual explanations that are not only more plausible and discriminative, but also provably robust under model change.

</details>


### [34] [Dynamic Expert-Guided Model Averaging for Causal Discovery](https://arxiv.org/abs/2601.16715)
*Adrick Tench,Thomas Demeester*

Main category: cs.LG

TL;DR: 提出一种利用动态请求专家知识（包括LLM）来集成多种因果发现算法的灵活模型平均方法，在干净和噪声数据上验证了方法有效性


<details>
  <summary>Details</summary>
Motivation: 因果模型对医疗健康至关重要，但因果发现算法众多且缺乏明确最佳选择，同时现实场景常违反算法假设而过度依赖专家知识

Method: 提出灵活的模型平均方法，利用动态请求的专家知识（包括LLM作为专家）来集成多样化的因果发现算法

Result: 实验证明该方法在干净和噪声数据上对包括LLM在内的不完美专家都有效，分析了不同专家正确度的影响，评估了LLM在临床因果发现中的能力

Conclusion: 该方法为实践者提供了有价值的洞见，通过动态专家知识集成有效解决了因果发现算法选择和假设违反的问题

Abstract: Understanding causal relationships is critical for healthcare. Accurate causal models provide a means to enhance the interpretability of predictive models, and furthermore a basis for counterfactual and interventional reasoning and the estimation of treatment effects. However, would-be practitioners of causal discovery face a dizzying array of algorithms without a clear best choice. This abundance of competitive algorithms makes ensembling a natural choice for practical applications. At the same time, real-world use cases frequently face challenges that violate the assumptions of common causal discovery algorithms, forcing heavy reliance on expert knowledge. Inspired by recent work on dynamically requested expert knowledge and LLMs as experts, we present a flexible model averaging method leveraging dynamically requested expert knowledge to ensemble a diverse array of causal discovery algorithms. Experiments demonstrate the efficacy of our method with imperfect experts such as LLMs on both clean and noisy data. We also analyze the impact of different degrees of expert correctness and assess the capabilities of LLMs for clinical causal discovery, providing valuable insights for practitioners.

</details>


### [35] [Sample-wise Constrained Learning via a Sequential Penalty Approach with Applications in Image Processing](https://arxiv.org/abs/2601.16812)
*Francesca Lanzillotta,Chiara Albisani,Davide Pucci,Daniele Baracchi,Alessandro Piva,Matteo Lapucci*

Main category: cs.LG

TL;DR: 提出一种用于深度学习场景的序列惩罚方法，能够在优化问题中严格处理个体数据样本的处理约束，而非仅依赖任意惩罚项。


<details>
  <summary>Details</summary>
Motivation: 在许多学习任务中，对单个数据样本处理的特定要求应该被形式化为优化问题中的严格约束，而不是通过任意惩罚项来处理。当前方法未能充分处理这些约束，需要一种能够严格满足约束的学习方法。

Method: 提出一种序列惩罚方法，该方法能够正确处理优化问题中的约束。算法设计考虑了深度学习场景的特点，并具有收敛性保证。

Result: 实验结果表明，该方法在图像处理任务中确实可行且实用。算法在深度学习场景下具有收敛保证，验证了其实际应用价值。

Conclusion: 序列惩罚方法能够有效处理深度学习中的约束优化问题，具有理论收敛保证和实际应用可行性，为需要严格约束的学习任务提供了有效解决方案。

Abstract: In many learning tasks, certain requirements on the processing of individual data samples should arguably be formalized as strict constraints in the underlying optimization problem, rather than by means of arbitrary penalties. We show that, in these scenarios, learning can be carried out exploiting a sequential penalty method that allows to properly deal with constraints. The proposed algorithm is shown to possess convergence guarantees under assumptions that are reasonable in deep learning scenarios. Moreover, the results of experiments on image processing tasks show that the method is indeed viable to be used in practice.

</details>


### [36] [Calibrated Probabilistic Interpolation for GEDI Biomass](https://arxiv.org/abs/2601.16834)
*Robin Young,Srinivasan Keshav*

Main category: cs.LG

TL;DR: 提出Attentive Neural Processes (ANPs)框架，用于从GEDI稀疏LiDAR观测中生成可靠的墙到墙生物量地图，通过显式建模空间协方差和不确定性校准，优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法（如随机森林、XGBoost）在从GEDI稀疏LiDAR观测插值生成生物量地图时存在局限性：将空间预测视为独立，未适应异质性地形的变化难度，且无法产生校准良好的预测区间。主要问题在于混淆了集成方差与偶然不确定性，并忽略了局部空间上下文。

Method: 引入Attentive Neural Processes (ANPs)，一种概率元学习框架。该方法显式地将预测条件化于局部观测集和地理空间基础模型嵌入。与静态集成不同，ANPs学习灵活的空间协方差函数，使不确定性估计在复杂地形中扩大，在均质区域收缩。通过少样本适应验证方法的操作实用性。

Result: 在五个不同生物群落（从热带亚马逊森林到北方和高山生态系统）进行验证，ANPs在保持接近理想不确定性校准的同时实现了竞争性精度。通过跨区域转移的少样本适应，模型使用最少本地数据恢复了大部分性能差距。

Conclusion: ANPs为大陆尺度地球观测提供了一个可扩展、理论严谨的替代方案，解决了传统集成方差方法的局限性，能够生成可靠且校准良好的生物量预测区间。

Abstract: Reliable wall-to-wall biomass mapping from NASA's GEDI mission requires interpolating sparse LiDAR observations across heterogeneous landscapes. While machine learning approaches like Random Forest and XGBoost are standard for this task, they treat spatial predictions of GEDI observations from multispectral or SAR remote sensing data as independent without adapting to the varying difficulty of heterogeneous landscapes. We demonstrate these approaches generally fail to produce calibrated prediction intervals. We identify that this stems from conflating ensemble variance with aleatoric uncertainty and ignoring local spatial context.
  To resolve this, we introduce Attentive Neural Processes (ANPs), a probabilistic meta-learning framework that explicitly conditions predictions on local observation sets and geospatial foundation model embeddings. Unlike static ensembles, ANPs learn a flexible spatial covariance function, allowing uncertainty estimates to expand in complex landscapes and contract in homogeneous areas. We validate this approach across five distinct biomes ranging from Tropical Amazonian forests to Boreal and Alpine ecosystems, demonstrating that ANPs achieve competitive accuracy while maintaining near-ideal uncertainty calibration. We demonstrate the operational utility of the method through few-shot adaptation, where the model recovers most of the performance gap in cross-region transfer using minimal local data. This work provides a scalable, theoretically rigorous alternative to ensemble variance for continental scale earth observation.

</details>


### [37] [The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics](https://arxiv.org/abs/2601.16849)
*Henri Nikoleit,Ankit Anand,Anurag Murty Naredla,Heiko Röglin*

Main category: cs.LG

TL;DR: 人类-LLM协作在理论计算机科学开放问题中展现出强大能力，通过改进FunSearch算法输出，在组合优化问题上获得了多个领域的最优下界，打破了长期停滞的记录。


<details>
  <summary>Details</summary>
Motivation: 探索人类与大型语言模型（LLM）协作在理论计算机科学研究中的潜力，特别是在组合优化领域，旨在突破长期停滞的问题，验证专家监督如何有效从LLM驱动的进化方法中提取算法洞察。

Method: 采用FunSearch算法作为基础，通过迭代改进其输出，针对特定组合优化问题生成对抗性实例（使标准启发式算法表现不佳的实例），重点关注层次k-中值聚类、装箱问题、背包问题以及Lovász汽油问题的推广。

Result: 在多个组合优化问题上获得了最先进的下界结果：改进了层次k-中值聚类、装箱问题、背包问题以及Lovász汽油问题推广的构造方法，其中一些问题在过去十多年中几乎没有改进，尽管期间有间断性关注。

Conclusion: LLM提供了关键初始模式，但人类专业知识对于将这些模式转化为数学严谨且有洞察力的构造至关重要；LLM是数学和计算机科学研究中的强大协作工具，人类-LLM协作能够突破长期存在的障碍。

Abstract: We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. Specifically, we target the generation of adversarial instances where these heuristics perform poorly. By iterating on FunSearch's outputs, we identify improved constructions for hierarchical $k$-median clustering, bin packing, the knapsack problem, and a generalization of Lovász's gasoline problem - some of these have not seen much improvement for over a decade, despite intermittent attention. These results illustrate how expert oversight can effectively extrapolate algorithmic insights from LLM-based evolutionary methods to break long-standing barriers.
  Our findings demonstrate that while LLMs provide critical initial patterns, human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. This work highlights that LLMs are a strong collaborative tool in mathematics and computer science research.

</details>


### [38] [FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization](https://arxiv.org/abs/2601.16897)
*Antesh Upadhyay,Sang Bin Moon,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: FedSGM是一个统一的联邦约束优化框架，解决了联邦学习中的功能约束、通信瓶颈、本地更新和部分客户端参与四大挑战，采用投影自由、仅需原始变量更新的方法，并提供收敛保证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临四大关键挑战：功能约束（如隐私、公平性等实际约束）、通信瓶颈（带宽限制）、本地更新（减少通信轮次）和部分客户端参与（设备可用性）。现有方法难以同时处理这些挑战，特别是功能约束与其他挑战的结合。

Method: 基于切换梯度方法构建FedSGM框架，采用投影自由、仅需原始变量更新的方法。为处理通信限制，引入双向误差反馈来校正压缩引入的偏差，并明确理解压缩噪声与多步本地更新之间的交互。还提出了软切换版本以稳定可行性边界附近的更新。

Result: 推导出收敛保证，显示平均迭代达到规范的$\mathcal{O}(1/\sqrt{T})$收敛率，并提供额外的高概率边界，将优化进展与部分参与引起的采样噪声解耦。在Neyman-Pearson分类和约束马尔可夫决策过程任务上的实验验证了理论保证。

Conclusion: FedSGM是首个统一处理功能约束、压缩、多步本地更新和部分客户端参与的框架，为约束联邦学习建立了理论基础，并通过实验验证了其有效性。

Abstract: We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provides projection-free, primal-only updates, avoiding expensive dual-variable tuning or inner solvers. To handle communication limits, FedSGM incorporates bi-directional error feedback, correcting the bias introduced by compression while explicitly understanding the interaction between compression noise and multi-step local updates. We derive convergence guarantees showing that the averaged iterate achieves the canonical $\boldsymbol{\mathcal{O}}(1/\sqrt{T})$ rate, with additional high-probability bounds that decouple optimization progress from sampling noise due to partial participation. Additionally, we introduce a soft switching version of FedSGM to stabilize updates near the feasibility boundary. To our knowledge, FedSGM is the first framework to unify functional constraints, compression, multiple local updates, and partial client participation, establishing a theoretically grounded foundation for constrained federated learning. Finally, we validate the theoretical guarantees of FedSGM via experimentation on Neyman-Pearson classification and constrained Markov decision process (CMDP) tasks.

</details>


### [39] [Embedding -based Crop Type Classification in the Groundnut Basin of Senegal](https://arxiv.org/abs/2601.16900)
*Madeline C. Lisaius,Srinivasan Keshav,Andrew Blake,Clement Atzberger*

Main category: cs.LG

TL;DR: 评估TESSERA和AlphaEarth地理空间基础模型嵌入在小农地区作物类型制图中的表现，发现TESSERA在性能、合理性、可迁移性和可访问性方面表现最佳


<details>
  <summary>Details</summary>
Motivation: 现有卫星遥感作物类型制图方法大多不适合小农条件，需要开发适合小农地区的实用嵌入方法

Method: 建立四部分标准（性能、合理性、可迁移性、可访问性），评估TESSERA和AlphaEarth地理空间基础模型嵌入方法，并与基线方法在塞内加尔花生盆地地区进行比较

Result: TESSERA方法在满足选择标准方面表现最佳，在一个时间迁移示例中比次优方法准确率高28%

Conclusion: TESSERA嵌入是塞内加尔作物类型分类和制图任务的有效方法

Abstract: Crop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) transferability and 4) accessibility and evaluate geospatial foundation model (FM) embeddings -based approaches using TESSERA and AlphaEarth against current baseline methods for a region in the groundnut basin of Senegal. We find that the TESSERA -based approach to land cover and crop type mapping fulfills the selection criteria best, and in one temporal transfer example shows 28% higher accuracy compared to the next best method. These results indicate that TESSERA embeddings are an effective approach for crop type classification and mapping tasks in Senegal.

</details>


### [40] [GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints](https://arxiv.org/abs/2601.16905)
*Andy Zhu,Rongzhe Wei,Yupu Gu,Pan Li*

Main category: cs.LG

TL;DR: GRIP框架通过几何约束保护MoE模型的路由稳定性，防止传统遗忘方法利用路由器漏洞进行表面遗忘，确保知识从专家参数中真正擦除。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法无法有效应用于MoE架构，因为它们会利用路由器的架构漏洞——通过操纵路由器将查询从知识专家处重定向，而不是真正擦除知识，导致模型效用损失和表面遗忘。

Method: 提出几何路由不变性保持(GRIP)框架，核心是几何约束：通过将路由器梯度更新投影到专家特定的零空间中实现。这解耦了路由稳定性与参数刚性：离散专家选择保持稳定以保留知识，连续路由器参数在零空间内保持可塑性，允许模型进行必要的内部重构以满足遗忘目标。

Result: 在大规模MoE模型上的广泛实验表明，GRIP适配器消除了专家选择偏移（实现超过95%的路由稳定性），同时保持了所有测试遗忘方法的效用。通过防止现有算法利用MoE模型的路由器漏洞，GRIP将现有遗忘研究从密集架构适配到MoE架构。

Conclusion: GRIP作为一个算法无关的适配器框架，通过几何约束保护MoE模型的路由稳定性，强制遗忘优化直接从专家参数中擦除知识，而不是利用表面路由器操纵的捷径，从而实现了对MoE架构的有效机器学习遗忘。

Abstract: Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model's router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs.

</details>


### [41] [The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning](https://arxiv.org/abs/2601.16906)
*Calarina Muslimani,Yunshu Du,Kenta Kawamoto,Kaushik Subramanian,Peter Stone,Peter Wurman*

Main category: cs.LG

TL;DR: TAC作为奖励函数评估指标，既能指导人工调优提升性能，又能作为可微损失函数直接学习奖励模型


<details>
  <summary>Details</summary>
Motivation: 强化学习依赖准确反映任务目标的奖励函数，但人工设计奖励函数耗时且易出错。需要工具支持RL从业者设计合适的奖励权重，并探索自动学习奖励模型的方法。

Method: 1) 利用轨迹对齐系数(TAC)评估奖励函数与领域专家偏好的匹配程度；2) 在Lunar Lander中进行人因研究，比较有/无TAC指导的奖励调优效果；3) 提出Soft-TAC作为TAC的可微近似，用作损失函数从人类偏好数据训练奖励模型；4) 在Gran Turismo 7赛车模拟器中验证Soft-TAC。

Result: 1) TAC指导下的奖励调优产生更高性能的奖励函数，参与者报告认知负荷更低；2) 但即使有TAC，人工奖励设计仍显繁琐；3) Soft-TAC训练的奖励模型成功捕捉偏好特定目标，相比标准交叉熵损失产生更具区分性的策略行为。

Conclusion: TAC既能作为指导奖励调优的实用工具，也能作为复杂领域中奖励学习的目标函数，为解决奖励函数设计问题提供了双重解决方案。

Abstract: The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajectory Alignment Coefficient (TAC), a metric that evaluates how closely a reward function's induced preferences match those of a domain expert. To evaluate whether TAC provides effective support in practice, we conducted a human-subject study in which RL practitioners tuned reward weights for Lunar Lander. We found that providing TAC during reward tuning led participants to produce more performant reward functions and report lower cognitive workload relative to standard tuning without TAC. However, the study also underscored that manual reward design, even with TAC, remains labor-intensive. This limitation motivated our second goal: to learn a reward model that maximizes TAC directly. Specifically, we propose Soft-TAC, a differentiable approximation of TAC that can be used as a loss function to train reward models from human preference data. Validated in the racing simulator Gran Turismo 7, reward models trained using Soft-TAC successfully captured preference-specific objectives, resulting in policies with qualitatively more distinct behaviors than models trained with standard Cross-Entropy loss. This work demonstrates that TAC can serve as both a practical tool for guiding reward tuning and a reward learning objective in complex domains.

</details>


### [42] [Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces](https://arxiv.org/abs/2601.16907)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 提出一种基于等渗回归的单调校准方法，解决预训练嵌入空间中余弦相似度的各向异性导致的绝对数值校准问题，同时保持排序相关性和局部稳定性。


<details>
  <summary>Details</summary>
Motivation: 预训练嵌入空间中的原始余弦相似度虽然与人类判断有强排序相关性，但由于各向异性导致绝对数值系统性地校准错误：无论实际语义相关性如何，分数都集中在狭窄的高相似度区间，限制了其作为定量度量的可解释性。先前工作通过修改嵌入空间（白化、对比微调）来解决，但这些变换会改变几何结构并需要重新计算所有嵌入。

Method: 使用基于人类相似度判断训练的等渗回归，构建单调变换，实现近乎完美的校准，同时保持排序相关性和局部稳定性（在七种扰动类型中达到98%）。该方法将等渗校准表征为保序重参数化，并证明所有基于顺序的构造（角度排序、最近邻、阈值图和基于分位数的决策）在此变换下保持不变。

Result: 该方法在保持排序相关性的同时实现了近乎完美的校准，局部稳定性在七种扰动类型中达到98%。等渗校准作为保序重参数化，确保所有基于顺序的构造保持不变。

Conclusion: 本文贡献不是替代余弦相似度，而是通过单调校准恢复其绝对数值的可解释性，而不改变其排序特性。等渗校准提供了一种无需修改嵌入空间几何结构或重新计算嵌入的解决方案。

Abstract: While raw cosine similarity in pretrained embedding spaces exhibits strong rank correlation with human judgments, anisotropy induces systematic miscalibration of absolute values: scores concentrate in a narrow high-similarity band regardless of actual semantic relatedness, limiting interpretability as a quantitative measure. Prior work addresses this by modifying the embedding space (whitening, contrastive fine tuning), but such transformations alter geometric structure and require recomputing all embeddings.
  Using isotonic regression trained on human similarity judgments, we construct a monotonic transformation that achieves near-perfect calibration while preserving rank correlation and local stability(98% across seven perturbation types). Our contribution is not to replace cosine similarity, but to restore interpretability of its absolute values through monotone calibration, without altering its ranking properties.
  We characterize isotonic calibration as an order-preserving reparameterization and prove that all order-based constructions (angular ordering, nearest neighbors, threshold graphs and quantile-based decisions) are invariant under this transformation.

</details>


### [43] [Group-realizable multi-group learning by minimizing empirical risk](https://arxiv.org/abs/2601.16922)
*Navid Ardeshir,Samuel Deng,Daniel Hsu,Jingwen Liu*

Main category: cs.LG

TL;DR: 多组学习在组可实现设置下的样本复杂度优于不可知设置，即使组族具有无限VC维；但实现该方法计算不可行，建议使用非恰当学习替代方案


<details>
  <summary>Details</summary>
Motivation: 研究多组学习在不同设置下的样本复杂度差异，特别是探索组可实现设置相对于不可知设置的潜在优势，即使面对无限组族的情况

Method: 通过经验风险最小化在组可实现概念类上进行学习，即使该类可能具有无限VC维；同时分析计算可行性并提出基于非恰当学习的替代方法

Result: 在组可实现设置下，即使组族具有无限VC维，多组学习的样本复杂度仍优于不可知设置；但实现该方法被证明是计算不可行的

Conclusion: 组可实现设置为多组学习提供了样本复杂度优势，但需要采用非恰当学习等替代方法来克服计算不可行性问题

Abstract: The sample complexity of multi-group learning is shown to improve in the group-realizable setting over the agnostic setting, even when the family of groups is infinite so long as it has finite VC dimension. The improved sample complexity is obtained by empirical risk minimization over the class of group-realizable concepts, which itself could have infinite VC dimension. Implementing this approach is also shown to be computationally intractable, and an alternative approach is suggested based on improper learning.

</details>


### [44] [Auto-Regressive Masked Diffusion Models](https://arxiv.org/abs/2601.16971)
*Mahdi Karami,Ali Ghodsi*

Main category: cs.LG

TL;DR: ARMD模型通过将掩码扩散过程重构为块级因果模型，统一了自回归模型的训练效率和扩散模型的并行生成能力，在语言建模基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）在语言建模中存在性能差距，需要更多训练迭代。需要一种既能保持自回归模型训练效率，又能实现扩散模型并行生成能力的架构。

Method: 提出自回归掩码扩散（ARMD）模型，将掩码扩散过程重构为块级因果模型。设计严格因果、置换等变架构，在单个并行前向传递中计算所有条件概率。支持高效的自回归式解码和渐进置换训练方案，学习从左到右和随机标记顺序。引入跨步并行生成策略，在并行流中生成标记同时保持全局一致性。

Result: ARMD在标准语言建模基准上达到最先进性能，优于现有扩散基线且需要显著更少的训练步骤。为并行文本生成建立了新基准，有效弥合了并行和顺序解码之间的性能差距。

Conclusion: ARMD模型成功统一了自回归模型的训练效率和扩散模型的并行生成能力，通过块级因果建模和并行生成策略，在语言建模中实现了性能突破。

Abstract: Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.

</details>


<div id='Xuanhe Zhou'></div>

# Xuanhe Zhou [[Back]](#toc)

### [45] [FraudFusion: A Multi-Modal Ensemble Framework for Anomaly-Based Fraud Detection in Retail POS Systems](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/11357914.pdf&hl=zh-CN&sa=X&d=3074489902817397630&ei=Xgd4aee7CvDB6rQPsMjv6As&scisig=AHkA5jRqOAwJ-V5OUDiv27UzBQIC&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=0&folt=cit)
*LN Mishra,B Senapati*

Main category: Xuanhe Zhou

TL;DR: 该论文针对零售POS系统日益复杂的欺诈活动，提出了一种多模态机器学习方法来提升欺诈检测能力，超越传统规则和单模态模型的局限。


<details>
  <summary>Details</summary>
Motivation: 针对零售POS系统的欺诈活动日益复杂化，传统基于规则的防御机制和单模态机器学习模型难以捕捉复杂且不断演变的欺诈模式，导致数字经济因POS欺诈造成的财务损失不断增加。

Method: 论文提出了一种多模态机器学习方法，通过整合多种数据源和特征来捕捉复杂的欺诈模式，超越传统的单模态模型。

Result: 该方法能够更有效地检测POS系统中的复杂欺诈活动，减少财务损失，提升欺诈检测的准确性和适应性。

Conclusion: 多模态机器学习方法为零售POS系统欺诈检测提供了更有效的解决方案，能够应对日益复杂的欺诈模式，保护数字经济免受财务损失。

Abstract: Fraudulent activities targeting retail Point-of-Sale (POS) systems are becoming increasingly sophisticated and surpassing the capabilities of predominant rule-based or single-modal machine learning model-based defense mechanisms, as they struggle to capture the complex and evolving fraudulent activity patterns. As a result, the digital economy is experiencing a growing concern originating from increasing financial losses based on fraudulent activities at retail POS systems. This paper …

</details>


### [46] [Hybrid MCTS and Greedy Beam Search for Chunk Optimization in RAG](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11333696/&hl=zh-CN&sa=X&d=10412094228447644945&ei=Xgd4aee7CvDB6rQPsMjv6As&scisig=AHkA5jRCuOYz4twDT1FE9QHpO2mg&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=1&folt=cit)
*S Farooq,MR Shakoori,N Iltaf,IU Islam*

Main category: Xuanhe Zhou

TL;DR: 提出了一种基于贪心波束搜索的新型RAG框架，解决传统方法中块排序、非单调效用和组合爆炸问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以获取最新信息，容易产生幻觉。现有检索增强生成方法存在块排序次优、块效用非单调以及块组合指数增长导致的效率低下等问题

Method: 提出一种新型RAG框架，利用贪心波束搜索算法优化检索过程，解决块排序和组合效率问题

Result: 该方法能够更有效地检索相关外部数据，减少幻觉，提高生成质量

Conclusion: 提出的基于贪心波束搜索的RAG框架解决了现有方法的局限性，为大语言模型提供更准确、高效的信息检索增强能力

Abstract: Large Language Models (LLMs) face challenges in accessing up-to-date information, often leading to hallucinations in their responses. Retrieval-Augmented Generation (RAG) solves this problem by retrieving relevant external data, but existing methods struggle with suboptimal chunk ordering, non-monotonic chunk utility, and inefficiencies due to the exponential growth of chunk combinations. To solve these issues, we propose a novel RAG framework that leverages Greedy Beam …

</details>


### [47] [SafeSQL-LLM: A Synthetic Data Approach to Privacy-Preserving Text-to-SQL](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11323205/&hl=zh-CN&sa=X&d=11570170354182543777&ei=Xgd4aee7CvDB6rQPsMjv6As&scisig=AHkA5jQNpnDk3KQ08zNK926J71Ry&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=3&folt=cit)
*R Zhang,S Zhao*

Main category: Xuanhe Zhou

TL;DR: SafeSQLLLM是一个保护隐私的文本到SQL混合框架，使用本地LLM生成合成表行来统计模拟原始数据分布，通过安全检查防止真实数据泄露，在保持查询准确性的同时解决云托管模型查询敏感表时的隐私问题。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的文本到SQL系统虽然实现了对结构化数据库的自然语言访问，但通过云托管模型查询敏感表格会引发严重的隐私担忧。需要一种既能保护隐私又不牺牲查询准确性的解决方案。

Method: 采用混合框架，使用本地LLM生成统计上模拟原始数据分布的合成表行，并通过安全检查验证这些合成数据，防止真实数据泄露，从而在保护隐私的同时保持查询功能。

Result: SafeSQLLLM框架能够在保护敏感数据隐私的同时，保持文本到SQL查询的准确性，解决了云托管模型访问敏感数据库时的隐私风险问题。

Conclusion: 提出的混合框架为文本到SQL系统提供了一种有效的隐私保护方案，通过本地生成合成数据替代真实敏感数据，在隐私保护和查询准确性之间取得了良好平衡。

Abstract: Text-to-SQL systems powered by large language models (LLMs) enable intuitive natural language access to structured databases, but querying sensitive tables via cloud-hosted models raises critical privacy concerns. We present SafeSQLLLM, a hybrid framework that preserves privacy without sacrificing query accuracy. Our approach uses a local LLM to generate synthetic table rows that statistically mimic the original data distribution, validates them through safety checks to prevent real …

</details>


### [48] [Geospatial Computing from Data Lakes to Deep Learning Applications](https://scholar.google.com/scholar_url?url=https://search.proquest.com/openview/3a6151dee2edf6c3782ee0cf5bb4b70f/1%3Fpq-origsite%3Dgscholar%26cbl%3D18750%26diss%3Dy&hl=zh-CN&sa=X&d=5531817598137605121&ei=Xgd4aee7CvDB6rQPsMjv6As&scisig=AHkA5jRS0GpbVovBi76pYqtpRh3y&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=4&folt=cit)
*M Saeedan*

Main category: Xuanhe Zhou

TL;DR: 该论文探讨地理空间矢量数据（点、线、多边形等几何形状）及其在导航、城市规划等应用中的重要性，研究处理大规模地理空间数据集的技术和工具。


<details>
  <summary>Details</summary>
Motivation: 地理空间矢量数据在导航、城市规划等众多应用中具有关键作用，但处理大规模地理空间数据集面临技术挑战，需要开发专门的技术和工具来高效管理和分析这些数据。

Method: 论文采用多学科研究方法，结合计算机科学、地理信息系统和数据分析等领域知识，开发处理地理空间矢量数据的技术和工具，重点关注几何形状数据的表示、存储、查询和分析方法。

Result: 论文建立了地理空间计算的理论框架和技术体系，提出了处理大规模地理空间矢量数据的有效方法，为导航、城市规划等应用提供了技术支持。

Conclusion: 地理空间计算作为一个多学科交叉领域，对于处理和分析大规模地理空间矢量数据至关重要，相关技术和工具的开发将推动导航、城市规划等应用的进一步发展。

Abstract: This thesis explores geospatial vector data, including geometric shapes such as points, lines, and polygons. This data is crucial in navigation, urban planning, and many more applications. Geospatial computing is a multidisciplinary field that focuses on creating techniques and tools to handle large geospatial datasets.

</details>


### [49] [SEMU: Concurrency-Optimized High-Performance Cache Management for Key-Value Caches](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11298510/&hl=zh-CN&sa=X&d=5627343096795225548&ei=Xgd4aee7CvDB6rQPsMjv6As&scisig=AHkA5jRKfCj8D30lEZxhpSVFHruJ&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=5&folt=cit)
*X Wang,P Jin,Y Yuan*

Main category: Xuanhe Zhou

TL;DR: 提出一种新的缓存替换策略，在保持LRU时间局部性的同时改善并发性能


<details>
  <summary>Details</summary>
Motivation: 软件管理缓存在现代应用中很重要，但LRU策略因频繁列表更新导致并发性能差，而现有并发感知方案如FIFO牺牲了LRU的时间局部性，导致命中率下降

Method: 未在摘要中明确描述，但从上下文推断可能提出了一种结合LRU时间局部性和并发性能优化的新缓存替换算法

Result: 摘要未提供具体结果，但暗示该方法能够改善缓存效率

Conclusion: 需要一种既能保持LRU时间局部性又能改善并发性能的缓存替换策略

Abstract: Improving software-managed cache efficiency is an important issue for various modern applications. Although LRU (Least Recently Used) is widely used as the default replacement policy in many key-value caches, it suffers from poor concurrency performance due to frequent list updates. However, existing concurrency-aware schemes like FIFO sacrifice the LRU's temporal locality of data access, leading to a drop in the cache's hit ratio. In this work, aiming to improve the …

</details>


### [50] [GroQ SQL Travel Planner: Book Flights, Hotels, Tours, Weather‑Based Recommendations and Chatbot Support with Firebase Login and SuperBase Captcha](https://scholar.google.com/scholar_url?url=https://www.scitepress.org/Papers/2025/139433/139433.pdf&hl=zh-CN&sa=X&d=16197203608263281323&ei=Xgd4aee7CvDB6rQPsMjv6As&scisig=AHkA5jQyq36p6oXBXWjq-lH5hNZ4&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=6&folt=cit)
*G Ramasamy,M Gurupriya,G Rohan,V Chowdary…*

Main category: Xuanhe Zhou

TL;DR: GroQ SQL旅游规划器是一个在线门户，集成了航班预订、酒店预订、导游服务、基于天气的推荐和聊天机器人支持等功能，使用React.js前端、MySQL数据库、Node.js/Express后端，以及Firebase认证和Google登录。


<details>
  <summary>Details</summary>
Motivation: 开发一个综合性的旅游规划平台，整合旅行相关的多种服务功能，为用户提供一站式旅行规划和管理体验，同时确保系统的安全性和用户体验的流畅性。

Method: 采用React.js构建前端用户界面，MySQL作为数据库管理系统，Node.js和Express框架构建后端服务，Firebase提供用户认证和Google登录功能，Superbase用于防止机器人访问。

Result: 开发了一个功能完整的旅游规划门户，具备航班预订、酒店预订、导游服务、天气推荐和聊天机器人支持等核心功能，实现了前后端分离的现代化Web应用架构。

Conclusion: GroQ SQL旅游规划器成功整合了多种旅行相关服务，通过现代Web技术栈实现了功能全面、用户体验良好的在线旅行规划平台。

Abstract: GroQ SQL travel planner is an online portal encompassing all the important features to make and manage a scheduled trip such as flight booking, hotel reservation, guided tours, with weather-based recommendations and a chatbot support. For a seamless user experience, this application employs a React. js frontend, a MySQL database, and Node. js and Express for the backend. For authentication, Firebase provides Authentication and Google Sign-in and to avoid bot access Superbase …

</details>


### [51] [Evaluating and Achieving Controllable Code Completion in Code LLM](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.15879&hl=zh-CN&sa=X&d=7626674770956427670&ei=XQd4aaHUCNaOieoPnNOJuQQ&scisig=AHkA5jQmAC0LTe7Bx2tcma3lVQPC&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=0&folt=rel)
*J Zhang,Z Cui,L Zhang,J Yang,J Yang,Q Liu…*

Main category: Xuanhe Zhou

TL;DR: 该论文探讨了基于大语言模型的代码补全任务，重点关注评估方法、基准测试和未来研究方向


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程中的广泛应用，代码补全成为核心任务。尽管LLM的代码补全能力已有显著提升，但当前评估方法存在局限性，需要更全面、系统的评估框架来推动该领域发展

Method: 论文可能采用文献综述、系统分析现有评估方法、提出新的评估框架或基准测试的方法。可能包括对现有代码补全评估指标的批判性分析，以及设计更全面的评估体系

Result: 论文可能发现当前代码补全评估存在以下问题：评估指标单一、基准测试不全面、缺乏现实场景考量等。可能提出新的评估框架或基准测试，为更准确的代码补全能力评估提供指导

Conclusion: 需要建立更系统、全面的代码补全评估体系，包括多样化的评估指标、更贴近实际开发场景的基准测试，以及考虑代码质量、安全性和可维护性等多维度评估标准

Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation …

</details>


### [52] [DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03178&hl=zh-CN&sa=X&d=5354146339337000068&ei=XQd4aaHUCNaOieoPnNOJuQQ&scisig=AHkA5jTe8GlVhBvamD9fQvcqDuM5&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=1&folt=rel)
*H Zhu,P Yang,J Wang,J Liu,Z Liu,D Li,Y Fang…*

Main category: Xuanhe Zhou

TL;DR: 扩散模型在图像和视频生成方面取得了显著成功，但其多步推理过程带来了巨大的计算开销，阻碍了实际部署。本文提出了一种加速扩散模型推理的方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量优秀，但需要多次迭代推理，计算成本高，难以在实时应用或资源受限环境中部署。需要开发高效的推理加速技术来克服这一瓶颈。

Method: 从摘要内容推断，可能涉及蒸馏技术、模型压缩、减少推理步数、架构优化或知识迁移等方法，旨在保持生成质量的同时显著降低计算复杂度。

Result: 预期能够实现推理速度的显著提升，同时保持与原始模型相当的生成质量，可能达到数量级的加速效果。

Conclusion: 扩散模型加速技术对于实际应用至关重要，通过优化推理过程可以在保持生成质量的同时大幅降低计算成本，促进扩散模型在现实世界中的广泛部署。

Abstract: Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion …

</details>


### [53] [From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.15690&hl=zh-CN&sa=X&d=10416968939142200042&ei=XQd4aaHUCNaOieoPnNOJuQQ&scisig=AHkA5jSwjiGYLdXfAjeTdhvQndyy&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=2&folt=rel)
*J Zhang,W Cui,Z Li,L Huang,B Malin,C Xiong…*

Main category: Xuanhe Zhou

TL;DR: 该论文是关于解决大型语言模型不可靠性问题的功能演进综述，探讨了从被动评估到主动增强的演变路径


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然表现出色，但其不可靠性仍是高风险领域部署的关键障碍，需要系统性的解决方案演进

Method: 采用文献综述方法，分析LLM可靠性研究的功能演进路径，从被动评估到主动增强的系统性框架

Result: 提出了LLM可靠性研究的演进框架，展示了从评估到增强的完整技术发展路径

Conclusion: LLM可靠性研究正在从被动评估向主动增强演进，这为解决模型不可靠性问题提供了系统性框架

Abstract: While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of …

</details>


### [54] [Large Language Models Are Multitask Chain-of-Thought Prompting Optimizers](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11318325/&hl=zh-CN&sa=X&d=977148780346769530&ei=XQd4aaHUCNaOieoPnNOJuQQ&scisig=AHkA5jTnK4r7SErkBVHW6j1UTF1q&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=3&folt=rel)
*F Jin,Y Tan*

Main category: Xuanhe Zhou

TL;DR: LLMs在推理任务上表现出色，但输出质量对提示设计高度敏感，需要精心设计的提示来引导模型生成高质量输出


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种推理基准测试中取得了显著性能，但其输出质量仍然对提示设计极为敏感。精心设计的提示可以引导LLM生成更高质量的推理结果，这突显了提示工程在LLM应用中的关键作用。

Method: 从摘要内容来看，论文可能探讨了提示设计对LLM推理性能的影响，可能涉及系统化的提示工程方法、提示优化策略或评估不同提示设计对模型输出的影响。

Result: 摘要暗示精心设计的提示能够显著提升LLM的推理输出质量，表明提示工程是解锁LLM潜力的关键因素，但具体实验结果需要查看完整论文。

Conclusion: 提示设计对大语言模型的推理性能具有决定性影响，系统化的提示工程方法对于充分发挥LLM的推理能力至关重要。

Abstract: Large language models (LLMs) have achieved striking performance across a broad range of reasoning benchmarks, yet the quality of their outputs remains acutely sensitive to prompt design. A meticulously engineered prompt can coax an LLM into …

</details>


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [55] [Synopsis-Alloyed Index for Exact and Approximate Query Processing](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/11323511/11316635.pdf&hl=zh-CN&sa=X&d=14646791791491727212&ei=XAd4ab7qIvStieoP-8T7kA0&scisig=AHkA5jRPszudbQLTH4RygZXN4FwX&oi=scholaralrt&hist=i6heNjgAAAAJ:8133205940682390297:AHkA5jQfTAaKkXi3kBQuJYouRHpx&html=&pos=0&folt=rel)
*M Takata,H Yuasa,K Goda*

Main category: Ziniu Wu

TL;DR: B+-树等传统数据结构在分析场景中面临查询速度瓶颈，需要新的索引结构来支持快速近似查询


<details>
  <summary>Details</summary>
Motivation: 传统结构化数据库系统（如使用B+-树）虽然能加速选择性查询，但在分析场景中用户越来越重视查询速度，并愿意接受高近似精度来换取更快的响应时间。现有索引结构无法有效支持这种近似查询需求。

Method: 论文提出了一种新的索引结构设计，专门针对分析场景的近似查询需求进行优化。该方法可能涉及概率数据结构、近似索引或查询加速技术，在保证高精度的同时显著提升查询速度。

Result: 新索引结构相比传统B+-树等数据结构，在分析查询场景中能够提供显著更快的查询速度，同时保持用户可接受的高近似精度水平。

Conclusion: 针对分析场景的近似查询需求，需要专门设计的索引结构来平衡查询速度与精度，传统B+-树等结构已无法满足现代分析工作负载的性能要求。

Abstract: Structured database systems, which organize business data, widely adopt data structures like a B+-tree to speed up selective queries. However, users in many analytical scenarios increasingly value query speed and are satisfied with high …

</details>


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [56] [TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08743&hl=en&sa=X&d=1335762194997350080&ei=XQd4abucGYeUywS5-prpDw&scisig=AHkA5jRNLzZhj5cMW-xdjf_c7I15&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=0&folt=rel)
*J Su,Y Hu,C Li,H Chen,J Li,L Ma,J Zhang*

Main category: Google Scholar

TL;DR: 针对Text-to-SQL任务中LLM方法包含完整数据库模式导致上下文过长和延迟增加的问题，提出利用用户查询通常聚焦于重复表集的特点进行优化


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的Text-to-SQL方法通常将完整数据库模式包含在提示中，导致上下文长度过长、预填充延迟增加。然而用户查询通常只关注数据库中重复出现的表集，这为优化提供了机会。

Method: 未在摘要中明确说明具体方法，但暗示将利用用户查询聚焦于重复表集的特点，可能通过表选择、模式剪枝或上下文优化来减少提示长度。

Result: 摘要未提供具体实验结果，但暗示该方法有望减少上下文长度和预填充延迟。

Conclusion: 通过利用用户查询聚焦于重复表集的特点，可以优化Text-to-SQL任务的提示设计，减少上下文长度和计算延迟。

Abstract: In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for …

</details>


### [57] [SQL Statement Generation Enhanced Through the Fusion of Large Language Models and Knowledge Graphs](https://scholar.google.com/scholar_url?url=https://www.mdpi.com/2079-9292/15/2/278&hl=en&sa=X&d=6977093582047777376&ei=XQd4abucGYeUywS5-prpDw&scisig=AHkA5jTbwxbjGk5YH2IU3_EgdSK4&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=1&folt=rel)
*B Wang,X Yu,X Zheng*

Main category: Google Scholar

TL;DR: 论文针对当前SQL生成方法在捕捉结构化数据语义信息和处理复杂查询任务方面的不足，提出了一种新的SQL生成框架，旨在解决大语言模型中的幻觉问题和准确率下降挑战。


<details>
  <summary>Details</summary>
Motivation: 当前主流的SQL生成方法在捕捉结构化数据语义信息和处理复杂查询任务方面存在不足，特别是在大语言模型中存在幻觉问题和准确率下降的挑战。

Method: 提出了一种新的SQL生成框架，该框架能够更好地捕捉结构化数据的语义信息，并有效处理复杂查询任务，以解决大语言模型中的幻觉和准确率问题。

Result: 该方法在SQL生成任务中表现出更好的性能，能够更准确地生成SQL查询语句，特别是在处理复杂查询时减少了幻觉现象并提高了准确率。

Conclusion: 提出的SQL生成框架有效解决了当前方法在语义信息捕捉和复杂查询处理方面的局限性，为SQL生成任务提供了更可靠和准确的解决方案。

Abstract: Current mainstream SQL generation approaches remain insufficient in capturing the semantic information of structured data and handling complex query tasks. To address the challenges of hallucination and accuracy degradation in large language …

</details>


### [58] [DMTree: Towards Efficient Tree Indexing on Disaggregated Memory via Compute-side Collaborative Design](https://scholar.google.com/scholar_url?url=https://www.usenix.org/system/files/conf%25C3%25A9rence/fast26/fast26spring-prepub_wei.pdf&hl=en&sa=X&d=3279692533362634135&ei=XQd4abucGYeUywS5-prpDw&scisig=AHkA5jT5ZB7OhC67n71ulKnvWftX&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=2&folt=rel)
*G Wei,Y Li,H Song,T Li,L Yao,Y Xu,H Cui*

Main category: Google Scholar

TL;DR: 论文探讨了在分解式内存架构中设计高效范围索引的挑战与解决方案


<details>
  <summary>Details</summary>
Motivation: 分解式内存架构将计算和内存资源分离为独立的资源池，提高了资源利用率和可扩展性，但这种新架构给范围索引设计带来了根本性挑战

Method: 未在摘要中明确说明，但推测论文提出了针对分解式内存架构优化的范围索引设计方法

Result: 未在摘要中明确说明，但推测论文展示了在分解式内存架构中实现高效范围索引的性能结果

Conclusion: 未在摘要中明确说明，但推测论文得出结论：需要重新设计范围索引以适应分解式内存架构的特性

Abstract: Disaggregated memory (DM) separates computing and memory resources into distinct resource pools, enhancing resource utilization and scalability. However, this new architecture presents fundamental design challenges on range indexes …

</details>


### [59] [Pangaea v2: CXL-based Disaggregated Memory System Architecture for Cloud-Native Orchestration](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11357541/&hl=en&sa=X&d=7965559730952223577&ei=XQd4abucGYeUywS5-prpDw&scisig=AHkA5jQD7teiAsQs41HdMO9T2IS_&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=3&folt=rel)
*HD Lee,J Park,Y Lee,J Im,J Jung,J So,S Tavallaei…*

Main category: Google Scholar

TL;DR: 数据中心资源利用率低的问题催生了分解式系统架构，通过独立扩展CPU和内存资源来解决资源错配问题


<details>
  <summary>Details</summary>
Motivation: 当前数据中心由于为最坏情况过度配置资源，导致CPU和内存资源利用率低下，存在资源错配问题

Method: 提出分解式系统架构，允许CPU和内存资源独立扩展，打破传统服务器中CPU和内存的固定绑定关系

Result: 通过资源解耦，能够更高效地利用数据中心资源，减少资源浪费，提高整体资源利用率

Conclusion: 分解式系统架构是解决数据中心资源错配问题的有效方案，能够显著提升资源利用效率

Abstract: Today's data centers suffer from CPU and memory resource stranding because they often over-provision resources when deploying servers for worst-case scenarios. This problem gives rise to a disaggregated system architecture allowing each type of …

</details>


### [60] [ORBITFLOW: SLO-Aware Long-Context LLM Serving with Fine-Grained KV Cache Reconfiguration](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.10729&hl=en&sa=X&d=5345457422684473681&ei=XQd4abucGYeUywS5-prpDw&scisig=AHkA5jTdHsaUotKNG-1eZCVkncRP&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=4&folt=rel)
*X Ma,H Hong,T Um,J Lee,S Choy,WY Lee,M Jeon*

Main category: Google Scholar

TL;DR: 长上下文LLM服务面临内存波动挑战，KV缓存卸载至主机内存受带宽限制，提出基于预测的弹性KV缓存管理方法


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM服务中，请求长度和批次组合在token生成过程中不断变化，导致运行时内存占用大幅波动。将KV缓存卸载到主机内存受到PCIe带宽限制，影响推理性能。需要一种能够动态适应内存需求变化的KV缓存管理方法。

Method: 提出基于预测的弹性KV缓存管理方法，通过预测未来内存需求来动态调整KV缓存的分配和释放策略。该方法能够根据请求特征和生成过程的变化，智能地管理GPU和主机内存之间的KV缓存分布。

Result: 该方法显著减少了内存波动对推理性能的影响，提高了长上下文LLM服务的吞吐量和响应时间。相比传统固定分配策略，能够更好地适应动态工作负载。

Conclusion: 弹性KV缓存管理是解决长上下文LLM服务内存波动问题的有效方法，通过预测驱动的动态资源分配，能够在保持高性能的同时处理变长请求。

Abstract: Serving long-context LLMs is challenging because request lengths and batch composition vary during token generation, causing the memory footprint to fluctuate significantly at runtime. Offloading KV caches to host memory limits effective memory …

</details>


### [61] [RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.04853&hl=en&sa=X&d=18397412137694735659&ei=XQd4abucGYeUywS5-prpDw&scisig=AHkA5jRIkFctgN2tv6wWQKvQrVpR&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=6&folt=rel)
*Z Liu,R Guo,B Qu,Y Jiang,M Peng,Q Xie…*

Main category: Google Scholar

TL;DR: 跨领域虚假信息检测面临挑战，现有方法依赖单一视角线索，难以泛化到新领域


<details>
  <summary>Details</summary>
Motivation: 虚假信息在不同领域出现，各领域知识和话语存在显著差异，现有检测方法难以有效应对跨领域场景

Method: 未在摘要中明确说明，但暗示需要超越单一视角的方法来应对跨领域挑战

Result: 未在摘要中提供具体结果

Conclusion: 需要开发更有效的跨领域虚假信息检测方法

Abstract: Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to …

</details>


### [62] [CORE-T: COherent REtrieval of Tables for Text-to-SQL](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13111&hl=en&sa=X&d=2070382532333428456&ei=XQd4abucGYeUywS5-prpDw&scisig=AHkA5jRQPRqC3rtGo_1-vep3m-X0&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=7&folt=rel)
*H Soliman,V Gupta,D Roth,I Gurevych*

Main category: Google Scholar

TL;DR: 研究多表连接场景下的文本到SQL转换，提出基于检索增强的方法解决表选择瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 现实中的文本到SQL工作流通常需要连接多个表，准确检索相关表集合成为端到端性能的关键瓶颈

Method: 采用开放书籍设置，查询必须基于检索到的相关表来回答，提出检索增强的方法来准确识别和选择相关表

Result: 通过改进表检索机制，显著提升了多表连接场景下文本到SQL系统的端到端性能

Conclusion: 有效的表检索是多表连接文本到SQL系统的关键组件，检索增强方法能够显著改善系统性能

Abstract: Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered …

</details>


### [63] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.10398&hl=en&sa=X&d=4731576465419772968&ei=XQd4abucGYeUywS5-prpDw&scisig=AHkA5jRt7nuDiGOoQkvvcMDm-n34&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=8&folt=rel)
*X Ren,S Hu,Z Lu,J Huang,Q Duan*

Main category: Google Scholar

TL;DR: 该论文关注LLM-based文本到SQL系统中，不可回答和未充分指定的用户查询可能生成错误文本和可执行程序，导致误导性结果或违反安全约束的问题。


<details>
  <summary>Details</summary>
Motivation: LLM-based文本到SQL系统中，不可回答和未充分指定的用户查询会生成错误SQL代码，这些代码不仅产生不正确文本，还能执行并产生误导性结果或违反安全约束，构成安全部署的主要障碍。

Method: 论文未在摘要中明确描述具体方法，但暗示需要解决LLM-based文本到SQL系统中不可回答和未充分指定查询带来的安全风险问题。

Result: 摘要未提供具体实验结果，但指出不可回答和未充分指定的查询会导致生成可执行的错误SQL程序，产生误导性结果并违反安全约束。

Conclusion: LLM-based文本到SQL系统中，处理不可回答和未充分指定的用户查询是确保系统安全部署的关键挑战，需要有效解决方案来防止生成误导性结果和违反安全约束。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe …

</details>


### [64] [Linking Cryptoasset Attribution Tags to Knowledge Graph Entities: An LLM-Based Approach](https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DH5ylEQAAQBAJ%26oi%3Dfnd%26pg%3DPA366%26ots%3De2bAxQ60FV%26sig%3DoH4WSHeI0NPtoduIolV39puzvcQ&hl=en&sa=X&d=2169516983021448480&ei=XQd4abucGYeUywS5-prpDw&scisig=AHkA5jQ1l_-WE8bNM6pFGxqSZwz1&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=9&folt=rel)
*B Haslhofer*

Main category: Google Scholar

TL;DR: 提出基于计算方法的加密资产取证标签验证框架，解决标签不一致和错误问题


<details>
  <summary>Details</summary>
Motivation: 现代加密资产取证严重依赖属性标签，但不一致或错误的标签会误导调查甚至导致错误指控，需要系统性的验证方法

Method: 提出一种新颖的计算方法，基于...（具体方法未在摘要中详细说明）

Result: 未在摘要中明确说明具体结果

Conclusion: 该方法能够有效解决加密资产取证中的标签一致性和准确性问题

Abstract: Attribution tags form the foundation of modern cryptoasset forensics. However, inconsistent or incorrect tags can mislead investigations and even result in false accusations. To address this issue, we propose a novel computational method based …

</details>


### [65] [Odinann: Direct insert for consistently stable performance in billion-scale graphbased vector search](https://scholar.google.com/scholar_url?url=https://www.usenix.org/system/files/conf%25C3%25A9rence/fast26/fast26spring-prepub_guo.pdf&hl=en&sa=X&d=12528248511283537910&ei=XAd4aYqlEayK6rQPm4H2iQs&scisig=AHkA5jT0QCmRssdHSd0ZrE3rnVZD&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=2&folt=cit)
*H Guo,Y Lu*

Main category: Google Scholar

TL;DR: 提出直接插入方法解决十亿级近似最近邻搜索中磁盘图索引插入新向量时性能不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 现有十亿级ANNS的磁盘图索引在插入新向量时无法保持稳定的搜索性能，需要改进插入机制

Method: 采用直接插入方法，将新向量直接插入磁盘图索引，而不是先在内存中缓冲

Result: 该方法能够在插入新向量时维持稳定的搜索性能

Conclusion: 直接插入方法为十亿级ANNS的磁盘图索引提供了有效的插入解决方案

Abstract: Abstract Approximate Nearest Neighbor Search (ANNS) is widely used in various scenarios. For billion-scale ANNS, on-disk graph-based indexes, which organize the vectors as a graph and store them on disk, are favored for their performance and cost-efficiency. However, existing indexes can not maintain a stable search performance while inserting new vectors. In this paper, we propose to use direct insert, which directly inserts vectors into the on-disk index, rather than buffering them in memory …

</details>


### [66] [Unpacking distributed machine learning: A unified taxonomy, formal foundations, and the rise of emerging paradigms](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0925231226002250&hl=en&sa=X&d=7274668652531169010&ei=XAd4aYqlEayK6rQPm4H2iQs&scisig=AHkA5jRWoh3GI_SX_1GYLGyYqx3X&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=3&folt=cit)
*T Ramírez*

Main category: Google Scholar

TL;DR: 一篇关于分布式机器学习的综述论文，提出了结构化分类法来解决该领域碎片化和缺乏统一框架的问题


<details>
  <summary>Details</summary>
Motivation: 数据快速增长和模型复杂度增加推动了分布式机器学习的发展，但该领域存在碎片化问题且缺乏统一框架，需要系统性的分类和整理

Method: 提出结构化分类法，将分布式机器学习方法分为四大类别，对近期进展进行全面综述

Result: 建立了系统的分类框架，帮助研究人员理解不同方法的异同，为领域发展提供结构化指导

Conclusion: 分布式机器学习是应对大规模数据和复杂模型的关键范式，提出的分类法有助于整合碎片化的研究进展，推动该领域的系统化发展

Abstract: The rapid growth of data and the increasing complexity of models have driven the emergence of Distributed Machine Learning as a key paradigm for scalable, efficient, and privacy-aware model training across decentralised environments. This paper presents a comprehensive review of recent advances in this topic, highlighting the field's fragmentation and the absence of unified frameworks. In order to address this issue, a structured taxonomy is proposed that categorises its approaches into four …

</details>


### [67] [Understanding the Research-Practice Gap in Visualization Design Guidelines](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11281526/&hl=en&sa=X&d=16364139041919915965&ei=XAd4aYqlEayK6rQPm4H2iQs&scisig=AHkA5jRGa2JBsY2dvgewxU1sZFwS&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=4&folt=cit)
*NW Kim,G Myers,J Choi,Y Cho,C Oh,YS Kim*

Main category: Google Scholar

TL;DR: 研究调查可视化设计指南中的研究-实践差距，通过混合方法分析学术研究与实践指南之间的差异


<details>
  <summary>Details</summary>
Motivation: 尽管感知与认知的实证研究为可视化设计提供了基础，并提炼为实用指南，但尚不清楚这些研究驱动的见解在实践者实际使用的指南中反映程度如何，因此需要研究可视化设计指南中的研究-实践差距

Method: 采用混合方法：首先收集实践者使用的设计指南，然后通过系统分析比较学术研究与实践指南之间的差异，可能包括内容分析、专家访谈或案例研究等方法

Result: 研究发现可视化设计指南存在显著的研究-实践差距，学术研究成果未能充分转化为实践者使用的实用指南，揭示了知识转化过程中的障碍和不足

Conclusion: 需要建立更有效的知识转化机制，将实证研究成果更好地整合到实践指南中，以弥合可视化设计领域的研究-实践差距

Abstract: Empirical research on perception and cognition has laid the foundation for visualization design, often distilled into practical guidelines intended to support effective chart creation. However, it remains unclear how well these research-driven insights are reflected in the guidelines practitioners actually use. In this paper, we investigate the research-practice gap in visualization design guidelines through a mixed-methods approach. We first collected design guidelines from practitioner …

</details>


### [68] [A Meta-Research Benchmark of Data Visualization Standards in Code Generated by Large Language Models](https://scholar.google.com/scholar_url?url=https://repositori.upf.edu/bitstreams/93e8c76c-cf46-471d-8962-5c1a7215afc8/download%23page%3D282&hl=en&sa=X&d=16045691308802042429&ei=XAd4aYqlEayK6rQPm4H2iQs&scisig=AHkA5jSVxOffWwn5qoS-7dOD7tWz&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=8&folt=cit)
*W Bailkoski,P Liehe,A Vecchi*

Main category: Google Scholar

TL;DR: 开发了一个标准感知的评估框架，用于评估大语言模型生成的绘图代码是否符合可视化最佳实践，并对三种LLM在多种绘图任务中的表现进行了系统性评估。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于生成科学图表绘图代码，需要评估这些模型生成的代码是否始终符合基本的可视化标准。目前缺乏系统性的评估方法来检验LLM生成的可视化代码是否遵循可视化最佳实践。

Method: 引入了一个标准感知的评估框架（rubric），作为元研究基准来评估LLM生成的图表对可视化最佳实践的遵循程度。使用该框架评估了三种大语言模型在不同提示风格和八种常见绘图任务中生成的绘图脚本，共分析了72个代码块。

Result: 研究结果显示，LLM生成的绘图代码在遵循可视化标准方面存在显著差异。不同模型、不同提示风格以及不同绘图任务之间的表现各不相同，揭示了当前LLM在生成符合可视化最佳实践的代码方面的局限性。

Conclusion: 需要专门设计的评估框架来系统评估LLM生成的可视化代码质量。该研究为未来改进LLM在科学可视化领域的应用提供了基准，并强调了在自动化代码生成中确保可视化标准遵循的重要性。

Abstract: Large language models (LLMs) are increasingly used to generate plotting code for scientific figures, but it is unclear whether their outputs consistently meet basic visualization standards. We introduce a standards-aware rubric, designed as a meta-research benchmark to evaluate adherence to visualization best practices in LLM-generated graphs. Using this rubric, we assess plotting scripts produced by three LLMs across prompt styles and eight common plotting tasks. Over 72 code blocks …

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [69] [Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple](https://arxiv.org/abs/2601.16294)
*Evangelos Georganas,Alexander Heinecke,Pradeep Dubey*

Main category: cs.DC

TL;DR: 该论文提出使用广义希尔伯特曲线等空间填充曲线技术来优化GEMM计算，实现平台无关和形状无关的高性能矩阵乘法，避免传统调优的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现代CPU平台上的矩阵乘法加速器具有高FLOP/Byte机器平衡，使得实现最优矩阵乘法具有挑战性。现有厂商库需要针对不同平台和矩阵形状进行繁琐的调优（张量布局、并行化方案、缓存分块），导致性能"玻璃下巴"问题。

Method: 1. 使用广义希尔伯特曲线等空间填充曲线将多维计算空间转换为一维顺序，保持数据局部性；2. 基于SFC进行工作分区，实现平台无关和形状无关的矩阵乘法方案；3. 扩展SFC工作分区实现通信避免算法，复制输入张量并最小化关键路径上的通信/数据移动。

Result: 在多个CPU平台上实现了最先进的结果，相比厂商库在多种GEMM形状上获得最高2倍的几何平均加速比，代码简洁（约30行LOC）。

Conclusion: 空间填充曲线技术能够有效解决GEMM调优的复杂性，提供平台无关和形状无关的高性能解决方案，同时通过通信避免算法进一步优化数据移动。

Abstract: General Matrix Multiplication (GEMM) is the cornerstone of Deep Learning and HPC workloads; accordingly, academia and industry have heavily optimized this kernel. Modern platforms with matrix multiplication accelerators exhibit high FLOP/Byte machine balance, which makes implementing optimal matrix multiplication challenging. On modern CPU platforms with matrix engines, state-of-the-art vendor libraries tune input tensor layouts, parallelization schemes, and cache blocking to minimize data movement across the memory hierarchy and maximize throughput. However, the best settings for these parameters depend strongly on the target platform (number of cores, memory hierarchy, cache sizes) and on the shapes of the matrices, making exhaustive tuning infeasible; in practice this leads to performance "glass jaws". In this work we revisit space filling curves (SFC) to alleviate the problem of this cumbersome tuning. SFC convert multi-dimensional coordinates (e.g. 2D) into a single dimension (1D), keeping nearby points in the high-dimensional space close in the 1D order. We partition the Matrix Multiplication computation space using recent advancements in generalized SFC (Generalized Hilbert Curves), and we obtain platform-oblivious and shape-oblivious matrix-multiplication schemes that exhibit inherently high degree of data locality. Furthermore, we extend the SFC-based work partitioning to implement Communication-Avoiding (CA) algorithms that replicate the input tensors and provably minimize communication/data-movement on the critical path. The integration of CA-algorithms is seamless and yields compact code (~30 LOC), yet it achieves state-of-the-art results on multiple CPU platforms, outperforming vendor libraries by up to 2x(geometric-mean speedup) for a range of GEMM shapes.

</details>


### [70] [Consensus In Asynchrony](https://arxiv.org/abs/2601.16460)
*Ivan Klianev*

Main category: cs.DC

TL;DR: 提出基于事件同步的确定性容错共识算法，在异步环境中实现向量一致性，挑战FLP不可能性定理的隐含假设


<details>
  <summary>Details</summary>
Motivation: 传统FLP不可能性定理表明异步系统中无法实现确定性容错共识，但该研究质疑FLP定理的隐含假设，探索在事件同步条件下实现共识的可能性

Method: 开发基于事件同步的算法，实现向量一致性协议，分析FLP定理的三种隐含假设，区分数据无关和数据相关两种一致性类型

Result: 算法在异步环境中实现确定性容错共识，具有安全性、活性和容忍单节点崩溃能力；实验表明FLP定理的第三个隐含假设缺乏证据支持

Conclusion: 事件同步足以实现确定性容错共识，FLP不可能性定理依赖于未经证实的隐含假设，特别是第三种假设缺乏实验证据支持

Abstract: We demonstrate sufficiency of events-based synchronisation for solving deterministic fault-tolerant consensus in asynchrony. Main result is an algorithm that terminates with valid vector agreement, hence operates with safety, liveness, and tolerance to one crash. Reconciling with the FLP impossibility result, we identified: i) existence of two types of agreements: data-independent and data-dependent; and ii) dependence of FLP theorem correctness on three implicit assumptions. Consensus impossibility with data-dependent agreement is contingent on two of them. The theorem-stated impossibility with every agreement type hinges entirely on the third. We provide experimental results showing that the third assumption has no evidence in support.

</details>


### [71] [W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs](https://arxiv.org/abs/2601.16536)
*Yuanhong He,Peiyu Niu,Jun Chen,Chenchen Zhang,Chao Yang*

Main category: cs.DC

TL;DR: 首个面向华为昇腾910 NPU的W4A16矩阵乘法内核，通过向量核心实时反量化、立方核心GEMM和Split-K并行化，在LLM解码场景中实现1.01-1.74倍加速


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，权重仅量化（W4A16）对减少内存占用至关重要，但在华为昇腾910 NPU上高效部署面临挑战，因为该架构缺乏原生混合精度支持且采用解耦计算架构

Method: 设计首个面向昇腾910 NPU的W4A16矩阵乘法内核，利用向量核心进行INT4到FP16的实时反量化，立方核心执行高吞吐量GEMM，采用Split-K并行化缓解内存延迟

Result: 在不同矩阵形状和批次大小下，当K>>N（典型LLM解码场景）时，方法优于数据并行方法，加速比1.01-1.74倍；分析显示主要瓶颈是权重的额外全局内存传输而非反量化计算，W4A16相对原生FP16最大加速1.48倍

Conclusion: 该方法为量化大语言模型在各种领域专用加速器上的高效部署奠定了坚实基础，并提供了深刻见解

Abstract: As Large Language Models (LLMs) scale, weight-only quantization (W4A16: 4-bit weights, 16-bit activations) becomes critical for reducing memory footprint with minimal accuracy loss. However, its efficient deployment on Huawei's Ascend 910 Neural Processing Unit (NPU) is challenging due to limited native mixed-precision support and the accelerator's decoupled compute architecture. To enable quantization on such architecture, we present the first practical W4A16 matrix multiplication kernel tailored for the Ascend 910 NPU. Our design leverages vector cores for on-the-fly INT4-to-FP16 dequantization, cube cores for high-throughput GEMM, and Split-K parallelization to mitigate memory latency. Performance evaluations across diverse matrix shapes and batch sizes show our method outperforms data-parallel approaches when K >> N, a typical scenario in LLM decoding. Specially, our method can achieve a speedup ranging from 1.01x to 1.74x. In addition, our profile reveals the primary bottleneck is not dequantization compution itself, but extra global memory transfer for the weight, making W4A16 only reaching a maximum speedup of 1.48x over native FP16xFP16 matrix multiplication in PyTorch. In the long run, our method lays a solid foundation and provides insightful views for the efficient deployment of quantized large language models on various domain-specific accelerators.

</details>


### [72] [GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms](https://arxiv.org/abs/2601.16637)
*Jun Doi,Tomonori Shirakawa,Yukio Kawashima,Seiji Yunoki,Hiroshi Horii*

Main category: cs.DC

TL;DR: 提出基于Thrust库的GPU加速SBD实现，通过数据并行原语和扁平化数据结构实现~40倍加速，显著减少SQD迭代总时间


<details>
  <summary>Details</summary>
Motivation: SBD在基于样本的量子对角化(SQD)中占据核心地位，其迭代对角化过程是主要经典计算负载。现有CPU实现效率有限，需要利用现代GPU架构加速以提升SQD整体性能。

Method: 使用Thrust库实现GPU加速的SBD，重构关键组件：配置处理、激发生成和矩阵向量运算，采用细粒度数据并行原语和扁平化GPU友好数据布局，充分利用GPU并行计算能力。

Result: Thrust-based SBD相比CPU执行实现高达~40倍的加速比，显著减少SQD迭代的总运行时间，证明GPU原生并行原语为加速SQD量子-经典工作流提供了简单、可移植且高性能的基础。

Conclusion: GPU原生并行原语为加速SQD量子-经典工作流提供了有效解决方案，通过Thrust库实现的SBD在保持可移植性的同时显著提升性能，为量子计算中的经典计算瓶颈提供了优化途径。

Abstract: Selected Basis Diagonalization (SBD) plays a central role in Sample-based Quantum Diagonalization (SQD), where iterative diagonalization of the Hamiltonian in selected configuration subspaces forms the dominant classical workload. We present a GPU-accelerated implementation of SBD using the Thrust library. By restructuring key components -- including configuration processing, excitation generation, and matrix-vector operations -- around fine-grained data-parallel primitives and flattened GPU-friendly data layouts, the proposed approach efficiently exploits modern GPU architectures. In our experiments, the Thrust-based SBD achieves up to $\sim$40$\times$ speedup over CPU execution and substantially reduces the total runtime of SQD iterations. These results demonstrate that GPU-native parallel primitives provide a simple, portable, and high-performance foundation for accelerating SQD-based quantum-classical workflows.

</details>


### [73] [DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers](https://arxiv.org/abs/2601.16956)
*Avinash Maurya,M. Mustafa Rafique,Franck Cappello,Bogdan Nicolae*

Main category: cs.DC

TL;DR: DataStates-LLM：一种针对大规模Transformer模型训练的新型检查点架构，通过状态提供者解耦状态抽象与数据移动，利用参数不变性实现异步非阻塞快照，显著提升检查点吞吐量并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 随着大型Transformer模型参数规模达到万亿级别，需要在数千个GPU上使用复杂的混合并行策略进行训练。现有检查点解决方案通常将模型状态视为不透明的二进制数据块，忽略了底层数据结构的"3D异质性"（内存位置、逻辑对象分片、数据类型、序列化要求），导致运行时开销大、设备到主机传输阻塞、数据无关序列化和存储I/O争用等问题。

Method: 引入DataStates-LLM架构，通过状态提供者（State Providers）解耦状态抽象与数据移动。利用前向和反向传播期间模型参数的不变性，执行"惰性"、非阻塞的异步快照。状态提供者有效合并碎片化的异构分片，并将元数据序列化与批量张量I/O重叠执行。

Result: 在256个A100-40GB GPU上对高达700亿参数的模型进行评估。DataStates-LLM实现了高达4倍的检查点吞吐量提升，并将端到端训练时间减少高达2.2倍，有效缓解了极端规模LLM训练中的序列化和异质性瓶颈。

Conclusion: DataStates-LLM通过创新的状态提供者架构解决了大规模Transformer模型训练中的检查点瓶颈问题，显著提升了检查点性能和训练效率，为极端规模LLM训练提供了有效的解决方案。

Abstract: The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resume, investigating undesirable training trajectories, and explaining model evolution. However, existing checkpointing solutions typically treat model state as opaque binary blobs, ignoring the ``3D heterogeneity'' of the underlying data structures--varying by memory location (GPU vs. Host), number of ``logical'' objects sharded and split across multiple files, data types (tensors vs. Python objects), and their serialization requirements. This results in significant runtime overheads due to blocking device-to-host transfers, data-oblivious serialization, and storage I/O contention. In this paper, we introduce DataStates-LLM, a novel checkpointing architecture that leverages State Providers to decouple state abstraction from data movement. DataStates-LLM exploits the immutability of model parameters during the forward and backward passes to perform ``lazy'', non-blocking asynchronous snapshots. By introducing State Providers, we efficiently coalesce fragmented, heterogeneous shards and overlap the serialization of metadata with bulk tensor I/O. We evaluate DataStates-LLM on models up to 70B parameters on 256 A100-40GB GPUs. Our results demonstrate that DataStates-LLM achieves up to 4$\times$ higher checkpointing throughput and reduces end-to-end training time by up to 2.2$\times$ compared to state-of-the-art solutions, effectively mitigating the serialization and heterogeneity bottlenecks in extreme-scale LLM training.

</details>


<div id='Carsten Binnig'></div>

# Carsten Binnig [[Back]](#toc)

### [74] [Give Me a Secure Ride: TEE-Blockchain Enabled Privacy-Aware and Verifiable Ride Sharing Services](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11353107/&hl=zh-CN&sa=X&d=12272627285584331776&ei=XQd4aeKKKfStieoP-8T7kA0&scisig=AHkA5jQb8notC8quNJFww9CYXjIc&oi=scholaralrt&hist=i6heNjgAAAAJ:15269883191641703195:AHkA5jTZCYhse3wIcZXHnO-iboY0&html=&pos=2&folt=cit)
*J Yang,H Wu,B Düdder,C Xiao,X Dong,Z Cao*

Main category: Carsten Binnig

TL;DR: 该论文探讨了拼车服务（RSS）中的安全挑战，提出了一种新的安全解决方案来应对恶意威胁模型，相比现有半诚实模型方案具有更好的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 移动互联网和共享经济的发展推动了拼车服务作为智能交通中空间众包范式的出现。与网约车相比，拼车服务由于行程计划的双向披露和复杂的匹配逻辑，在安全性和服务质量方面面临更大挑战。现有安全解决方案主要基于半诚实威胁模型，或存在性能瓶颈。

Method: 论文提出了一种新的安全拼车服务方案，能够抵御恶意威胁模型。该方法可能涉及安全多方计算、隐私保护匹配算法或加密协议等技术，以在保护用户隐私的同时实现高效的行程匹配。

Result: 相比现有半诚实模型方案，所提方案在安全性和效率方面均有显著提升，能够有效应对恶意攻击，同时保持合理的计算和通信开销。

Conclusion: 该研究为拼车服务提供了更强大的安全解决方案，能够在恶意威胁模型下保护用户隐私和行程安全，对智能交通系统的安全发展具有重要意义。

Abstract: The proliferation of mobile internet and sharing economy has catalyzed the emergence of Ride-Sharing Services (RSSs) as a paradigm of spatial crowdsourcing in intelligent transportation. Compared with ride-hailing, RSSs present heightened challenges in security and service quality management due to bidirectional disclosure of trip plans and complex matching logic. Existing secure RSS solutions predominantly operate under semi-honest threat models or suffer from prohibitive …

</details>


### [75] [LASER: Line-Aware and Self-Balancing Learned Index for Rapid Key Lookup](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11304630/&hl=zh-CN&sa=X&d=318514596616997353&ei=XQd4aeKKKfStieoP-8T7kA0&scisig=AHkA5jQnMiKj1tqkgiy02AINhSt5&oi=scholaralrt&hist=i6heNjgAAAAJ:15269883191641703195:AHkA5jTZCYhse3wIcZXHnO-iboY0&html=&pos=4&folt=cit)
*S Zhao,J Zhou,S Jiang*

Main category: Carsten Binnig

TL;DR: 该论文探讨了学习索引在动态工作负载下的性能优化，特别是针对复杂键分布和频繁插入场景的层次化模型结构设计。


<details>
  <summary>Details</summary>
Motivation: 学习索引相比传统树索引在速度和空间效率上有显著优势，但现有方法在处理复杂键分布和动态工作负载（特别是频繁插入）时面临挑战，需要更有效的层次化模型结构来维持性能。

Method: 采用层次化的树结构组织多个预测模型，通过模型分层来适应复杂键分布，并设计支持动态插入的机制，使学习索引能够处理频繁更新的工作负载。

Result: 层次化模型结构的学习索引在复杂键分布和动态工作负载下表现出色，相比传统树索引在速度和空间效率上都有显著提升，特别是在支持频繁插入时仍能保持高性能。

Conclusion: 通过层次化模型结构设计，学习索引能够有效处理复杂键分布和动态工作负载，为数据库索引系统提供了比传统树索引更高效、更灵活的解决方案。

Abstract: Learned indexes have received significant attention for their potential to dramatically outperform traditional treebased indexes in both speed and space efficiency. Their core strength lies in using predictive models to estimate the position of a key within a sorted array. To handle complex key distributions and support frequent insertions in dynamic workloads, learned indexes typically organize multiple models hierarchically in a tree structure. These indexes perform best when a model can …

</details>


<div id='Zongheng Yang'></div>

# Zongheng Yang [[Back]](#toc)

### [76] [IaaS Capacity Planning with Multiple Pricing Models Using Predictive Heuristics](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11331429/&hl=zh-CN&sa=X&d=5588495785410844778&ei=Wwd4aYz-OoqM6rQP-o20EQ&scisig=AHkA5jQ8XawHZ__VANIHRePGZIxc&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=0&folt=cit)
*M Alves,F Morais*

Main category: Zongheng Yang

TL;DR: 云提供商提供多种虚拟机定价模型（按需付费和预留实例），在不确定需求下进行IaaS容量规划时选择合适模型具有挑战性。传统在线启发式方法仅使用当前需求和购买历史，忽略了影响决策质量的工作负载模式。


<details>
  <summary>Details</summary>
Motivation: 云基础设施即服务（IaaS）容量规划面临的主要挑战是在需求不确定的情况下，如何在按需付费（灵活但昂贵）和预留实例（成本较低但需长期承诺）之间做出最优选择。传统方法仅基于当前需求和历史购买决策，无法充分利用工作负载模式信息，导致决策质量受限。

Method: 论文提出了一种改进的在线启发式方法，该方法不仅考虑当前需求和购买历史，还整合了未来工作负载模式的预测信息。通过分析工作负载的时间序列特征和周期性模式，该方法能够做出更明智的预留实例购买决策，优化长期成本效益。

Result: 提出的方法相比传统在线启发式方法，在成本节约方面取得了显著改进。通过考虑工作负载模式，该方法能够更准确地预测未来需求，从而做出更优的预留实例购买决策，减少了过度预留或预留不足的情况，提高了云资源采购的整体经济效益。

Conclusion: 在IaaS容量规划中，整合工作负载模式信息的在线启发式方法能够显著改善预留实例购买决策的质量。这种方法在保持在线算法实时决策优势的同时，通过利用工作负载的时序特征，实现了更好的成本优化效果，为云资源采购提供了更有效的决策支持。

Abstract: Cloud providers offer multiple pricing models for acquiring virtual machines (VMs), such as on-demand, with payper-use flexibility, and reserved instances, which reduce costs under long-term commitments. Choosing between these models is challenging in IaaS capacity planning under uncertain demand. Traditional online heuristics use the current demand and purchase history, offering performance guarantees but ignoring future workload patterns, which influence the quality of …

</details>


### [77] [Broker-assisted Computation Offloading and Resource Pricing in MEC Networks: a Two-Stage Stackelberg Game Approach](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11249720/&hl=zh-CN&sa=X&d=6775639767144035518&ei=Wwd4aYz-OoqM6rQP-o20EQ&scisig=AHkA5jQamVaCy9yDPGf0GmoyYUP6&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=1&folt=cit)
*H Zhou,D Meng,J Guo,P Sun,L Zhao,B Guo,Z Yu*

Main category: Zongheng Yang

TL;DR: 移动边缘计算中任务卸载的资源定价与激励机制研究


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算虽然能提升服务响应速度和服务质量，但由于参与实体（移动用户和边缘服务器）的自私性，缺乏有效的资源定价和补偿机制，导致任务卸载难以实现。移动用户不愿在没有合理定价的情况下卸载任务，边缘服务器也缺乏提供计算资源的动机。

Method: 论文可能提出了一种资源定价和激励机制，通过设计合理的定价策略和补偿方案来解决移动边缘计算中的任务卸载问题。可能包括博弈论方法、激励机制设计或拍卖机制等。

Result: 通过提出的资源定价和激励机制，能够激励移动用户主动卸载计算密集型任务，同时确保边缘服务器获得适当补偿，从而提高系统整体效率和服务质量。

Conclusion: 有效的资源定价和激励机制对于移动边缘计算中任务卸载的成功实施至关重要，能够平衡移动用户和边缘服务器的利益，促进系统资源的高效利用。

Abstract: Mobile Edge Computing (MEC) significantly enhances service response speeds and improves the Quality of Service (QoS) by facilitating the offloading of computation-intensive tasks from Mobile Users (MUs) to nearby Edge Servers (ESs). However, due to the inherent selfishness of involved entities, MUs may be unwilling to offload tasks without reasonable resource pricing, and ESs may lack motivation to provide computation resources without appropriate compensation. Furthermore, improving …

</details>


### [78] [SkySync: Accelerating File Synchronization with Collaborative Delta Generation](https://scholar.google.com/scholar_url?url=https://www.usenix.org/system/files/conf%25C3%25A9rence/fast26/fast26spring-prepub_zhang-zhihao.pdf&hl=zh-CN&sa=X&d=2152779287447864684&ei=Wwd4aYz-OoqM6rQP-o20EQ&scisig=AHkA5jSu_yRjiljwZRJNXyyXAEZD&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=2&folt=cit)
*Z Zhang,H Li,L Tang,G Xue,J Shu,Y Zhang*

Main category: Zongheng Yang

TL;DR: 该论文探讨了在天空计算范式下，现有文件同步方案（FSC和CDC）的局限性，并提出了一种新的基于内容感知的同步方法


<details>
  <summary>Details</summary>
Motivation: 随着云计算向天空计算范式演进，跨云文件同步变得日益重要。现有固定大小分块（FSC）和内容定义分块（CDC）同步方案依赖复杂算法生成文件指纹，存在效率问题，需要更高效的同步方案

Method: 提出了一种基于内容感知的文件同步方法，该方法可能通过智能内容分析来优化分块策略，减少算法复杂度并提高同步效率

Result: 论文结果表明，提出的内容感知同步方法相比传统FSC和CDC方案，在同步效率和资源消耗方面有显著改进

Conclusion: 内容感知同步方法为天空计算环境下的跨云文件同步提供了更有效的解决方案，能够更好地支持分布式云基础设施上的应用和服务

Abstract: File synchronization (sync) is of increasing significance for not only intra-cloud but also inter-cloud applications and services, as cloud computing is evolving into the Sky computing paradigm with the illusion of utility computing on an infrastructure of multiple geographically-distributed clouds. However, existing file sync schemes, mainly including fixedsized chunking (FSC) based sync and content-defined chunking (CDC) based sync, heavily rely on complex algorithms for generating the …

</details>


### [79] [MultiSiFer: Detecting Multiple-Speaker Fake Voice Without Speaker-Irrelative Features](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11316509/&hl=zh-CN&sa=X&d=9545551376183499080&ei=Wwd4aYz-OoqM6rQP-o20EQ&scisig=AHkA5jSqZvQ8rHpduKMfNDIBr7n6&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=3&folt=cit)
*X Liu,X Hai,Z Yu,Z Zhang,Q Fei,Q Zhou*

Main category: Zongheng Yang

TL;DR: MultiSiFer：基于多说话人不变特征学习的新型伪造语音检测器，解决现有方法对说话人无关特征过拟合、鲁棒性差及多说话人场景失效的问题


<details>
  <summary>Details</summary>
Motivation: 语音合成技术快速发展引发内容安全和信任担忧。现有伪造语音检测器在受控环境下表现良好，但普遍存在对说话人无关特征过拟合、鲁棒性差、多说话人场景失效等问题，需要更有效的检测方法

Method: 提出MultiSiFer检测器，采用新设计理念：不仅区分合成与人类语音，更专注于学习多说话人不变特征，避免对说话人无关特征的过拟合，提升多说话人场景下的检测性能

Result: 未提供具体实验结果，但从方法描述推断应能显著提升多说话人场景下的检测准确率、鲁棒性和泛化能力

Conclusion: MultiSiFer通过专注于多说话人不变特征学习，为解决伪造语音检测中的过拟合、鲁棒性差和多说话人场景失效问题提供了新思路

Abstract: Voice synthesis technologies have advanced rapidly, raising serious concerns about content security and trust. While many fake voice detectors achieve strong performance in controlled settings, they often overfit to speaker-irrelative features (SiFs), exhibit poor robustness, and fail in multi-speaker scenarios. To address these limitations, we propose MultiSiFer, a novel fake voice detector grounded in a new design philosophy: rather than merely distinguishing synthetic from human voices, it …

</details>


### [80] [Emo-DiT: Emotional Speech Synthesis With a Diffusion Model Approach to Enhance Naturalness and Emotional Expressiveness](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11352958/&hl=zh-CN&sa=X&d=573113590073204256&ei=Wwd4aYz-OoqM6rQP-o20EQ&scisig=AHkA5jR0eA-RwDvaaS872m9Y8wE4&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=4&folt=cit)
*Y Wang,L Yang,B Wang,J Xu,D Yang,M Zhou…*

Main category: Zongheng Yang

TL;DR: 提出基于DiT架构的Grad-DiT模型，通过梯度引导机制改进情感TTS中情感模块与TTS模型的协同，解决情感表达受限问题


<details>
  <summary>Details</summary>
Motivation: 当前情感TTS任务通过将情感模块集成到TTS模型中实现了高质量情感语音，但情感模块在TTS模型中的嵌入研究有限，合成语音的情感表达同时受TTS模型和情感模块约束，往往无法达到最优效果

Method: 提出基于扩散模型DiT架构的新型TTS模型Grad-DiT，采用梯度引导机制来优化情感模块与TTS模型的协同工作

Result: 未在摘要中明确说明具体实验结果，但暗示该方法旨在解决情感表达受限问题，预期能提升情感TTS性能

Conclusion: Grad-DiT模型通过创新的梯度引导机制，有望改善情感模块与TTS模型的协同，从而提升情感语音合成的质量和表现力

Abstract: Current emotional text-to-speech tasks have achieved high-quality emotional speech by incorporating emotion modules into text-to-speech models. However, there has been limited in-depth research on embedding emotion modules within TTS models, and the expression of emotion in synthesized speech is constrained by both the TTS model and the emotion module, often preventing optimal results. This paper presents a novel TTS model, Grad-DiT, based on the DiT architecture of diffusion models …

</details>


### [81] [PerSemCom: A Personalized Semantic Communication Framework for Speech Transmission](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11348985/&hl=zh-CN&sa=X&d=16198888609230187452&ei=Wwd4aYz-OoqM6rQP-o20EQ&scisig=AHkA5jRd8bBfXBO9uFaDfqIjpioU&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=5&folt=cit)
*K Tan,H Zhao,L Zhou,Y Zhang,J Xiong,Y Zhang…*

Main category: Zongheng Yang

TL;DR: 提出PerSemCom框架，将说话人声学特征与语义信息结合，实现个性化语音传输


<details>
  <summary>Details</summary>
Motivation: 语义通信从物理比特传输转向个性化语义服务，语音中的说话人个性化特征对源恢复和理解至关重要

Method: 提出语义驱动的个性化语音传输框架PerSemCom，结合说话人声学特征与语义信息

Result: 未在摘要中明确说明具体实验结果

Conclusion: PerSemCom框架为个性化语音传输提供新范式，结合说话人特征与语义信息提升通信效果

Abstract: By focusing on the intrinsic meaning of information, semantic communication (SemCom) marks a fundamental paradigm shift from physical bit transmission to personalized semantic service. Considering the importance of personalized features related to the speaker in speech for source recovery and understanding, we propose a semantic-driven framework for personalized speech transmission, named PerSemCom, which combines speaker acoustic features with semantic information …

</details>


### [82] [A Ground-Truth-Free Framework for Validating Emotions in Generative AI Speech Synthesis](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/11359665.pdf&hl=zh-CN&sa=X&d=18021016571261658953&ei=Wwd4aYz-OoqM6rQP-o20EQ&scisig=AHkA5jTVO3y4ewuX8QuOFU30DdXc&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=6&folt=cit)
*AR Özcan,GA Figen*

Main category: Zongheng Yang

TL;DR: 提出基于原型的框架，整合Emotion2Vec情感嵌入与WavLM声学-语言表征，实现可扩展、系统无关的合成语音情感表达性评估


<details>
  <summary>Details</summary>
Motivation: 合成语音情感表达性评估面临缺乏真实情感标签和依赖昂贵人工感知研究的挑战，需要可扩展且系统无关的评估方法

Method: 集成Emotion2Vec情感专用嵌入与WavLM通用声学-语言表征，将嵌入投影到共享潜在空间，基于原型进行情感分类和评估

Result: 框架能够实现可扩展的合成语音情感表达性评估，减少对人工标注的依赖，提供系统无关的评估方案

Conclusion: 原型框架为合成语音情感表达性评估提供了有效的解决方案，整合专用与通用表征实现了更好的评估效果

Abstract: Evaluating emotional expressivity in synthetic speech is challenging due to the absence of ground-truth affective labels and the reliance on costly human perceptual studies. This paper introduces a prototype-based framework that integrates affect-specialized Emotion2Vec embeddings with general-purpose acoustic and linguistic representations from WavLM to enable scalable and system-agnostic evaluation. Embeddings are projected into a shared latent space where each emotion category …

</details>


### [83] [Evaluating Performance and Scalability of GPT-2 and DistilGPT on CPU-Based Clusters using Intel's LLM-on-Ray and Locust](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11333852/&hl=zh-CN&sa=X&d=788974877544282339&ei=Wwd4aYz-OoqM6rQP-o20EQ&scisig=AHkA5jRbkmQdBrUEqzkFEqiqCJLK&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=8&folt=cit)
*S Patel,T Bansod,P Patel,S Deshpande,S Shinde…*

Main category: Zongheng Yang

TL;DR: 评估GPT-2和DistilGPT在CPU集群上的性能和可扩展性，使用LLM-on-Ray框架和Locust负载测试，重点关注延迟和吞吐量指标


<details>
  <summary>Details</summary>
Motivation: 填补对大型语言模型（如GPT-2和DistilGPT）在CPU集群上性能和可扩展性理解的研究空白，为CPU环境中的LLM部署提供实践指导

Method: 在联想ThinkCentre集群上使用Intel的LLM-on-Ray框架进行CPU推理，采用Locust进行全面的负载测试，分析延迟和吞吐量等关键性能指标

Result: GPT-2性能通常随节点数量增加而提升，但存在扩展限制；DistilGPT在CPU集群上表现出更好的可扩展性和效率

Conclusion: CPU集群能够有效支持LLM推理，但模型选择和集群配置对性能有显著影响；DistilGPT等轻量模型在CPU环境中更具优势

Abstract: This research fills a notable gap in understanding the performance and scalability of Large Language Models (LLMs) such as GPT-2 and DistilGPT on CPU-based clusters. We evaluated these models on a Lenovo ThinkCentre cluster, utilizing Intel's LLM-on-Ray framework for CPU inference and Locust for thorough load testing, with a focus on key metrics such as latency and throughput. In the case of GPT-2, we observed that performance generally improved with a greater number of …

</details>
