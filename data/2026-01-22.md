<div id=toc></div>

# Table of Contents

- [Google Scholar](#Google Scholar) [Total: 4]
- [cs.LG](#cs.LG) [Total: 45]
- [cs.DC](#cs.DC) [Total: 6]
- [Surajit Chaudhuri](#Surajit Chaudhuri) [Total: 6]
- [Alekh Jindal](#Alekh Jindal) [Total: 2]
- [Zongheng Yang](#Zongheng Yang) [Total: 1]
- [Ziniu Wu](#Ziniu Wu) [Total: 2]
- [Matei Zaharia](#Matei Zaharia) [Total: 10]
- [cs.DB](#cs.DB) [Total: 1]
- [Xuanhe Zhou](#Xuanhe Zhou) [Total: 4]


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [1] [Exploring Feasibility of Seamless Multiscale Zooming in Genome Browsers](https://scholar.google.com/scholar_url?url=https://diglib.eg.org/bitstreams/1565da0a-7db2-4254-92a9-4d0bc6232e4b/download&hl=en&sa=X&d=13297937486876759632&ei=ZwFxaYKWF_DB6rQPsMjv6As&scisig=AHkA5jRqZQYVQ3B6vp358z41RGM2&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=0&folt=cit)
*E Ståhlbom,J Molin,C Lundström,A Ynnerman*

Main category: Google Scholar

TL;DR: 基因组浏览器缺乏流畅的多尺度导航功能，本文提出改进方案


<details>
  <summary>Details</summary>
Motivation: 基因组数据可视化面临尺度差异巨大的挑战：局部结构约150碱基对，而整个基因组超过30亿碱基对。现有主流基因组浏览器在多尺度导航支持方面有限，通常只能通过输入坐标或点击按钮进行缩放，缺乏流畅的交互体验。

Method: 论文提出了一种无缝多尺度导航方法，具体技术细节未在摘要中明确说明，但暗示了改进现有基因组浏览器缩放交互的方案。

Result: 摘要未提供具体实验结果，但暗示提出的方法能够改善基因组浏览器的多尺度导航体验。

Conclusion: 基因组浏览器需要更好的多尺度导航支持，本文提出的无缝多尺度方法有望解决现有系统的局限性。

Abstract: A central challenge when visualizing genomics data is the vast difference in the scales of interest. At a detailed level, structures around the size of 150 base pairs are studied. These are positioned throughout a genome that exceeds three billion base pairs. Surprisingly, the most prominently used genome browsers have limited multiscale navigation support. As an example, zooming is controlled by inputting coordinates or by clicking buttons. In this paper, we highlight seamless multiscale …

</details>


### [2] [Hybrid Serverless–Container Architectures for Large-Scale ETL Workloads](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Aremu-Oluwaferanmi-2/publication/399434131_Hybrid_Serverless-Container_Architectures_for_Large-Scale_ETL_Workloads/links/695ac3b90c98040d48273567/Hybrid-Serverless-Container-Architectures-for-Large-Scale-ETL-Workloads.pdf&hl=en&sa=X&d=1704180281420078194&ei=aAFxaZGyINvWieoPkLe3iAs&scisig=AHkA5jTtYvJ12bhQeR-Y32I7Qaf1&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=0&folt=rel)
*S Youseff,A Philip,F Hamzah,A Taofeek,B Barnanas…*

Main category: Google Scholar

TL;DR: 论文探讨了在数据爆炸式增长背景下，如何构建可扩展、灵活且经济高效的ETL（提取-转换-加载）管道，特别关注云原生架构和现代数据集成技术。


<details>
  <summary>Details</summary>
Motivation: 数字平台、物联网设备、企业系统和实时用户交互产生的数据呈指数级增长，对可扩展、灵活且经济高效的ETL解决方案需求日益迫切。传统ETL系统难以应对现代数据环境的复杂性、多样性和规模要求。

Method: 论文可能提出基于云原生架构的ETL方法，利用容器化、微服务、无服务器计算等技术。可能涉及现代数据集成框架、流处理与批处理融合、自动化数据管道编排，以及成本优化策略。

Result: 预期结果包括：1) 可扩展的ETL架构设计；2) 处理效率提升；3) 成本降低；4) 灵活适应不同数据源和格式；5) 实时数据处理能力增强；6) 运维自动化水平提高。

Conclusion: 云原生ETL架构是应对数据爆炸挑战的有效解决方案，能够提供所需的可扩展性、灵活性和成本效益，为现代数据驱动型组织提供关键基础设施支持。

Abstract: The exponential growth of data generated by digital platforms, Internet of Things (IoT) devices, enterprise systems, and real-time user interactions has intensified the demand for scalable, flexible, and cost-efficient Extract–Transform–Load (ETL) …

</details>


### [3] [Machine Learning–Augmented ETL Pipelines in Microservices Architectures: Overcoming Legacy Constraints Through Serverless and Intelligent Automation](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Philip-Adekola-3/publication/399645899_Machine_Learning-Augmented_ETL_Pipelines_in_Microservices_Architectures_Overcoming_Legacy_Constraints_Through_Serverless_and_Intelligent_Automation/links/69625ec121a46d6f701edd6e/Machine-Learning-Augmented-ETL-Pipelines-in-Microservices-Architectures-Overcoming-Legacy-Constraints-Through-Serverless-and-Intelligent-Automation.pdf&hl=en&sa=X&d=7145324154824491497&ei=aAFxaZGyINvWieoPkLe3iAs&scisig=AHkA5jTJVhqDz3D5fB_SpkmJnHoh&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=1&folt=rel)
*S Youseff,A Philip,F Hamzah,A Taofeek,B Barnanas…*

Main category: Google Scholar

TL;DR: 论文探讨了在云原生和微服务架构背景下，数据集成管道面临的挑战与解决方案


<details>
  <summary>Details</summary>
Motivation: 数据驱动应用的指数级增长与云原生、微服务架构的快速采用，从根本上改变了组织设计、部署和管理数据集成管道的方式，这带来了新的挑战和机遇

Method: 论文未提供具体方法细节，但从摘要推断可能涉及对现有数据集成架构的分析、云原生技术的应用、微服务模式在数据管道中的实现，以及相关解决方案的设计

Result: 摘要未明确说明具体结果，但暗示了在云原生环境下数据集成管道需要新的设计和管理方法，以适应现代应用架构的需求

Conclusion: 组织需要重新思考数据集成策略，采用适应云原生和微服务架构的新方法，以应对数据驱动应用快速增长带来的挑战

Abstract: The exponential growth of data-driven applications, coupled with the rapid adoption of cloud-native and microservices architectures, has fundamentally transformed the way organizations design, deploy, and manage data integration pipelines …

</details>


### [4] [Overview of WSC Workloads](https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-3-031-99489-0_3&hl=en&sa=X&d=18371040029163166320&ei=aAFxaZGyINvWieoPkLe3iAs&scisig=AHkA5jRf1aChkpcuGG6EoH_oLTUB&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=2&folt=rel)
*LA Barroso,U Hölzle,P Ranganathan*

Main category: Google Scholar

TL;DR: 本章介绍仓库级计算机（WSC）上运行的应用程序，这些应用定义了系统设计权衡，为后续讨论奠定基础


<details>
  <summary>Details</summary>
Motivation: 理解WSC系统设计需要首先了解其运行的应用程序，因为这些应用决定了系统架构的关键权衡和设计决策

Method: 通过概述WSC应用程序的广泛类别和特征，分析这些应用如何影响系统设计参数和性能要求

Result: 建立了WSC应用程序与系统设计之间的关联框架，为后续章节讨论具体设计权衡提供基础

Conclusion: WSC应用程序特性是理解系统设计的关键起点，应用需求直接决定了硬件和软件架构的优化方向

Abstract: The applications that run on warehouse-scale computers (WSCs) define the system design tradeoffs that we will be discussing in the rest of this book. Hence, we first discuss these applications in this chapter. We start with an overview of broad …

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [GCG Attack On A Diffusion LLM](https://arxiv.org/abs/2601.14266)
*Ruben Neyroud,Sam Corley*

Main category: cs.LG

TL;DR: 探索GCG风格对抗提示攻击在扩散语言模型LLaDA上的适用性研究


<details>
  <summary>Details</summary>
Motivation: 虽然大多数LLM是自回归的，但基于扩散的LLM最近作为替代生成方法出现。GCG攻击已被证明对自回归模型有效，但其在扩散语言模型上的适用性尚未充分探索

Method: 对开源扩散LLM LLaDA进行GCG风格对抗提示攻击的探索性研究，评估多种攻击变体，包括前缀扰动和基于后缀的对抗生成，使用AdvBench数据集中的有害提示

Result: 研究提供了关于扩散语言模型鲁棒性和攻击面的初步见解

Conclusion: 研究结果促使在该设置下开发替代的优化和评估策略以进行对抗分析

Abstract: While most LLMs are autoregressive, diffusion-based LLMs have recently emerged as an alternative method for generation. Greedy Coordinate Gradient (GCG) attacks have proven effective against autoregressive models, but their applicability to diffusion language models remains largely unexplored. In this work, we present an exploratory study of GCG-style adversarial prompt attacks on LLaDA (Large Language Diffusion with mAsking), an open-source diffusion LLM. We evaluate multiple attack variants, including prefix perturbations and suffix-based adversarial generation, on harmful prompts drawn from the AdvBench dataset. Our study provides initial insights into the robustness and attack surface of diffusion language models and motivates the development of alternative optimization and evaluation strategies for adversarial analysis in this setting.

</details>


### [6] [Quality or Quantity? Error-Informed Selective Online Learning with Gaussian Processes in Multi-Agent Systems: Extended Version](https://arxiv.org/abs/2601.14275)
*Zewen Yang,Xiaobing Dai,Jiajun Cheng,Yulong Huang,Peng Shi*

Main category: cs.LG

TL;DR: 提出首个分布式选择性在线学习框架EIGP，通过误差信息量化选择高质量GP模型，实现质量优先的协同学习，优于现有分布式GP方法。


<details>
  <summary>Details</summary>
Motivation: 分布式多智能体系统中，机器学习模型的数量与质量交互作用至关重要。现有方法不加选择地包含所有模型进行联合预测存在不合理性，需要优先考虑质量而非数量。

Method: 提出分布式误差信息GP（EIGP）框架：1）每个智能体评估邻居协作者的质量；2）使用选择函数选择预测误差更小的高质量GP模型；3）嵌入贪婪算法（gEIGP）加速预测；4）自适应算法（aEIGP）提高预测精度；5）结合误差信息量化项迭代和数据删除策略实现快速预测和模型更新。

Result: 数值仿真验证了所提方法的有效性，展示了其在多个基准测试中优于现有最先进的分布式GP方法。

Conclusion: 分布式选择性在线学习框架EIGP通过质量优先的模型选择策略，实现了更有效的协同学习，为分布式GP回归提供了新的解决方案。

Abstract: Effective cooperation is pivotal in distributed learning for multi-agent systems, where the interplay between the quantity and quality of the machine learning models is crucial. This paper reveals the irrationality of indiscriminate inclusion of all models on agents for joint prediction, highlighting the imperative to prioritize quality over quantity in cooperative learning. Specifically, we present the first selective online learning framework for distributed Gaussian process (GP) regression, namely distributed error-informed GP (EIGP), that enables each agent to assess its neighboring collaborators, using the proposed selection function to choose the higher quality GP models with less prediction errors. Moreover, algorithmic enhancements are embedded within the EIGP, including a greedy algorithm (gEIGP) for accelerating prediction and an adaptive algorithm (aEIGP) for improving prediction accuracy. In addition, approaches for fast prediction and model update are introduced in conjunction with the error-informed quantification term iteration and a data deletion strategy to achieve real-time learning operations. Numerical simulations are performed to demonstrate the effectiveness of the developed methodology, showcasing its superiority over the state-of-the-art distributed GP methods with different benchmarks.

</details>


### [7] [Which Quantization Should I Use? A Unified Evaluation of llama.cpp Quantization on Llama-3.1-8B-Instruct](https://arxiv.org/abs/2601.14277)
*Uygar Kurt*

Main category: cs.LG

TL;DR: 对llama.cpp量化方案进行统一实证研究，评估Llama-3.1-8B-Instruct模型在3-8位K-quant和传统格式下的性能，提供量化方案选择指南


<details>
  <summary>Details</summary>
Motivation: 量化技术可降低大语言模型的部署门槛，但现有量化格式评估不一致，难以选择合适的方案。需要系统评估llama.cpp量化方案在不同精度下的性能表现，为用户提供实用的选择指导。

Method: 对Llama-3.1-8B-Instruct模型（FP16, GGUF格式）进行统一实证研究，涵盖3-8位K-quant和传统量化格式。评估下游任务性能（推理、知识、指令遵循、真实性基准测试），同时测量困惑度、CPU吞吐量（预填充/解码）、模型大小、压缩率和量化时间。

Result: 研究提供了不同量化方案在性能、效率和资源消耗方面的详细对比数据，揭示了精度与性能之间的权衡关系，量化方案对推理速度、内存使用和任务性能的具体影响。

Conclusion: 该研究为llama.cpp量化方案选择提供了实用指南，帮助用户根据具体使用场景和资源预算做出明智决策，平衡模型性能与部署可行性。

Abstract: Quantization is a practical technique for making large language models easier to deploy by reducing the precision used to store and operate on model weights. This can lower memory use and improve runtime feasibility on constrained hardware, which is especially relevant for users running models locally. Quantization in llama.cpp enables large language models to run on commodity hardware, but available formats are often evaluated inconsistently, making it hard to choose among schemes. We present a unified empirical study of the llama.cpp quantization on a single modern model, Llama-3.1-8B-Instruct (FP16, GGUF), covering 3-8 bit K-quant and legacy formats. We evaluate downstream task performance across standard reasoning, knowledge, instruction-following, and truthfulness benchmarks, and also measure perplexity and CPU throughput (prefill/decoding) alongside model size, compression, and quantization time. Ultimately, this work is a practical guide for choosing a llama.cpp quantization scheme, helping readers make informed, context-aware decisions for their intended use and resource budget.

</details>


### [8] [Beyond Denial-of-Service: The Puppeteer's Attack for Fine-Grained Control in Ranking-Based Federated Learning](https://arxiv.org/abs/2601.14687)
*Zhihao Chen,Zirui Gong,Jianting Ning,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种针对联邦排序学习（FRL）的新型细粒度控制攻击——边缘控制攻击（ECA），该攻击能够精确控制目标模型的准确率下降程度，同时保持正常的收敛轨迹以规避检测。


<details>
  <summary>Details</summary>
Motivation: 虽然联邦排序学习（FRL）通过离散排序机制增强了对抗模型投毒攻击的鲁棒性，但作者发现FRL仍然存在安全漏洞。现有攻击多为拒绝服务（DoS）式攻击，容易被检测，而缺乏能够精确控制攻击效果同时保持隐蔽性的细粒度攻击方法。

Method: 提出边缘控制攻击（ECA），包含两个阶段：1）识别并操纵上升和下降边缘，使全局模型与目标模型对齐；2）扩大选择边界间隙，稳定全局模型在目标准确率水平。攻击通过操纵客户端提交的排序信息实现细粒度控制。

Result: 在7个基准数据集和9种拜占庭鲁棒聚合规则上的实验表明，ECA能够实现细粒度准确率控制，平均误差仅为0.224%，比基线方法提升高达17倍。攻击能够精确控制目标准确率下降程度，同时保持正常的收敛轨迹。

Conclusion: 尽管联邦排序学习具有内在的鲁棒性优势，但仍面临新型细粒度控制攻击的威胁。边缘控制攻击（ECA）揭示了FRL框架的安全漏洞，强调了需要开发更强大的防御机制来应对高级投毒攻击。

Abstract: Federated Rank Learning (FRL) is a promising Federated Learning (FL) paradigm designed to be resilient against model poisoning attacks due to its discrete, ranking-based update mechanism. Unlike traditional FL methods that rely on model updates, FRL leverages discrete rankings as a communication parameter between clients and the server. This approach significantly reduces communication costs and limits an adversary's ability to scale or optimize malicious updates in the continuous space, thereby enhancing its robustness. This makes FRL particularly appealing for applications where system security and data privacy are crucial, such as web-based auction and bidding platforms. While FRL substantially reduces the attack surface, we demonstrate that it remains vulnerable to a new class of local model poisoning attack, i.e., fine-grained control attacks. We introduce the Edge Control Attack (ECA), the first fine-grained control attack tailored to ranking-based FL frameworks. Unlike conventional denial-of-service (DoS) attacks that cause conspicuous disruptions, ECA enables an adversary to precisely degrade a competitor's accuracy to any target level while maintaining a normal-looking convergence trajectory, thereby avoiding detection. ECA operates in two stages: (i) identifying and manipulating Ascending and Descending Edges to align the global model with the target model, and (ii) widening the selection boundary gap to stabilize the global model at the target accuracy. Extensive experiments across seven benchmark datasets and nine Byzantine-robust aggregation rules (AGRs) show that ECA achieves fine-grained accuracy control with an average error of only 0.224%, outperforming the baseline by up to 17x. Our findings highlight the need for stronger defenses against advanced poisoning attacks. Our code is available at: https://github.com/Chenzh0205/ECA

</details>


### [9] [On the Limits of Learned Importance Scoring for KV Cache Compression](https://arxiv.org/abs/2601.14279)
*Brady Steele*

Main category: cs.LG

TL;DR: 研究KV缓存压缩的Speculative Importance Prediction方法，发现1.7M参数的非查询感知评分器在多个任务中未能超越简单基线方法，包括随机选择。


<details>
  <summary>Details</summary>
Motivation: 探索通过学习方法来预测KV缓存中token的重要性，以实现高效的KV缓存压缩，减少大语言模型推理时的内存占用。

Method: 提出Speculative Importance Prediction方法，使用1.7M参数的非查询感知评分器，仅从KV表示预测token重要性，包含多时间跨度前瞻和交叉注意力等复杂架构。

Result: SIP方法在5个随机种子、4个保留水平和3个任务上未能超越简单基线，包括随机选择。基于位置的启发式方法（保留前4个+最后N个token）表现相当或更好。

Conclusion: KV表示中除了位置和预填充注意力之外的信息对重要性预测的价值有限；未来查询和生成轨迹之间的循环依赖关系增加了学习难度；简单启发式方法在实际应用中可能更有效。

Abstract: We investigate learned KV cache compression through Speculative Importance Prediction (SIP), a 1.7M parameter non-query-aware scorer that predicts token importance from KV representations alone. Despite architectural sophistication (multi-horizon lookahead, cross-attention), SIP does not outperform simple baselines, including random selection, across 5 seeds, 4 retention levels, and 3 tasks. Key findings: (1) position-based heuristics (keep first 4 + last N tokens) match or exceed learned approaches; (2) prefill attention provides equivalent signal to complex learned scorers; (3) marginal information in KV representations beyond position and prefill attention appears limited for importance prediction. We hypothesize that circular dependence between future queries and generation trajectories contributes to this difficulty.

</details>


### [10] [RadixMLP - Intra-batch Deduplication for Causal Transformers](https://arxiv.org/abs/2601.15013)
*Michael Feil,Julius Lipp*

Main category: cs.LG

TL;DR: RadixMLP是一种优化因果Transformer模型批量推理的技术，通过消除共享前缀的冗余计算，利用MLP、LayerNorm等组件的逐位置特性，将批次映射到前缀树中压缩计算，实现1.44-1.59倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 因果Transformer模型的批量推理工作负载经常处理具有共享前缀的序列（如系统提示、少样本示例、共享查询），但标准推理引擎独立处理每个序列，对每个共享前缀副本冗余计算相同的MLP激活，造成计算浪费。

Method: RadixMLP利用MLP、LayerNorm、线性投影和嵌入层的逐位置特性，动态将批次映射到前缀树，将共享段收集到压缩表示中进行逐位置计算，仅在注意力边界处散射回结果。该方法是无状态的，在单次前向传播中完成。

Result: 在MS~MARCO v1.1数据集上使用Qwen3模型（0.6B到8B参数）的端到端服务基准测试中，RadixMLP在实际重排序工作负载中实现1.44-1.59倍加速；在具有更长共享前缀的合成基准测试中，最高达到5倍加速。

Conclusion: RadixMLP通过消除共享前缀的冗余计算，显著提升了因果Transformer模型的批量推理效率，特别适用于具有共同前缀的序列处理场景，代码已开源。

Abstract: Batch inference workloads for causal transformer models frequently process sequences that share common prefixes, such as system prompts, few-shot examples, or shared queries. Standard inference engines treat each sequence independently, redundantly recomputing identical MLP activations for every copy of the shared prefix. We introduce RadixMLP, a technique that exploits the position-wise nature of MLPs, LayerNorms, linear projections, and embeddings to eliminate this redundancy. RadixMLP dynamically maps batches to a prefix trie, gathering shared segments into a compressed representation for position-wise computation and scattering results back only at attention boundaries. RadixMLP is stateless and operates within a single forward pass. In end-to-end serving benchmarks on MS~MARCO v1.1 with Qwen3 models (0.6B to 8B parameters), RadixMLP achieves 1.44-1.59$\times$ speedups in realistic reranking workloads, with up to $5\times$ speedups on synthetic benchmarks with longer shared prefixes. Our code is available at https://github.com/michaelfeil/radix-mlp.

</details>


### [11] [Beyond Affinity: A Benchmark of 1D, 2D, and 3D Methods Reveals Critical Trade-offs in Structure-Based Drug Design](https://arxiv.org/abs/2601.14283)
*Kangyu Zheng,Kai Zhang,Jiale Tan,Xuehan Chen,Yingzhou Lu,Zaixi Zhang,Lichao Sun,Marinka Zitnik,Tianfan Fu,Zhiding Liang*

Main category: cs.LG

TL;DR: 该研究建立了首个跨算法类别的结构药物设计基准，评估了15种不同算法模型在药物性质、对接亲和力和构象方面的表现，揭示了各类算法的独特优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 当前结构药物设计领域主要存在搜索算法、深度生成模型和强化学习三类算法，但现有研究通常只比较同一算法类别内的模型，缺乏跨算法类别的系统性比较。为填补这一空白，需要建立一个统一的基准来评估不同算法基础模型的性能。

Method: 建立了一个基准测试框架，评估了15个模型在三种不同算法基础（搜索算法、深度生成模型、强化学习）上的表现。评估指标包括生成分子的药物性质、与指定靶蛋白的对接亲和力以及对接构象。特别强调了1D/2D配体中心药物设计方法可以通过将对接函数视为黑盒oracle应用于SBDD。

Result: 评估揭示了不同模型类别的明显模式：3D结构模型在结合亲和力方面表现出色，但在化学有效性和构象质量方面存在不一致性；1D模型在标准分子指标上表现可靠，但很少达到最佳结合亲和力；2D模型提供平衡性能，保持高化学有效性同时获得中等结合分数。通过多个蛋白靶点的详细分析，识别了每个模型类别的关键改进领域。

Conclusion: 该研究为未来SBDD模型设计提供了建议，强调需要结合不同方法的优势并解决其局限性。特别指出1D/2D配体中心方法在SBDD中的潜力，通常被忽视。所有基准测试代码已开源，为研究者提供了统一的评估框架。

Abstract: Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning. While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce. In this paper, to fill the gap, we establish a benchmark to evaluate the performance of fifteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities and poses with specified target proteins. We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models. We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected. Our evaluation reveals distinct patterns across model categories. 3D structure-based models excel in binding affinities but show inconsistencies in chemical validity and pose quality. 1D models demonstrate reliable performance in standard molecular metrics but rarely achieve optimal binding affinities. 2D models offer balanced performance, maintaining high chemical validity while achieving moderate binding scores. Through detailed analysis across multiple protein targets, we identify key improvement areas for each model category, providing insights for researchers to combine strengths of different approaches while addressing their limitations. All the code that are used for benchmarking is available in https://github.com/zkysfls/2025-sbdd-benchmark

</details>


### [12] [A Comparison of Polynomial-Based Tree Clustering Methods](https://arxiv.org/abs/2601.14285)
*Pengyu Liu,Mariel Vázquez,Nataša Jonoska*

Main category: cs.LG

TL;DR: 本文比较了基于树多项式距离的聚类方法性能，发现基于条目级归一化距离的方法在树结构聚类中准确率最高


<details>
  <summary>Details</summary>
Motivation: 随着测序技术和人工智能的发展，生命科学中产生了大量可用树结构表示的数据（如RNA二级结构、系统发育树等），需要新的树结构数据分析方法。树多项式提供了一种计算高效、可解释且全面的树结构编码方式，但需要评估不同距离度量在树聚类中的性能。

Method: 1. 使用树区分多项式编码树结构；2. 比较基于不同距离度量的树聚类方法性能；3. 实现两种基本的自编码器模型用于树聚类；4. 评估基于Canberra距离及其他距离度量的聚类方法。

Result: 基于条目级归一化距离的方法在所有比较方法中具有最高的聚类准确率。距离基于方法在树结构聚类任务中表现优于自编码器模型。

Conclusion: 树多项式为树结构数据分析提供了有效的编码框架，基于条目级归一化距离的聚类方法在准确率方面表现最佳，为生命科学中的树结构数据分析提供了实用工具。

Abstract: Tree structures appear in many fields of the life sciences, including phylogenetics, developmental biology and nucleic acid structures. Trees can be used to represent RNA secondary structures, which directly relate to the function of non-coding RNAs. Recent developments in sequencing technology and artificial intelligence have yielded numerous biological data that can be represented with tree structures. This requires novel methods for tree structure data analytics. Tree polynomials provide a computationally efficient, interpretable and comprehensive way to encode tree structures as matrices, which are compatible with most data analytics tools. Machine learning methods based on the Canberra distance between tree polynomials have been introduced to analyze phylogenies and nucleic acid structures. In this paper, we compare the performance of different distances in tree clustering methods based on a tree distinguishing polynomial. We also implement two basic autoencoder models for clustering trees using the polynomial. We find that the distance based methods with entry-level normalized distances have the highest clustering accuracy among the compared methods.

</details>


### [13] [Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity](https://arxiv.org/abs/2601.14300)
*Jun Liu,Leo Yu Zhang,Fengpeng Li,Isao Echizen,Jiantao Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种新的硬标签黑盒攻击框架，通过零查询频域初始化和模式驱动优化策略，在仅能获取top-1预测标签的受限反馈下，实现了比现有方法更高的攻击成功率和查询效率。


<details>
  <summary>Details</summary>
Motivation: 硬标签黑盒设置（仅能观察top-1预测标签）是理解模型行为的重要反馈模型，但面临从离散响应中恢复梯度信息的核心挑战。现有硬标签攻击方法缺乏统一理论解释，需要更高效、理论保证的攻击框架。

Method: 提出统一理论视角，将现有符号翻转硬标签攻击解释为隐式近似真实损失梯度的符号。基于此提出新攻击框架：1）零查询频域初始化，在温和假设下获得更高期望余弦相似度；2）模式驱动优化策略，比现有结构化搜索方法显著降低查询复杂度。

Result: 在CIFAR-10、ImageNet、ObjectNet数据集上验证，覆盖标准模型、对抗训练模型、商业API和CLIP模型。方法在攻击成功率和查询效率上均超越SOTA硬标签攻击，尤其在低查询区域表现优异。还能有效绕过Blacklight防御（0%检测率），并泛化到损坏数据、生物医学数据集和密集预测任务。

Conclusion: 该工作为硬标签黑盒攻击提供了统一理论视角，提出的新框架在理论和实证上都优于现有方法，具有更好的泛化能力和防御规避能力，为受限反馈下的模型安全性分析提供了有效工具。

Abstract: Hard-label black-box settings, where only top-1 predicted labels are observable, pose a fundamentally constrained yet practically important feedback model for understanding model behavior. A central challenge in this regime is whether meaningful gradient information can be recovered from such discrete responses. In this work, we develop a unified theoretical perspective showing that a wide range of existing sign-flipping hard-label attacks can be interpreted as implicitly approximating the sign of the true loss gradient. This observation reframes hard-label attacks from heuristic search procedures into instances of gradient sign recovery under extremely limited feedback. Motivated by this first-principles understanding, we propose a new attack framework that combines a zero-query frequency-domain initialization with a Pattern-Driven Optimization (PDO) strategy. We establish theoretical guarantees demonstrating that, under mild assumptions, our initialization achieves higher expected cosine similarity to the true gradient sign compared to random baselines, while the proposed PDO procedure attains substantially lower query complexity than existing structured search approaches. We empirically validate our framework through extensive experiments on CIFAR-10, ImageNet, and ObjectNet, covering standard and adversarially trained models, commercial APIs, and CLIP-based models. The results show that our method consistently surpasses SOTA hard-label attacks in both attack success rate and query efficiency, particularly in low-query regimes. Beyond image classification, our approach generalizes effectively to corrupted data, biomedical datasets, and dense prediction tasks. Notably, it also successfully circumvents Blacklight, a SOTA stateful defense, resulting in a $0\%$ detection rate. Our code will be released publicly soon at https://github.com/csjunjun/DPAttack.git.

</details>


### [14] [Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2601.14327)
*YuanLab. ai,Shawn Wu,Jiangang Luo,Tong Yu,Darcy Chen,Sean Wang,Xudong Zhao,Louie Li,Claire Wang,Hunter He,Carol Wang,Allen Wang*

Main category: cs.LG

TL;DR: 提出LAEP算法用于MoE LLM预训练阶段，通过自适应专家剪枝和重组提高训练效率，在1010B模型上实现48.3%效率提升和33.3%参数减少


<details>
  <summary>Details</summary>
Motivation: MoE LLMs虽然通过减少激活参数实现了更好的准确性，但其预训练阶段存在计算瓶颈，主要原因是专家利用不足和训练效率有限

Method: 提出层自适应专家剪枝算法，在预训练阶段根据token分布统计选择性剪枝未充分利用的专家，并在计算设备间重组专家分布

Result: LAEP有效减少模型规模并显著提升预训练效率，在1010B Base模型预训练中实现48.3%训练效率提升和33.3%参数减少，同时在多个领域保持优异性能

Conclusion: LAEP算法解决了MoE LLM预训练的计算效率问题，通过专家剪枝和重组策略在保持性能的同时大幅提升训练效率

Abstract: Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.

</details>


### [15] [Hierarchical Contextual Uplift Bandits for Catalog Personalization](https://arxiv.org/abs/2601.14333)
*Anupam Agrawal,Rajesh Mohanty,Shamik Bhattacharjee,Abhimanyu Mittal*

Main category: cs.LG

TL;DR: 提出分层上下文提升赌博机框架，解决幻想体育动态环境中传统CB算法频繁重训练问题，通过动态调整上下文粒度实现有效策略迁移，提升推荐质量和收入


<details>
  <summary>Details</summary>
Motivation: 传统上下文赌博机算法在幻想体育等动态环境中表现不佳，用户行为快速变化和外部因素导致的奖励分布剧烈波动需要频繁重训练，需要解决冷启动问题和策略迁移挑战

Method: 提出分层上下文提升赌博机框架，动态调整上下文粒度（从系统级洞察到用户特定上下文），利用上下文相似性促进有效策略迁移，集成提升建模原理

Result: 在Dream11幻想体育平台的大规模A/B测试中，方法显著提升推荐质量，实现0.4%收入增长并改善用户满意度指标；2025年5月部署为默认目录个性化系统后，观察到额外0.5%收入提升

Conclusion: 分层上下文提升赌博机框架有效解决了动态环境中的个性化推荐挑战，通过动态上下文粒度调整和策略迁移机制，在幻想体育平台实现了显著的业务指标提升

Abstract: Contextual Bandit (CB) algorithms are widely adopted for personalized recommendations but often struggle in dynamic environments typical of fantasy sports, where rapid changes in user behavior and dramatic shifts in reward distributions due to external influences necessitate frequent retraining. To address these challenges, we propose a Hierarchical Contextual Uplift Bandit framework. Our framework dynamically adjusts contextual granularity from broad, system-wide insights to detailed, user-specific contexts, using contextual similarity to facilitate effective policy transfer and mitigate cold-start issues. Additionally, we integrate uplift modeling principles into our approach. Results from large-scale A/B testing on the Dream11 fantasy sports platform show that our method significantly enhances recommendation quality, achieving a 0.4% revenue improvement while also improving user satisfaction metrics compared to the current production system. We subsequently deployed this system to production as the default catalog personalization system in May 2025 and observed a further 0.5% revenue improvement.

</details>


### [16] [VJEPA: Variational Joint Embedding Predictive Architectures as Probabilistic World Models](https://arxiv.org/abs/2601.14354)
*Yongchao Huang*

Main category: cs.LG

TL;DR: VJEPA是JEPA的概率泛化，通过变分目标学习未来潜在状态的预测分布，统一了表示学习与PSR和贝叶斯滤波，为高维噪声环境中的可扩展、鲁棒、不确定性感知规划提供基础框架。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用确定性回归目标，掩盖了概率语义，限制了在随机控制中的应用。需要一种概率泛化方法来学习预测分布，实现不确定性估计和鲁棒规划。

Method: 提出变分JEPA(VJEPA)，通过变分目标学习未来潜在状态的预测分布；进一步提出贝叶斯JEPA(BJEPA)，将预测信念分解为学习到的动态专家和模块化先验专家，通过专家乘积实现零样本任务转移和约束满足。

Result: VJEPA/BJEPA能成功过滤高方差干扰因素，避免生成基线中的表示崩溃；提供原则性不确定性估计（如通过采样构建可信区间），同时保持对观测的似然无关性。

Conclusion: VJEPA为高维噪声环境中的可扩展、鲁棒、不确定性感知规划提供了基础框架，统一了表示学习与预测状态表示和贝叶斯滤波，证明序列建模不需要自回归观测似然。

Abstract: Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \emph{Variational JEPA (VJEPA)}, a \textit{probabilistic} generalization that learns a predictive distribution over future latent states via a variational objective. We show that VJEPA unifies representation learning with Predictive State Representations (PSRs) and Bayesian filtering, establishing that sequential modeling does not require autoregressive observation likelihoods. Theoretically, we prove that VJEPA representations can serve as sufficient information states for optimal control without pixel reconstruction, while providing formal guarantees for collapse avoidance. We further propose \emph{Bayesian JEPA (BJEPA)}, an extension that factorizes the predictive belief into a learned dynamics expert and a modular prior expert, enabling zero-shot task transfer and constraint (e.g. goal, physics) satisfaction via a Product of Experts. Empirically, through a noisy environment experiment, we demonstrate that VJEPA and BJEPA successfully filter out high-variance nuisance distractors that cause representation collapse in generative baselines. By enabling principled uncertainty estimation (e.g. constructing credible intervals via sampling) while remaining likelihood-free regarding observations, VJEPA provides a foundational framework for scalable, robust, uncertainty-aware planning in high-dimensional, noisy environments.

</details>


### [17] [Adaptive KDE for Real-Time Thresholding: Prioritized Queues for Financial Crime Investigation](https://arxiv.org/abs/2601.14473)
*Danny Butvinik,Nana Boateng,Achi Hackmon*

Main category: cs.LG

TL;DR: 提出一种自适应阈值方法，将风险评分流转换为审查队列，通过核密度估计和尾质量曲线满足容量约束，无需标签且支持多队列路由


<details>
  <summary>Details</summary>
Motivation: 传统基于top-K或手动调整阈值的方法在动态评分流中难以稳定满足审查容量约束，需要一种自适应、无标签的实时阈值设定方法

Method: 在线自适应核密度估计评分流，将密度转换为尾质量曲线以满足容量约束，通过检测跨带宽的持久密度谷来稳定阈值选择

Result: 在合成、漂移、多模态流上，该方法在保持容量依从性的同时减少了阈值抖动，每个事件更新成本为O(G)，每个活动内存恒定

Conclusion: 该方法提供了一种有效的无标签自适应阈值机制，支持多队列路由和实时操作，适用于动态评分流的审查队列管理

Abstract: We study the problem of converting a stream of risk scores into one or more review queues under explicit intake constraints[cite: 6]. Instead of top-$K$ or manually tuned cutoffs, we fit an online adaptive kernel density to the score stream, transform the density into a tail-mass curve to meet capacity, and ``snap'' the resulting cut to a persistent density valley detected across bandwidths[cite: 7]. The procedure is label-free, supports multi-queue routing, and operates in real time with sliding windows or exponential forgetting[cite: 8]. On synthetic, drifting, multimodal streams, the method achieves competitive capacity adherence while reducing threshold jitter[cite: 9]. Updates cost $O(G)$ per event with constant memory per activity

</details>


### [18] [GPU-accelerated simulated annealing based on p-bits with real-world device-variability modeling](https://arxiv.org/abs/2601.14476)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.LG

TL;DR: 本文提出了一种基于概率比特(p-bit)的GPU加速模拟退火框架，发现器件变异性不仅会降低计算性能，还能通过时序变异性增强算法性能，在MAX-CUT基准测试中实现了比CPU快两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 概率计算使用概率比特(p-bit)为复杂问题求解提供了比传统CMOS逻辑更高效的替代方案。然而，使用磁隧道结(MTJ)等新兴器件实现p-bit会引入器件变异性，传统观点认为这会负面影响计算性能。本研究旨在深入探究器件变异性对算法性能的实际影响，并开发一个能够准确建模真实器件行为的仿真框架。

Method: 本文开发了一个基于CUDA的GPU加速开源模拟退火框架，专门针对p-bit系统设计。该框架能够建模三种关键的器件变异性因素：时序变异性、强度变异性和偏移变异性，以准确反映真实器件的非理想行为。通过GPU并行计算实现了高效的仿真，并在MAX-CUT基准测试问题上进行了验证，问题规模从800到20,000个节点不等。

Result: 研究发现了一个反直觉的结果：器件变异性不仅会降低计算性能，在某些情况下还能通过时序变异性增强算法性能。在计算效率方面，基于CUDA的GPU实现相比CPU实现获得了两个数量级的加速。该框架能够处理大规模优化问题（最高20,000个节点），为概率计算研究提供了一个可扩展且易于使用的工具。

Conclusion: 器件变异性在概率计算中具有双重作用，既能降低也能增强算法性能，特别是通过时序变异性。本文提出的GPU加速模拟退火框架为概率计算研究提供了强大的工具，能够准确建模真实器件行为，并显著加速仿真过程，有望推动优化算法在多个领域的应用发展。

Abstract: Probabilistic computing using probabilistic bits (p-bits) presents an efficient alternative to traditional CMOS logic for complex problem-solving, including simulated annealing and machine learning. Realizing p-bits with emerging devices such as magnetic tunnel junctions (MTJs) introduces device variability, which was expected to negatively impact computational performance. However, this study reveals an unexpected finding: device variability can not only degrade but also enhance algorithm performance, particularly by leveraging timing variability. This paper introduces a GPU-accelerated, open-source simulated annealing framework based on p-bits that models key device variability factors -timing, intensity, and offset- to reflect real-world device behavior. Through CUDA-based simulations, our approach achieves a two-order magnitude speedup over CPU implementations on the MAX-CUT benchmark with problem sizes ranging from 800 to 20,000 nodes. By providing a scalable and accessible tool, this framework aims to advance research in probabilistic computing, enabling optimization applications in diverse fields.

</details>


### [19] [On the Runway Cascade of Transformers for Language Modeling](https://arxiv.org/abs/2601.14522)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: 本文提出"跑道感知重连"机制，通过显式地将间接信息传播路径（跑道）整合到直接注意力中，解决因果Transformer中直接路径与间接路径信息传播不匹配的问题，提升语言建模性能。


<details>
  <summary>Details</summary>
Motivation: 因果Transformer中，因果掩码创建的计算图通过直接路径注意力和由中间令牌形成的间接路径传播信息。最近研究表明，因果Transformer的某些失败模式可能源于这两种信息传播模式之间的不匹配，导致冗余和无关信息在令牌表示中传播，尽管注意力模式已充分学习。

Method: 提出"跑道感知重连"机制，基于每个令牌的"跑道"景观摘要重新连接其注意力模式。该方法将跑道上下文直接整合到每个令牌的直接路径注意力中，使模型能够感知累积的表征影响，实现更平衡的信息传播。该方法不引入额外参数，可无缝集成到标准注意力机制中。

Result: 经验结果表明，重连后的Transformer在通用语言建模方面获得稳定改进，在信息检索和外推能力方面相比标准Transformer表现出明显更强的性能。

Conclusion: 跑道感知重连通过显式整合间接信息传播路径，有效解决了因果Transformer中信息传播不匹配的问题，提升了模型性能，特别是在信息检索和外推任务上表现突出。

Abstract: In decoder-only (causal) transformers, the computation graph created by causal masking routes information through both direct-path attention and indirect paths formed by intermediate tokens. We denote these indirect paths between token pairs as their runways. We argue that certain failure modes of causal transformers as observed by a growing body of recent works are likely exacerbated by a misalignment between these two information propagation modes. We formalize runway cascade as a phenomenon whereby this misalignment results in redundancies and irrelevant information cascading to token representations despite adequately learned attention patterns. As a solution, we propose runway-aware rewiring as a more explicit way of incorporating runway context directly into each token's direct-path attention. This mechanism re-wires the attention pattern for each token based on a summary of its runway landscape, enabling awareness of accumulating representational influences and allowing for more balanced information propagation. Our proposed methodology introduces no additional parameters and can seamlessly be integrated into standard attention mechanism. Empirically, our rewired transformer results in steady improvements in general language modeling as well as noticeably stronger information retrieval and extrapolation abilities compared to standard transformers.

</details>


### [20] [Search over Self-Edit Strategies for LLM Adaptation](https://arxiv.org/abs/2601.14532)
*Alistair Cheong,Haolin Cong,Tyler Yang,Dustin Miao*

Main category: cs.LG

TL;DR: 该研究探索了LLM能否利用任务反馈自主决定如何更新权重，通过放松SEAL框架的固定模板约束，让模型生成自己的自编辑模板来控制训练数据和超参数。


<details>
  <summary>Details</summary>
Motivation: 许多基于LLM的开放式搜索系统冻结了提出改进方案的基础模型，这可能成为长期进展的瓶颈。虽然已有研究探索在测试时更新提案模型，但更新策略通常仍是人工指定的。因此，本研究旨在探索LLM是否能够利用任务反馈自主决定如何更新其权重。

Method: 研究采用SEAL框架作为测试平台，放松其固定人工模板约束，允许模型生成自己的自编辑模板，从而让模型能够控制其训练数据和关键NTP超参数。研究了两种变体：无存档版本和基于轻量级历史模板存档的条件生成版本。实验在SEAL的单段落知识整合设置中使用Qwen3-8B模型在SQuAD数据集上进行。

Result: 在SEAL的单段落知识整合设置中，无存档变体的表现与较弱的"Implications"基线相当，而存档变体优于"Implications"基线，接近最强的人工设计"Rewrite"基线但未超越它。进一步分析发现，简单的存档虽然能提供短期鲁棒性，但也会加速同质化，表明可能需要显式的新颖性压力才能持续超越精心优化的人工策略。

Conclusion: LLM能够在一定程度上利用任务反馈自主决定权重更新策略，但简单的存档机制虽然提供短期鲁棒性，却可能加速同质化。要持续超越精心优化的人工策略，可能需要引入显式的新颖性压力机制。

Abstract: Many LLM-based open-ended search systems freeze the foundation model that proposes improvements to existing solutions, which may bottleneck long-run progress. Recent work has explored updating the proposal model at test time [arXiv:2511.23473], but the update strategy is still typically hand-specified. Therefore, this study investigated whether an LLM can use task feedback to decide how it should update its weights. For tractability, we focused on the simpler case where there is only one round of self-improvement, and restricted the update operator to self-supervised next token prediction (NTP), leaving the model freedom in choosing its training data and key NTP hyperparameters. Using the Self-Adapting Language Models (SEAL) [arXiv:2506.10943] framework as a testbed, we relaxed its fixed human template constraint and allowed the model to generate its own self-edit templates, thereby giving it more control over its training data and hyperparameters. Two variants were studied, differing in whether template generation was conditioned on a lightweight archive of past templates. In SEAL's Single-Passage Knowledge Incorporation setting with Qwen3-8B on SQuAD [arXiv:1606.05250], the no-archive variant performed comparably to the weaker "Implications" baseline, while the archive variant outperformed "Implications" and approached the strongest human-designed "Rewrite" baseline without surpassing it. Further analysis of collapse in the model's exploration revealed that a naive archive can confer some short-term robustness but can also accelerate homogenization, suggesting that explicit novelty pressure may be required to consistently advance beyond carefully optimized human strategies. Our code is available at https://github.com/cheongalc/search-self-edit-strategies .

</details>


### [21] [QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design](https://arxiv.org/abs/2601.14549)
*Nilesh Prasad Pandey,Jangseon Park,Onat Gungor,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: QMC提出了一种无需重新训练的量化方法，结合新型异构内存架构，通过识别SLM中的内点和离群权重，分别存储在ReRAM和MRAM中，显著降低了内存使用、数据传输、能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 在边缘平台上部署小型语言模型面临内存、延迟和能耗限制。传统量化方法受新兴非易失性存储器设备噪声影响，而传统内存层次结构（SRAM密度低、DRAM带宽争用、Flash推理期间不活跃）限制了效率，需要针对LLM推理的混合内存组织。

Method: 提出QMC（Outlier-aware Quantization with Memory Co-design）：1）无需重新训练的量化方法；2）识别SLM中的内点权重和关键离群权重；3）内点权重存储在紧凑的多级ReRAM中；4）关键离群权重保存在高精度片上MRAM中；5）缓解噪声引起的性能退化。

Result: 在语言建模和推理基准测试中，QMC优于或匹配使用先进算法和混合数据格式的最先进量化方法，同时在算法评估和实际部署设置下实现更大压缩。相比FP16，内存使用减少6.3x-7.3x，外部数据传输减少7.6x，能耗降低11.7x，延迟降低12.5x。

Conclusion: QMC作为一种可扩展、可部署的协同设计，为高效设备端推理提供了解决方案，通过量化与内存架构的协同设计，有效解决了边缘AI平台上的资源限制问题。

Abstract: Deploying Small Language Models (SLMs) on edge platforms is critical for real-time, privacy-sensitive generative AI, yet constrained by memory, latency, and energy budgets. Quantization reduces model size and cost but suffers from device noise in emerging non-volatile memories, while conventional memory hierarchies further limit efficiency. SRAM provides fast access but has low density, DRAM must simultaneously accommodate static weights and dynamic KV caches, which creates bandwidth contention, and Flash, although dense, is primarily used for initialization and remains inactive during inference. These limitations highlight the need for hybrid memory organizations tailored to LLM inference. We propose Outlier-aware Quantization with Memory Co-design (QMC), a retraining-free quantization with a novel heterogeneous memory architecture. QMC identifies inlier and outlier weights in SLMs, storing inlier weights in compact multi-level Resistive-RAM (ReRAM) while preserving critical outliers in high-precision on-chip Magnetoresistive-RAM (MRAM), mitigating noise-induced degradation. On language modeling and reasoning benchmarks, QMC outperforms and matches state-of-the-art quantization methods using advanced algorithms and hybrid data formats, while achieving greater compression under both algorithm-only evaluation and realistic deployment settings. Specifically, compared against SoTA quantization methods on the latest edge AI platform, QMC reduces memory usage by 6.3x-7.3x, external data transfers by 7.6x, energy by 11.7x, and latency by 12.5x when compared to FP16, establishing QMC as a scalable, deployment-ready co-design for efficient on-device inference.

</details>


### [22] [Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation](https://arxiv.org/abs/2601.14590)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 该论文评估了使用大型语言模型（LLMs）生成反事实解释（CFEs）的方法，在临床数据上验证了其干预质量、特征多样性和数据增强效果，发现微调后的LLMs能生成高质量、临床可操作的反事实解释。


<details>
  <summary>Details</summary>
Motivation: 反事实解释通过识别改变机器学习模型预测所需的最小、可操作的改变，提供以人为中心的解释性。然而，现有方法在生成临床可操作、语义连贯的反事实方面存在局限，需要探索LLMs在反事实生成中的潜力。

Method: 使用GPT-4（零样本和少样本）以及两个开源模型（BioMistral-7B和LLaMA-3.1-8B）在预训练和微调配置下生成反事实解释。在多模态AI-READI临床数据集上进行评估，从干预质量、特征多样性和增强效果三个维度分析。与DiCE、CFNOW、NICE等优化基线方法进行比较。

Result: 微调后的LLMs（特别是LLaMA-3.1-8B）生成的反事实具有高合理性（高达99%）、强有效性（高达0.99）和现实、行为可修改的特征调整。在标签稀缺设置下用于数据增强时，LLM生成的反事实显著恢复分类器性能，在三种稀缺场景下平均F1恢复20%。相比优化基线方法，LLMs提供了更灵活、模型无关的方法，生成更具临床可操作性和语义连贯性的反事实。

Conclusion: LLM驱动的反事实解释在可解释干预设计和传感器数字健康中的数据高效模型训练方面展现出巨大潜力。SenseCF方法通过微调LLM生成有效的、有代表性的反事实解释，并补充不平衡数据集中的少数类，从而改善模型训练、提升模型鲁棒性和预测性能。

Abstract: Counterfactual explanations (CFEs) provide human-centric interpretability by identifying the minimal, actionable changes required to alter a machine learning model's prediction. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. We conduct a comprehensive evaluation of CF generation using large language models (LLMs), including GPT-4 (zero-shot and few-shot) and two open-source models-BioMistral-7B and LLaMA-3.1-8B, in both pretrained and fine-tuned configurations. Using the multimodal AI-READI clinical dataset, we assess CFs across three dimensions: intervention quality, feature diversity, and augmentation effectiveness. Fine-tuned LLMs, particularly LLaMA-3.1-8B, produce CFs with high plausibility (up to 99%), strong validity (up to 0.99), and realistic, behaviorally modifiable feature adjustments. When used for data augmentation under controlled label-scarcity settings, LLM-generated CFs substantially restore classifier performance, yielding an average 20% F1 recovery across three scarcity scenarios. Compared with optimization-based baselines such as DiCE, CFNOW, and NICE, LLMs offer a flexible, model-agnostic approach that generates more clinically actionable and semantically coherent counterfactuals. Overall, this work demonstrates the promise of LLM-driven counterfactuals for both interpretable intervention design and data-efficient model training in sensor-based digital health.
  Impact: SenseCF fine-tunes an LLM to generate valid, representative counterfactual explanations and supplement minority class in an imbalanced dataset for improving model training and boosting model robustness and predictive performance

</details>


### [23] [Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective](https://arxiv.org/abs/2601.14599)
*Xiao Hu,Hong Xie,Tao Tan,Defu Lian,Jianyu Han*

Main category: cs.LG

TL;DR: 本文通过自下而上的实验流程，系统分析了强化学习微调LLMs中各种优化选择的角色和瓶颈，揭示了设计选择的新理解


<details>
  <summary>Details</summary>
Motivation: 现有大量启发式方法用于优化LLMs的强化学习微调，但存在不一致的声明，两个基本问题缺乏清晰理解：1) 每个优化选择的作用是什么？2) 哪些是瓶颈？本文旨在澄清这些问题

Method: 提出自下而上的实验流程：底层采用极简配置（单一训练数据、每轮一次rollout、奖励直接作为学习信号而无优势函数设计），该配置连接到大离散动作空间的多臂老虎机学习；上层逐步扩展极简配置，逐层检查每个设计选择的作用

Result: 在三个LLMs和两个推理数据集上的实验结果表明，不仅揭示了设计选择的新理解，还产生了塑造该领域的重要见解

Conclusion: 通过系统化的实验分析，本文澄清了LLMs强化学习微调中优化选择的作用和瓶颈，为该领域提供了理论基础和实践指导

Abstract: A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.

</details>


### [24] [Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum](https://arxiv.org/abs/2601.14603)
*Jingru Li,Yibo Fan,Huan Li*

Main category: cs.LG

TL;DR: Muon优化器通过正交动量更新加速LLM预训练，提出两种变体Muon-NSR和Muon-VS，在GPT-2和LLaMA预训练中比AdamW和Muon基准收敛更快、验证损失更低


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练计算成本高昂，优化器效率成为重要实践考虑。现有Adam优化器可视为方差自适应符号更新算法，但仍有改进空间以加速收敛

Method: 提出Muon优化器，采用正交动量更新作为元素级符号算子的矩阵模拟。进一步提出两种变体：Muon-NSR应用信噪比调制，Muon-VS执行基于方差的缩放而不引入额外超参数

Result: 在GPT-2和LLaMA预训练实验中，Muon-NSR和Muon-VS加速收敛，持续获得比精心调优的AdamW和Muon基准更低的验证损失。对于LLaMA-1.2B模型，达到目标验证损失所需的迭代次数减少1.36倍

Conclusion: Muon优化器及其变体通过正交动量更新和方差自适应归一化，显著加速LLM预训练收敛，提供比现有优化器更高效的训练方案

Abstract: Large Language Models (LLMs) achieve competitive performance across diverse natural language processing (NLP) tasks, yet pretraining is computationally demanding, making optimizer efficiency an important practical consideration. Muon accelerates LLM pretraining via orthogonal momentum updates that serve as a matrix analogue of the element-wise sign operator. Motivated by the recent perspective that Adam is a variance-adaptive sign update algorithm, we propose two variants of Muon, Muon-NSR and Muon-VS, which apply variance-adaptive normalization to momentum before orthogonalization. Muon-NSR applies noise-to-signal ratio (NSR) modulation, while Muon-VS performs variance-based scaling without introducing additional hyperparameters. Experiments on GPT-2 and LLaMA pretraining demonstrate that our proposed methods accelerate convergence and consistently achieve lower validation loss than both competitive, well-tuned AdamW and Muon baselines. For example, on the LLaMA-1.2B model, Muon-NSR and Muon-VS reduce the iterations required to reach the target validation loss by $1.36\times$ relative to the well-tuned Muon following the recent benchmark.

</details>


### [25] [Efficient Imputation for Patch-based Missing Single-cell Data via Cluster-regularized Optimal Transport](https://arxiv.org/abs/2601.14653)
*Yuyu Liu,Jiannan Yang,Ziyang Yu,Weishen Pan,Fei Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: CROT：基于最优传输的插补算法，专门处理表格数据中的块状缺失数据，在保持高精度的同时显著降低运行时间


<details>
  <summary>Details</summary>
Motivation: 单细胞测序数据中的缺失数据对提取生物学见解构成重大挑战。现有插补方法通常假设数据均匀性和完整性，难以处理大规模块状缺失数据的情况

Method: 提出CROT算法，基于最优传输理论设计，专门处理表格格式中的块状缺失数据。该方法能有效捕捉存在显著缺失情况下的底层数据结构

Result: CROT在保持优异插补精度的同时，显著减少了运行时间，展示了其在大规模数据集上的可扩展性和效率优势

Conclusion: CROT为异构高维数据集中的结构化数据缺失问题提供了稳健的解决方案，解决了生物和临床数据分析中的关键挑战

Abstract: Missing data in single-cell sequencing datasets poses significant challenges for extracting meaningful biological insights. However, existing imputation approaches, which often assume uniformity and data completeness, struggle to address cases with large patches of missing data. In this paper, we present CROT, an optimal transport-based imputation algorithm designed to handle patch-based missing data in tabular formats. Our approach effectively captures the underlying data structure in the presence of significant missingness. Notably, it achieves superior imputation accuracy while significantly reducing runtime, demonstrating its scalability and efficiency for large-scale datasets. This work introduces a robust solution for imputation in heterogeneous, high-dimensional datasets with structured data absence, addressing critical challenges in both biological and clinical data analysis. Our code is available at Anomalous Github.

</details>


### [26] [CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation](https://arxiv.org/abs/2601.14695)
*Yutong Chen,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: CoScale-RL是一种用于训练大型推理模型的新型扩展策略，通过扩展解决方案和rollout计算来提高数据效率和训练稳定性，无需大量监督微调数据集。


<details>
  <summary>Details</summary>
Motivation: 训练大型推理模型通常不稳定且难以预测，特别是在处理困难问题或基础模型较弱的情况下。现有的后训练扩展策略在这些情况下仍有改进空间，需要更高效的数据和计算利用方法。

Method: 提出CoScale-RL扩展策略：1) 扩展解决方案：为每个问题收集多个解决方案而非简单扩大数据集；2) 扩展rollout计算：稳定强化学习训练；3) 使用Re-distillation模型合并技术维持计算效率。该方法无需大量监督微调数据集即可提升模型能力边界。

Result: 在四个基准测试上平均获得3.76倍的准确率提升，显著提高了数据和计算效率，能够改进大型推理模型的能力边界。

Conclusion: CoScale-RL为提升大型推理模型的推理能力提供了新的扩展方向，通过更高效的数据和计算利用策略解决了训练不稳定性和效率问题。

Abstract: Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.

</details>


### [27] [Case-Guided Sequential Assay Planning in Drug Discovery](https://arxiv.org/abs/2601.14710)
*Tianchi Chen,Jan Bima,Sean L. Wu,Otto Ritter,Bingjia Yang,Xiang Yu*

Main category: cs.LG

TL;DR: IBMDP框架为无模拟器的药物发现实验序列优化提供解决方案，通过隐式贝叶斯模型和集成MCTS规划，在资源受限下实现高效决策。


<details>
  <summary>Details</summary>
Motivation: 药物发现中的实验序列优化面临严重不确定性和资源约束，标准强化学习因缺乏环境模拟器或转移数据而受限，需要仅依赖静态历史数据库进行规划。

Method: 提出隐式贝叶斯马尔可夫决策过程（IBMDP）框架，通过相似历史结果构建非参数信念分布形成隐式转移动态模型，采用贝叶斯信念更新和集成MCTS规划来平衡信息增益与资源效率。

Result: 在真实世界CNS药物发现任务中，IBMDP相比现有启发式方法减少高达92%的资源消耗同时保持决策置信度；在合成环境中，IBMDP与可计算最优策略的匹配度显著高于确定性值迭代替代方案。

Conclusion: IBMDP为数据丰富但模拟器匮乏领域的序列实验设计提供了实用解决方案，其集成规划器在决策质量上优于确定性方法。

Abstract: Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s')$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.

</details>


### [28] [PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning](https://arxiv.org/abs/2601.14716)
*Yao Lu,Dengdong Fan,Jianzheng Nie,Fan Xu,Jie Chen,Bin Zhou,Yonghong Tian*

Main category: cs.LG

TL;DR: PCL-Reasoner-V1.5是一个基于Qwen2.5-32B构建的320亿参数数学推理大语言模型，采用监督微调加强化学习的训练方法，在AIME数学竞赛数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个专门用于数学推理的高性能大语言模型，探索比标准在线强化学习方法更稳定高效的训练范式。

Method: 基于Qwen2.5-32B模型，采用监督微调(SFT)后接强化学习(RL)的两阶段训练方法，核心创新是提出的离线RL方法，相比GRPO等在线RL方法具有更好的训练稳定性和效率。

Result: 在AIME 2024上达到90.9%的平均准确率，在AIME 2025上达到85.6%的平均准确率，在基于Qwen2.5-32B后训练的模型中达到最先进性能。

Conclusion: 离线强化学习是一种稳定高效的范式，能够显著提升大语言模型的推理能力，该研究为数学推理模型的开发提供了有效方法。

Abstract: We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.

</details>


### [29] [Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models](https://arxiv.org/abs/2601.14758)
*Injin Kong,Hyoungjoon Lee,Yohan Jo*

Main category: cs.LG

TL;DR: 该研究通过电路分析比较自回归模型(ARMs)与其掩码扩散模型(MDMs)对应版本，发现扩散后训练不仅调整参数，还从根本上重组内部计算机制，支持非序列全局规划。


<details>
  <summary>Details</summary>
Motivation: 探索将预训练自回归模型(ARMs)后训练为掩码扩散模型(MDMs)的内部算法转变机制，研究这种范式转换是否让MDMs获得真正的双向推理能力，还是仅仅重新包装了自回归启发式方法。

Method: 对ARMs及其MDM对应版本进行对比电路分析，从结构和语义两个维度考察机制转变：结构上分析局部因果依赖任务与全局规划任务中的电路差异；语义上识别从局部专业化到分布式整合的转变。

Result: 发现系统性的"机制转变"：对于局部因果依赖主导的任务，MDMs基本保留自回归电路；对于全局规划任务，MDMs放弃初始化路径，表现出明显的重新布线特征，包括增加早期层处理。语义上观察到从ARMs中尖锐的局部专业化向MDMs中分布式整合的过渡。

Conclusion: 扩散后训练不仅仅是调整模型参数，而是从根本上重组内部计算机制以支持非序列全局规划，表明MDMs确实获得了超越简单自回归重新包装的新的推理能力。

Abstract: Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative circuit analysis of ARMs and their MDM counterparts. Our analysis reveals a systematic "mechanism shift" dependent on the structural nature of the task. Structurally, we observe a distinct divergence: while MDMs largely retain autoregressive circuitry for tasks dominated by local causal dependencies, they abandon initialized pathways for global planning tasks, exhibiting distinct rewiring characterized by increased early-layer processing. Semantically, we identify a transition from sharp, localized specialization in ARMs to distributed integration in MDMs. Through these findings, we conclude that diffusion post-training does not merely adapt model parameters but fundamentally reorganizes internal computation to support non-sequential global planning.

</details>


### [30] [Anytime Optimal Decision Tree Learning with Continuous Features](https://arxiv.org/abs/2601.14765)
*Harold Kiossou,Pierre Schaus,Siegfried Nijssen*

Main category: cs.LG

TL;DR: 提出一种基于有限差异搜索的随时完整算法，用于学习具有连续特征的最优决策树，改善现有深度优先搜索方法的随时性能


<details>
  <summary>Details</summary>
Motivation: 现有学习连续特征最优决策树的精确算法采用深度优先搜索策略，虽然能找到最优解，但计算时间随深度增加而急剧增长，且早期中断时得到的树往往高度不平衡且次优，导致较差的随时性能

Method: 提出基于有限差异搜索的随时完整方法，将计算努力更均匀地分布在整个树结构中，而不是像深度优先搜索那样先完全优化左子树再探索右子树

Result: 实验结果表明，该方法在随时性能方面优于现有方法，能够在任何中断点提供高质量的决策树

Conclusion: 有限差异搜索策略能够有效改善学习连续特征最优决策树的随时性能，提供更平衡的搜索过程，确保在任何计算时间限制下都能获得高质量的解

Abstract: In recent years, significant progress has been made on algorithms for learning optimal decision trees, primarily in the context of binary features. Extending these methods to continuous features remains substantially more challenging due to the large number of potential splits for each feature. Recently, an elegant exact algorithm was proposed for learning optimal decision trees with continuous features; however, the rapidly increasing computational time limits its practical applicability to shallow depths (typically 3 or 4). It relies on a depth-first search optimization strategy that fully optimizes the left subtree of each split before exploring the corresponding right subtree. While effective in finding optimal solutions given sufficient time, this strategy can lead to poor anytime behavior: when interrupted early, the best-found tree is often highly unbalanced and suboptimal. In such cases, purely greedy methods such as C4.5 may, paradoxically, yield better solutions. To address this limitation, we propose an anytime, yet complete approach leveraging limited discrepancy search, distributing the computational effort more evenly across the entire tree structure, and thus ensuring that a high-quality decision tree is available at any interruption point. Experimental results show that our approach outperforms the existing one in terms of anytime performance.

</details>


### [31] [Reflecting in the Reflection: Integrating a Socratic Questioning Framework into Automated AI-Based Question Generation](https://arxiv.org/abs/2601.14798)
*Ondřej Holub,Essi Ryymin,Rodrigo Alves*

Main category: cs.LG

TL;DR: 提出"反思中的反思"框架，利用大语言模型通过双智能体对话自动生成反思问题，在中学ICT教学中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 设计高质量的反思问题对教学很重要，但教师需要花费大量时间且支持不均衡。需要一种自动化方法来生成有效的反思问题，减轻教师负担并提高教学质量。

Method: 采用"反思中的反思"框架，协调两个角色专业化智能体：学生-教师和教师-教育者。通过苏格拉底式多轮对话迭代优化问题，输入包括教师指定的主题、关键概念、学生水平和可选教学材料。学生-教师提出候选问题并给出简要理由，教师-教育者从清晰度、深度、相关性、参与度和概念互联性五个维度评估，仅提供针对性指导问题或停止信号。

Result: 在中学ICT教学中评估，使用GPT-4o-mini作为骨干模型，GPT-4级LLM作为外部评估器进行成对比较。结果显示：1) 动态停止结合上下文信息优于固定5步或10步优化，过长对话易偏离或过度复杂化；2) 双智能体协议生成的问题在相关性、深度和整体质量上显著优于单次提示基线。

Conclusion: 反思中的反思框架能够有效生成高质量的反思问题，动态停止机制和上下文信息的结合是关键成功因素。该方法为教师提供了实用的自动化工具，可减轻工作负担并提升教学效果。

Abstract: Designing good reflection questions is pedagogically important but time-consuming and unevenly supported across teachers. This paper introduces a reflection-in-reflection framework for automated generation of reflection questions with large language models (LLMs). Our approach coordinates two role-specialized agents, a Student-Teacher and a Teacher-Educator, that engage in a Socratic multi-turn dialogue to iteratively refine a single question given a teacher-specified topic, key concepts, student level, and optional instructional materials. The Student-Teacher proposes candidate questions with brief rationales, while the Teacher-Educator evaluates them along clarity, depth, relevance, engagement, and conceptual interconnections, responding only with targeted coaching questions or a fixed signal to stop the dialogue. We evaluate the framework in an authentic lower-secondary ICT setting on the topic, using GPT-4o-mini as the backbone model and a stronger GPT- 4-class LLM as an external evaluator in pairwise comparisons of clarity, relevance, depth, and overall quality. First, we study how interaction design and context (dynamic vs.fixed iteration counts; presence or absence of student level and materials) affect question quality. Dynamic stopping combined with contextual information consistently outperforms fixed 5- or 10-step refinement, with very long dialogues prone to drift or over-complication. Second, we show that our two-agent protocol produces questions that are judged substantially more relevant and deeper, and better overall, than a one-shot baseline using the same backbone model.

</details>


### [32] [Statistical Learning Theory for Distributional Classification](https://arxiv.org/abs/2601.14818)
*Christian Fiedler*

Main category: cs.LG

TL;DR: 该论文研究了在分布输入的两阶段采样设置下的监督学习问题，特别关注使用支持向量机（SVM）进行分布输入分类。通过核均值嵌入将分布映射到希尔伯特空间，建立了新的oracle不等式，推导了收敛性和学习速率结果，并针对高斯核和铰链损失提出了新的噪声假设。


<details>
  <summary>Details</summary>
Motivation: 在基于学习的医学筛查或因果学习等应用中，输入是概率分布，但在学习阶段只能获得这些分布的样本（两阶段采样设置）。现有方法使用核均值嵌入将分布映射到希尔伯特空间，然后应用SVM等标准核方法，但这种方法缺乏充分的理论分析。

Method: 采用核均值嵌入（KMEs）将分布或样本嵌入到希尔伯特空间，然后应用支持向量机（SVM）进行分类。建立了新的oracle不等式，推导了收敛性和学习速率理论结果。特别针对使用高斯核和铰链损失的SVM，提出了新的噪声假设变体。

Result: 建立了新的oracle不等式，证明了方法的收敛性并推导了学习速率。针对高斯核和铰链损失的SVM，在新的噪声假设下获得了学习速率结果。开发的技术工具（如高斯核在希尔伯特空间上的新特征空间）具有独立的理论价值。

Conclusion: 该研究为分布输入的两阶段采样设置下的SVM分类提供了坚实的理论分析框架，建立了收敛性和学习速率保证，提出的技术工具对相关领域有独立的理论贡献。

Abstract: In supervised learning with distributional inputs in the two-stage sampling setup, relevant to applications like learning-based medical screening or causal learning, the inputs (which are probability distributions) are not accessible in the learning phase, but only samples thereof. This problem is particularly amenable to kernel-based learning methods, where the distributions or samples are first embedded into a Hilbert space, often using kernel mean embeddings (KMEs), and then a standard kernel method like Support Vector Machines (SVMs) is applied, using a kernel defined on the embedding Hilbert space. In this work, we contribute to the theoretical analysis of this latter approach, with a particular focus on classification with distributional inputs using SVMs. We establish a new oracle inequality and derive consistency and learning rate results. Furthermore, for SVMs using the hinge loss and Gaussian kernels, we formulate a novel variant of an established noise assumption from the binary classification literature, under which we can establish learning rates. Finally, some of our technical tools like a new feature space for Gaussian kernels on Hilbert spaces are of independent interest.

</details>


### [33] [Adaptive Exponential Integration for Stable Gaussian Mixture Black-Box Variational Inference](https://arxiv.org/abs/2601.14855)
*Baojun Che,Yifan Chen,Daniel Zhengyu Huang,Xinying Mao,Weijie Wang*

Main category: cs.LG

TL;DR: 提出了一种稳定高效的BBVI框架，结合自然梯度、指数积分器和自适应步长，用于高斯混合族近似复杂后验分布


<details>
  <summary>Details</summary>
Motivation: 黑盒变分推断(BBVI)使用高斯混合族可以灵活近似复杂后验分布，但标准数值优化方法存在不稳定和低效的问题，需要更稳定高效的优化框架

Method: 结合三个关键组件：(1)通过自然梯度公式实现仿射不变预条件；(2)无条件保持协方差矩阵正定性的指数积分器；(3)自适应时间步长以适应不同的预热和收敛阶段

Result: 对于高斯后验，证明了在无噪声设置下的指数收敛性和蒙特卡洛估计下的几乎必然收敛，数值实验在多模态分布、Neal的多尺度漏斗和基于PDE的贝叶斯反问题中验证了方法的有效性

Conclusion: 提出的框架为BBVI提供了稳定高效的优化方法，具有与流形优化和镜像下降的自然联系，自适应时间步长对确保收敛至关重要

Abstract: Black-box variational inference (BBVI) with Gaussian mixture families offers a flexible approach for approximating complex posterior distributions without requiring gradients of the target density. However, standard numerical optimization methods often suffer from instability and inefficiency. We develop a stable and efficient framework that combines three key components: (1) affine-invariant preconditioning via natural gradient formulations, (2) an exponential integrator that unconditionally preserves the positive definiteness of covariance matrices, and (3) adaptive time stepping to ensure stability and to accommodate distinct warm-up and convergence phases. The proposed approach has natural connections to manifold optimization and mirror descent. For Gaussian posteriors, we prove exponential convergence in the noise-free setting and almost-sure convergence under Monte Carlo estimation, rigorously justifying the necessity of adaptive time stepping. Numerical experiments on multimodal distributions, Neal's multiscale funnel, and a PDE-based Bayesian inverse problem for Darcy flow demonstrate the effectiveness of the proposed method.

</details>


### [34] [Strategic Doctrine Language Models (sdLM): A Learning-System Framework for Doctrinal Consistency and Geopolitical Forecasting](https://arxiv.org/abs/2601.14862)
*Olaf Yunus Laitinen Imanov,Taner Yilmaz,Derya Umut Kulali*

Main category: cs.LG

TL;DR: sdLM框架通过多文档注意力、时间编码和教义一致性层，在战略推理中实现教义一致性约束和校准不确定性，提升长期预测和计划合理性。


<details>
  <summary>Details</summary>
Motivation: 现有通用大语言模型在战略推理任务中缺乏对多文档信息的整合能力，无法保证教义一致性，且不确定性校准不足，需要专门框架来改进长期战略预测质量。

Method: 提出sdLM框架，结合多文档注意力机制处理多个战略文档，时间编码捕捉时序信息，教义一致性层确保推理符合既定教义约束，同时实现校准的不确定性估计。

Result: 在三个基准测试中表现优异：专家小组战略场景评分(N=47)显示更高战略质量；336份教义出版物(12,847条声明)上保持教义一致性；127个历史反事实(1945-2020)的12-60个月地缘政治预测中优于通用LLM基线，与人类专家在长期判断上竞争。

Conclusion: sdLM框架有效提升了战略推理的质量、教义一致性和不确定性校准，为战略规划和长期预测提供了可靠的技术基础，并通过消融实验和部署特性分析验证了各组件贡献。

Abstract: We introduce Strategic Doctrine Language Models (sdLM), a learning-system framework for multi-document strategic reasoning with doctrinal consistency constraints and calibrated uncertainty. The approach combines multi-document attention, temporal encoding, and a doctrine-consistency layer to improve long-horizon forecasting and plan plausibility while reducing severe doctrinal violations. We evaluate sdLM using (i) expert-panel scoring of strategic scenarios (N=47), (ii) doctrine consistency on 336 doctrine publications (12,847 statements), and (iii) geopolitical forecasting on 127 historical counterfactuals (1945-2020) across 12-60 month horizons. Across these benchmarks, sdLM achieves higher strategic quality and better calibration than strong general-purpose LLM baselines, and remains competitive with human experts on long-horizon judgments. We further report ablations, scaling trends, and deployment-oriented performance/latency characteristics to clarify which components drive improvements and how they translate to operational settings.

</details>


### [35] [What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study](https://arxiv.org/abs/2601.14888)
*Keyu Lv,Manyi Zhang,Xiaobo Xia,Jingchen Ni,Shannan Yan,Xianzhi Yu,Lu Hou,Chun Yuan,Haoli Bai*

Main category: cs.LG

TL;DR: 该研究系统探索了推理模型的量化感知训练，提出了Reasoning-QAT工作流，在低比特设置下显著提升推理效率同时保持准确性


<details>
  <summary>Details</summary>
Motivation: 推理模型在复杂任务上表现出色，但推理速度慢且token效率低。后训练量化在低比特设置下通常导致准确性大幅下降，特别是在推理任务中。需要探索量化感知训练方法来提高推理模型的效率同时保持性能。

Method: 系统实证研究推理模型的量化感知训练，包括：1）使用知识蒸馏作为监督微调或强化学习的鲁棒目标；2）利用PTQ作为QAT的强初始化；3）探索量化模型的强化学习可行性；4）对齐PTQ校准域与QAT训练域。整合这些发现形成Reasoning-QAT优化工作流。

Result: Reasoning-QAT在多个LLM骨干和推理数据集上一致优于最先进的PTQ方法。例如在Qwen3-0.6B上，MATH-500上超越GPTQ 44.53%，在2比特设置下持续恢复性能。

Conclusion: 量化感知训练是提升推理模型效率的有效方法，通过系统的工作流设计可以在低比特设置下显著改善性能，为高效推理模型部署提供实用解决方案。

Abstract: Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.

</details>


### [36] [Tailoring Adverse Event Prediction in Type 1 Diabetes with Patient-Specific Deep Learning Models](https://arxiv.org/abs/2601.14917)
*Giorgia Rigamonti,Mirko Paolo Barbato,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的个性化血糖预测方法，通过利用患者特定数据来提高预测准确性，相比传统通用模型能更好地处理个体差异，为糖尿病管理提供更精确的决策支持。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴血糖监测设备和移动健康应用的普及，准确的血糖预测对于增强自动化胰岛素输送和决策支持系统至关重要。传统通用模型无法充分处理个体差异，需要开发能够适应患者特定动态的个性化预测方法。

Method: 提出基于深度学习的个性化血糖预测方法，采用患者特定数据进行模型训练。比较了留一受试者交叉验证与微调策略，评估它们对患者特定动态的建模能力。通过实验对比多模态患者特定方法与传统的仅使用连续血糖监测数据的方法，并进行消融研究以确定有效个性化所需的最小数据量。

Result: 个性化模型显著提高了不良事件的预测准确性，能够实现更精确和及时的干预。多模态患者特定方法优于传统的CGM-only方法。消融研究确定了有效个性化所需的最小数据量，这对实际应用中数据收集受限的情况具有重要意义。

Conclusion: 自适应、个性化的血糖预测模型在推进下一代糖尿病管理方面具有巨大潜力，特别是在可穿戴和移动健康平台中，能够增强面向消费者的糖尿病护理解决方案。

Abstract: Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.

</details>


### [37] [Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed Learning](https://arxiv.org/abs/2601.14942)
*Hang Zhao,Hongru Li,Dongfang Xu,Shenghui Song,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出三阶段通信感知分布式学习框架，用于多模态边缘推理，通过自监督学习、分布式微调和不确定性引导反馈机制，在减少通信开销的同时提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态边缘推理面临两大挑战：1) 多模态特性导致分布式学习在带宽有限无线链路上的通信开销巨大；2) 在变化信道和噪声多模态输入下鲁棒性有限。需要设计通信高效的训练和鲁棒推理方案。

Method: 三阶段框架：阶段I - 本地多模态自监督学习，无需设备-服务器交换，获得共享和模态特定编码器；阶段II - 分布式微调与集中证据融合，校准每模态不确定性并可靠聚合受噪声或信道衰落影响的特征；阶段III - 不确定性引导反馈机制，为不确定样本选择性请求额外特征，优化通信-精度权衡。

Result: 在RGB-深度室内场景分类实验中，该框架以更少的训练通信轮次获得更高精度，对模态退化或信道变化保持鲁棒性，优于现有自监督和全监督基线方法。

Conclusion: 提出的三阶段通信感知分布式学习框架有效解决了多模态边缘推理的通信效率和鲁棒性问题，通过自监督学习减少通信开销，证据融合提升鲁棒性，不确定性反馈优化通信-精度权衡。

Abstract: Semantic communication is emerging as a key enabler for distributed edge intelligence due to its capability to convey task-relevant meaning. However, achieving communication-efficient training and robust inference over wireless links remains challenging. This challenge is further exacerbated for multi-modal edge inference (MMEI) by two factors: 1) prohibitive communication overhead for distributed learning over bandwidth-limited wireless links, due to the \emph{multi-modal} nature of the system; and 2) limited robustness under varying channels and noisy multi-modal inputs. In this paper, we propose a three-stage communication-aware distributed learning framework to improve training and inference efficiency while maintaining robustness over wireless channels. In Stage~I, devices perform local multi-modal self-supervised learning to obtain shared and modality-specific encoders without device--server exchange, thereby reducing the communication cost. In Stage~II, distributed fine-tuning with centralized evidential fusion calibrates per-modality uncertainty and reliably aggregates features distorted by noise or channel fading. In Stage~III, an uncertainty-guided feedback mechanism selectively requests additional features for uncertain samples, optimizing the communication--accuracy tradeoff in the distributed setting. Experiments on RGB--depth indoor scene classification show that the proposed framework attains higher accuracy with far fewer training communication rounds and remains robust to modality degradation or channel variation, outperforming existing self-supervised and fully supervised baselines.

</details>


### [38] [Improving Regret Approximation for Unsupervised Dynamic Environment Generation](https://arxiv.org/abs/2601.14957)
*Harry Mead,Bruno Lacerda,Jakob Foerster,Nick Hawes*

Main category: cs.LG

TL;DR: DEGen通过动态环境生成和MNA遗憾近似改进UED，解决信用分配问题并提升大规模环境下的零样本性能


<details>
  <summary>Details</summary>
Motivation: 当前无监督环境设计方法在环境参数空间较大时面临信用分配困难和遗憾近似不准确的问题，难以识别真正具有挑战性的训练关卡

Method: 提出DEGen（动态环境生成）方法，提供更密集的关卡生成器奖励信号；引入MNA（最大化负优势）作为改进的遗憾近似指标，更好地识别高难度关卡

Result: 实验表明MNA优于现有遗憾近似方法，DEGen与MNA结合后在大规模环境中持续超越现有方法，尤其在环境规模增大时表现更优

Conclusion: DEGen和MNA的组合有效解决了UED中的信用分配和挑战性关卡识别问题，使无监督环境设计能够扩展到更大规模的环境

Abstract: Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.

</details>


### [39] [InstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement](https://arxiv.org/abs/2601.14968)
*Mingyue Cheng,Xiaoyu Tao,Huajian Zhang,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: InstructTime++：将时间序列分类重构为多模态生成任务，通过离散化时间序列、跨模态对齐和隐式特征建模，利用语言模型生成文本类别标签


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类方法主要采用判别式范式，直接将输入序列映射到one-hot编码的类别标签。这种方法难以融入上下文特征，也无法捕捉类别间的语义关系。需要新的框架来解决这些限制。

Method: 1. 将时间序列分类重构为多模态生成任务：连续数值序列、上下文文本特征和任务指令作为多模态输入，类别标签作为语言模型生成的文本输出
2. 时间序列离散化模块：将连续序列转换为离散时间标记
3. 对齐投影层和生成式自监督预训练策略：增强跨模态表示对齐
4. InstructTime++扩展：加入隐式特征建模，通过专门工具包挖掘原始时间序列和上下文输入中的信息模式（统计特征提取和视觉语言图像描述），并将其转换为文本描述进行集成

Result: 在多个基准数据集上的广泛实验表明，InstructTime++具有优越的性能表现

Conclusion: 通过将时间序列分类重构为多模态生成任务，并引入隐式特征建模，InstructTime++能够有效整合上下文特征、捕捉类别语义关系，在时间序列分类任务上取得了显著改进

Abstract: Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.

</details>


### [40] [Lineup Regularized Adjusted Plus-Minus (L-RAPM): Basketball Lineup Ratings with Informed Priors](https://arxiv.org/abs/2601.15000)
*Christos Petridis,Konstantinos Pelechrinis*

Main category: cs.LG

TL;DR: 提出L-RAPM回归方法，通过控制对手阵容和利用球员信息，解决篮球阵容评估中因频繁换人导致的数据稀疏和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 篮球等体育运动中，识别表现良好的球员组合（阵容）是体育分析的重要任务。主要挑战在于比赛中频繁换人导致数据高度稀疏，例如NBA球队一个赛季使用超过600个阵容，每个阵容平均只有25-30次进攻回合，统计数据噪声大、预测价值低。目前公开文献中缺乏解决此问题的方法。

Method: 提出基于回归的方法L-RAPM，该方法控制每个阵容面对的对手因素，同时利用组成阵容的球员信息。通过回归模型整合对手阵容信息和球员特征，提高对稀疏阵容数据的评估准确性。

Result: 实验表明，L-RAPM相比当前使用的基线方法具有更好的预测能力。随着阵容样本量变小，L-RAPM的改进效果更加明显，说明该方法特别适用于处理数据稀疏问题。

Conclusion: L-RAPM方法通过控制对手因素和利用球员信息，有效解决了篮球阵容评估中的数据稀疏和噪声问题，在预测性能上优于现有基线方法，特别是在小样本情况下表现更优。

Abstract: Identifying combinations of players (that is, lineups) in basketball - and other sports - that perform well when they play together is one of the most important tasks in sports analytics. One of the main challenges associated with this task is the frequent substitutions that occur during a game, which results in highly sparse data. In particular, a National Basketball Association (NBA) team will use more than 600 lineups during a season, which translates to an average lineup having seen the court in approximately 25-30 possessions. Inevitably, any statistics that one collects for these lineups are going to be noisy, with low predictive value. Yet, there is no existing work (in the public at least) that addresses this problem. In this work, we propose a regression-based approach that controls for the opposition faced by each lineup, while it also utilizes information about the players making up the lineups. Our experiments show that L-RAPM provides improved predictive power than the currently used baseline, and this improvement increases as the sample size for the lineups gets smaller.

</details>


### [41] [Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control](https://arxiv.org/abs/2601.15015)
*Jannis Becktepe,Aleksandra Franz,Nils Thuerey,Sebastian Peitz*

Main category: cs.LG

TL;DR: FluidGym：首个独立、完全可微的强化学习主动流控制基准套件，基于GPU加速的PICT求解器构建，无需外部CFD软件，提供标准化评估协议。


<details>
  <summary>Details</summary>
Motivation: 强化学习在主动流控制中显示出前景，但现有研究因观测/执行方案、数值设置和评估协议不一致而难以评估进展。现有基准依赖外部CFD求解器、不完全可微、且3D和多智能体支持有限。

Method: 基于GPU加速的PICT求解器在PyTorch上构建完全独立的基准套件，提供标准化评估协议，包含PPO和SAC基线结果，所有环境、数据集和训练模型公开。

Result: 开发了FluidGym基准套件，作为首个独立、完全可微的RL-AFC基准，在单一Python栈中运行，无需外部CFD软件，支持系统化方法比较。

Conclusion: FluidGym克服了现有基准的局限性，为基于学习的流控制研究建立了可扩展基础，促进了控制方法的系统化比较。

Abstract: Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.

</details>


### [42] [Mixture-of-Experts Models in Vision: Routing, Optimization, and Generalization](https://arxiv.org/abs/2601.15021)
*Adam Rokah,Daniel Veress,Caleb Caulk,Sourav Sharan*

Main category: cs.LG

TL;DR: 该研究在图像分类任务中比较了密集、SoftMoE和SparseMoE架构的性能，发现MoE变体在CIFAR10上略优于密集基线，但条件路由在实际硬件上未实现推理加速。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在图像分类任务中的行为，而非传统的大语言模型缩放场景。关注预测性能、专家利用率和泛化特性，探索MoE在小规模视觉任务中的实际效果。

Method: 在CIFAR10数据集上比较密集、SoftMoE和SparseMoE分类头，保持可比模型容量。使用正则化平衡专家利用率，通过Hessian矩阵的锐度指标（最大特征值和迹）分析泛化特性，并进行损失曲面扰动分析。

Result: 两种MoE变体在验证准确率上略高于密集基线，且通过正则化避免了专家崩溃。SoftMoE表现出更高的Hessian锐度，而密集和SparseMoE处于相似的曲率状态。条件路由在实际硬件上未实现推理加速。

Conclusion: MoE架构在图像分类中能获得轻微性能提升并保持专家平衡，但实际推理效率受硬件限制。曲率分析揭示了模型间的差异，但未直接解释泛化性能的相似性。

Abstract: Mixture-of-Experts (MoE) architectures enable conditional computation by routing inputs to multiple expert subnetworks and are often motivated as a mechanism for scaling large language models. In this project, we instead study MoE behavior in an image classification setting, focusing on predictive performance, expert utilization, and generalization. We compare dense, SoftMoE, and SparseMoE classifier heads on the CIFAR10 dataset under comparable model capacity. Both MoE variants achieve slightly higher validation accuracy than the dense baseline while maintaining balanced expert utilization through regularization, avoiding expert collapse. To analyze generalization, we compute Hessian-based sharpness metrics at convergence, including the largest eigenvalue and trace of the loss Hessian, evaluated on both training and test data. We find that SoftMoE exhibits higher sharpness by these metrics, while Dense and SparseMoE lie in a similar curvature regime, despite all models achieving comparable generalization performance. Complementary loss surface perturbation analyses reveal qualitative differences in non-local behavior under finite parameter perturbations between dense and MoE models, which help contextualize curvature-based measurements without directly explaining validation accuracy. We further evaluate empirical inference efficiency and show that naively implemented conditional routing does not yield inference speedups on modern hardware at this scale, highlighting the gap between theoretical and realized efficiency in sparse MoE models.

</details>


### [43] [Field-Space Autoencoder for Scalable Climate Emulators](https://arxiv.org/abs/2601.15102)
*Johannes Meuer,Maximilian Witte,Étiénne Plésiat,Thomas Ludwig,Christopher Kadow*

Main category: cs.LG

TL;DR: 提出Field-Space Autoencoder框架，通过球面压缩模型解决千米尺度地球系统模型计算成本高、输出数据量大的问题，支持零样本超分辨率和生成式模拟。


<details>
  <summary>Details</summary>
Motivation: 千米尺度地球系统模型计算成本高昂且产生PB级输出，限制了其在概率风险评估等应用中的实用性，需要开发可扩展的气候模拟框架来克服这些挑战。

Method: 提出Field-Space Autoencoder框架，采用Field-Space Attention在原生气候模型输出上高效操作，避免球面数据强制到欧几里得网格导致的几何失真；通过结构化压缩场作为下游生成式模拟的基线；支持零样本超分辨率，将低分辨率大集合和稀缺高分辨率数据映射到共享表示；在压缩场上训练生成扩散模型。

Result: 该方法比卷积基线显著更好地保持物理结构；能够同时从丰富的低分辨率数据学习内部变异性，从稀疏高分辨率数据学习精细尺度物理；桥接了低分辨率集合统计的高容量与高分辨率物理细节稀缺性之间的差距。

Conclusion: Field-Space Autoencoder提供了一个可扩展的气候模拟框架，克服了千米尺度地球系统模型的限制，为下游生成式模拟提供了良好基线，并实现了低分辨率和高分辨率数据之间的有效整合。

Abstract: Kilometer-scale Earth system models are essential for capturing local climate change. However, these models are computationally expensive and produce petabyte-scale outputs, which limits their utility for applications such as probabilistic risk assessment. Here, we present the Field-Space Autoencoder, a scalable climate emulation framework based on a spherical compression model that overcomes these challenges. By utilizing Field-Space Attention, the model efficiently operates on native climate model output and therefore avoids geometric distortions caused by forcing spherical data onto Euclidean grids. This approach preserves physical structures significantly better than convolutional baselines. By producing a structured compressed field, it serves as a good baseline for downstream generative emulation. In addition, the model can perform zero-shot super-resolution that maps low-resolution large ensembles and scarce high-resolution data into a shared representation. We train a generative diffusion model on these compressed fields. The model can simultaneously learn internal variability from abundant low-resolution data and fine-scale physics from sparse high-resolution data. Our work bridges the gap between the high volume of low-resolution ensemble statistics and the scarcity of high-resolution physical detail.

</details>


### [44] [Auditing Language Model Unlearning via Information Decomposition](https://arxiv.org/abs/2601.15111)
*Anmol Goel,Alan Ritter,Iryna Gurevych*

Main category: cs.LG

TL;DR: 当前机器学习遗忘方法存在关键局限：尽管遗忘算法表面成功，但被遗忘数据的信息仍能从模型内部表示中线性解码。论文引入基于部分信息分解的信息论框架来审计遗忘效果，发现冗余信息构成残留知识，并提出基于表示的风险评分来指导推理时对敏感输入的弃权。


<details>
  <summary>Details</summary>
Motivation: 揭示当前语言模型机器学习遗忘方法的根本局限性，即表面成功的遗忘算法实际上未能完全消除被遗忘数据的信息，这些信息仍可从模型内部表示中线性解码，存在隐私泄露风险。

Method: 引入基于部分信息分解（PID）的可解释信息论框架，通过比较遗忘前后的模型表示，将与被遗忘数据的互信息分解为不同组件，形式化遗忘知识和残留知识的概念。利用这些洞察提出基于表示的风险评分机制。

Result: 分析表明冗余信息（在遗忘前后模型间共享）构成残留知识，这些知识在遗忘后持续存在，并与已知对抗重建攻击的易感性相关。提出的风险评分能够有效指导推理时对敏感输入的弃权。

Conclusion: 该工作为机器学习遗忘提供了原则性的表示层面审计方法，提供了理论洞察和实用工具，有助于语言模型更安全的部署。揭示了当前遗忘方法的根本局限性并提出了缓解隐私泄露的实际机制。

Abstract: We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.

</details>


### [45] [Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.15124)
*Haonan Yuan,Qingyun Sun,Jiacheng Tao,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: RAG-GFM：一种检索增强生成的图基础模型，通过将知识从参数中卸载到外部存储，解决现有图基础模型的内存瓶颈问题，实现更好的可扩展性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型（GFMs）存在内存瓶颈问题：它们试图将知识编码到模型参数中，这限制了语义容量，引入了严重的损失性压缩和冲突，并以阻碍高效适应的方式将图表示与知识纠缠在一起，从而削弱了可扩展性和可解释性。

Method: 提出RAG-GFM（检索增强生成的图基础模型），通过双模态统一检索模块外部化图知识：语义存储基于前缀结构文本，结构存储基于中心性基序。设计双视图对齐目标来保留异构信息，对比两种模态以捕获内容和关系模式。通过上下文增强进行高效下游适应，使用检索到的文本和基序作为上下文证据来丰富支持实例。

Result: 在五个基准图数据集上的广泛实验表明，RAG-GFM在跨域节点和图分类任务中始终优于13个最先进的基线方法，实现了卓越的有效性和效率。

Conclusion: RAG-GFM通过将知识从参数中卸载并补充参数化学习，解决了图基础模型的内存瓶颈问题，实现了更好的可扩展性、适应性和性能。

Abstract: Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.

</details>


### [46] [CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning](https://arxiv.org/abs/2601.15141)
*Tianshi Xu,Yuteng Chen,Meng Li*

Main category: cs.LG

TL;DR: CLEANER提出了一种利用LLM内在自校正能力净化轨迹的方法，通过相似性感知自适应回滚机制在数据收集阶段直接消除错误污染，显著提升参数受限模型的强化学习效率。


<details>
  <summary>Details</summary>
Motivation: 参数受限模型（4B-7B）在工具使用强化学习中面临探索阶段频繁执行失败的问题，产生噪声轨迹阻碍策略优化。标准基于结果的奖励设置导致关键信用分配问题，错误动作与成功结果同时被强化。现有缓解方法面临困境：密集奖励易引发奖励黑客攻击，而过采样则带来高昂计算成本。

Method: 提出CLEANER框架，核心是相似性感知自适应回滚（SAAR）机制。该方法利用模型内在自校正能力，在数据收集阶段直接消除错误污染上下文。SAAR通过回顾性替换失败步骤为成功的自校正，自适应构建纯净轨迹。基于语义相似性，SAAR从浅层执行修复到深层推理替换自适应调节替换粒度。

Result: 在AIME24/25、GPQA和LiveCodeBench基准测试中，相比基线平均准确率分别提升6%、3%和5%。特别值得注意的是，CLEANER仅使用三分之一训练步数即达到最先进性能，证明了轨迹净化作为高效智能体强化学习的可扩展解决方案。

Conclusion: CLEANER通过利用LLM内在自校正能力净化轨迹，有效解决了参数受限模型在工具使用强化学习中的信用分配问题。该方法不仅提升了性能，还大幅降低了训练成本，为高效智能体强化学习提供了可扩展的解决方案。

Abstract: Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub

</details>


### [47] [Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data](https://arxiv.org/abs/2601.15158)
*Yuval Ran-Milo,Yotam Alexander,Shahar Mendel,Nadav Cohen*

Main category: cs.LG

TL;DR: 论文分析了Transformer在稀疏奖励下如何自发产生链式思维推理的机制，通过图遍历任务的理论分析和实验验证，揭示了简单示例在梯度流动态中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer在仅基于最终答案正确性的强化学习训练中，如何自发发展出中间推理步骤（链式思维）的机制。目前稀疏奖励如何驱动梯度下降发现这种系统性推理仍不清楚。

Method: 1. 理论分析单层Transformer在合成图遍历任务上的梯度流动态，该任务没有链式思维无法解决但存在简单迭代解。2. 证明仅基于最终答案正确性的训练会使模型收敛到结构化、可解释的迭代遍历算法。3. 识别"简单示例"（需要较少推理步骤的实例）在分布特性中的关键作用。4. 通过合成数据和真实语言模型在数学推理任务上的实验验证理论发现。

Result: 1. 证明了梯度流会驱动模型收敛到顶点逐顶点迭代遍历的结构化算法。2. 识别了训练分布中简单示例质量的关键作用：当简单示例足够多时，模型学习可泛化的遍历策略并能外推到更长链；当简单示例消失时，基于梯度的学习变得不可行。3. 实验验证了理论发现在实际设置中的适用性。

Conclusion: Transformer在稀疏奖励下自发产生链式思维的能力源于梯度流动态，其中训练分布中简单示例的存在是关键因素。这为理解大语言模型中系统性推理的涌现提供了理论框架。

Abstract: Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of "simple examples": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.

</details>


### [48] [ZENITH: Automated Gradient Norm Informed Stochastic Optimization](https://arxiv.org/abs/2601.15212)
*Dhrubo Saha*

Main category: cs.LG

TL;DR: ZENITH优化器通过梯度范数的时间演化自适应调整学习率，无需手动调度，在图像分类、目标检测等任务中取得更高精度和更快训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有自适应优化器存在计算和内存开销大、与正则化不兼容、学习率选择次优等问题，需要开发更高效的自适应学习率调度方法。

Method: 提出ZENITH优化器，利用梯度范数的时间演化信息来自适应调整学习率，实现零开销的自动学习率调度。

Result: 在6种CNN架构和6个基准测试的图像分类实验中，ZENITH在更短的挂钟时间内达到更高的测试精度；在MS COCO的目标检测、关键点检测和实例分割任务中，使用R-CNN系列模型获得更优的mAP。

Conclusion: ZENITH优化器通过梯度范数演化自适应调整学习率，实现了高效、兼容正则化的训练，在多种计算机视觉任务中表现出优越性能。

Abstract: Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.

</details>


### [49] [Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism](https://arxiv.org/abs/2601.15249)
*Garrett G. Wen,Buxin Su,Natalie Collina,Zhun Deng,Weijie Su*

Main category: cs.LG

TL;DR: 提出一种基于等渗机制的作者辅助最佳论文评选方法，通过作者对自身论文的排名评估来调整原始评审分数，从而提高奖项评选质量。


<details>
  <summary>Details</summary>
Motivation: NeurIPS、ICML等AI顶会每年收到数万篇投稿，给同行评审质量和一致性带来巨大挑战，尤其是最佳论文奖项的评选日益引发争议，需要更可靠的评选机制。

Method: 采用等渗机制收集作者对自己投稿的排名评估，利用这些排名信息调整原始评审分数，以最优估计论文的真实质量。当作者效用函数为凸可加函数时，作者有动机如实报告。特别地，当作者只有一个提名名额时，即使效用函数仅为非递减可加函数也能保证真实性。

Result: 使用2019-2023年ICLR和2021-2023年NeurIPS的公开评审数据验证了凸性假设。模拟结果显示，该机制显著提高了奖项评选的论文质量。

Conclusion: 作者辅助的等渗机制为大规模会议的最佳论文评选提供了有效的解决方案，显著放松了先前工作所需的假设条件，并能处理作者重叠的实际情况。

Abstract: Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [50] [JAXMg: A multi-GPU linear solver in JAX](https://arxiv.org/abs/2601.14466)
*Roeland Wiersema*

Main category: cs.DC

TL;DR: JAXMg为JAX提供多GPU稠密线性代数支持，通过集成cuSOLVERMg实现跨GPU的Cholesky线性求解和对称特征分解，解决单GPU内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 大规模稠密线性系统和特征值问题在科学计算中至关重要，但现有编程框架难以扩展到多GPU环境。虽然存在优化的多GPU求解库，但它们难以集成到可组合的JIT编译Python工作流中。

Method: 通过XLA外部函数接口将JAX与NVIDIA的cuSOLVERMg连接，将分布式GPU求解器暴露为JIT兼容的JAX原语。采用Cholesky-based线性求解和对称特征分解算法。

Result: JAXMg实现了多GPU稠密线性代数功能，支持超过单GPU内存限制的矩阵运算，保持了与JAX变换的可组合性，并能在端到端科学工作流中实现多GPU执行。

Conclusion: JAXMg成功将分布式GPU求解器集成到JAX生态系统中，为科学计算提供了可扩展的线性代数解决方案，同时保持了JAX的编程模型优势。

Abstract: Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows.

</details>


### [51] [Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI](https://arxiv.org/abs/2601.14608)
*Torben R. Lahnor,Mia Reitz,Jonas Posner,Patrick Diehl*

Main category: cs.DC

TL;DR: 该研究将Itoyori和ItoyoriFBC两种异步多任务运行时集成到Task Bench框架中，与MPI和HPX进行性能与编程效率的综合对比评估。


<details>
  <summary>Details</summary>
Motivation: 异步多任务运行时作为MPI的替代方案具有编程效率优势，但AMT生态系统多样性使得公平比较困难。需要系统评估框架来比较不同并行编程系统的性能与生产力。

Method: 使用Task Bench参数化评估框架，集成Itoyori（基于PGAS和RDMA工作窃取）和ItoyoriFBC（扩展了基于future的同步）。通过应用效率、最小有效任务粒度评估性能，通过代码行数和库构造数评估编程效率。

Result: MPI在规则、通信轻量负载中效率最高但代码冗长；HPX在不同节点数下保持稳定效率但生产力指标最差；Itoyori在通信密集型配置中效率最高且编程效率领先；ItoyoriFBC效率略低但future同步适合不规则负载。

Conclusion: 不同系统存在明显权衡：MPI性能最优但编程繁琐；AMT不一定自动提高生产力；Itoyori在通信密集场景中表现最佳；ItoyoriFBC的future机制为不规则负载提供潜力。

Abstract: Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.
  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).
  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads.

</details>


### [52] [Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies](https://arxiv.org/abs/2601.14612)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 提出ROSS随机调度算法，在混合云环境中实现截止时间感知的在线调度，将竞争比从Ω(K)优化到√K，成本节省提升30%


<details>
  <summary>Details</summary>
Motivation: 混合云环境中，作业可以在成本较低但不稳定的spot实例或更昂贵的on-demand实例上运行，需要在硬截止时间约束下进行调度优化。现有确定性策略存在Ω(K)的最坏情况竞争比限制，需要更优的调度算法来平衡成本优化和截止时间保证。

Method: 提出ROSS（随机调度算法），这是一种新颖的随机化调度策略。首先建立了现有确定性策略的基本极限，证明了Ω(K)的最坏情况竞争比。然后设计了ROSS算法，在合理的截止时间条件下实现了√K的可证明最优竞争比。

Result: 在Azure和AWS的真实世界跟踪数据上进行广泛评估，ROSS在成本优化和截止时间保证之间取得了有效平衡，在各种spot市场条件下，相比最先进方法实现了高达30%的成本节省。

Conclusion: ROSS算法在混合云截止时间感知在线调度问题上取得了显著突破，通过随机化策略将竞争比从Ω(K)优化到√K，在实际应用中表现出优越的性能，为云成本优化提供了有效的解决方案。

Abstract: This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $Ω(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\%$ in cost savings, across diverse spot market conditions.

</details>


### [53] [Optimizing FaaS Platforms for MCP-enabled Agentic Workflows](https://arxiv.org/abs/2601.14735)
*Varad Kulkarni,Vaibhav Jha,Nikhil Reddy,Yogesh Simmhan*

Main category: cs.DC

TL;DR: FAME：基于FaaS的架构，用于编排支持MCP的智能体工作流，通过函数分解、状态管理和优化策略实现高效可扩展的AI工作流部署


<details>
  <summary>Details</summary>
Motivation: 基于LLM和MCP的自主AI智能体工作流快速增长，但面临云部署扩展性和状态管理挑战。传统VM部署资源密集且缺乏弹性，而FaaS平台虽然模块化、可自动扩展且成本效益高，但本质上是无状态的，无法满足智能体工作流的持久状态需求。

Method: 提出FAME架构，将智能体模式（如ReAct）分解为可组合的智能体：规划器、执行器和评估器，每个智能体都是使用LangGraph构建的FaaS函数，并编排为FaaS工作流。使用AWS Step Functions进行模块化组合，避免单体工作流的函数超时问题。通过DynamoDB自动化智能体内存持久化和注入，解决跨用户请求的上下文持久化问题。通过AWS Lambda包装器优化MCP服务器部署，在S3中缓存工具输出，并提出函数融合策略。

Result: 在两个代表性应用（研究论文摘要和日志分析）上评估FAME，在不同内存和缓存配置下，结果显示：延迟降低高达13倍，输入令牌减少88%，成本节省66%，工作流完成率得到改善。这证明了无服务器平台能够大规模托管复杂的多智能体AI工作流。

Conclusion: FAME架构通过将智能体工作流分解为可组合的FaaS函数，结合状态管理和优化策略，成功解决了基于LLM和MCP的智能体工作流在无服务器平台上的部署挑战，实现了显著的性能提升和成本节约，验证了无服务器平台托管复杂多智能体AI工作流的可行性。

Abstract: Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88% fewer input tokens and 66% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale.

</details>


### [54] [AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems](https://arxiv.org/abs/2601.14912)
*Guangba Yu,Genting Mai,Rui Wang,Ruipeng Li,Pengfei Chen,Long Pan,Ruijie Xu*

Main category: cs.DC

TL;DR: AlertGuardian框架结合LLM和图模型优化云系统告警生命周期管理，通过告警去噪、摘要生成和规则优化三个阶段，显著减少告警疲劳并提升故障诊断效率。


<details>
  <summary>Details</summary>
Motivation: 大规模云系统产生海量告警导致告警疲劳，现有系统告警生命周期管理效率低下，影响运维效率和系统可靠性。

Method: 提出AlertGuardian框架，结合大语言模型和轻量级图模型：1) Alert Denoise使用带虚拟噪声的图学习模型过滤噪声；2) Alert Summary采用RAG增强的LLM生成可操作摘要；3) Alert Rule Refinement利用多智能体迭代反馈优化告警规则质量。

Result: 在四个真实数据集上评估：告警减少率达94.8%，故障诊断准确率达90.5%，优化了1,174条告警规则，其中375条被SRE接受（接受率32%）。

Conclusion: AlertGuardian有效缓解云系统告警疲劳，提升运维效率，并在Company-X成功部署，分享了告警生命周期管理的实践经验。

Abstract: Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Company-X to optimize alert life-cycle management, addressing alert fatigue in cloud systems. We propose AlertGuardian, a framework collaborating large language models (LLMs) and lightweight graph models to optimize the alert life-cycle through three phases: Alert Denoise uses graph learning model with virtual noise to filter noise, Alert Summary employs Retrieval Augmented Generation (RAG) with LLMs to create actionable summary, and Alert Rule Refinement leverages multi-agent iterative feedbacks to improve alert rule quality. Evaluated on four real-world datasets from Company-X's services, AlertGuardian significantly mitigates alert fatigue (94.8\% alert reduction ratios) and accelerates fault diagnosis (90.5\% diagnosis accuracy). Moreover, AlertGuardian improves 1,174 alert rules, with 375 accepted by SREs (32% acceptance rate). Finally, we share success stories and lessons learned about alert life-cycle management after the deployment of AlertGuardian in Company-X.

</details>


### [55] [Application-level observability for adaptive Edge to Cloud continuum systems](https://arxiv.org/abs/2601.14923)
*Kaddour Sidi,Daniel Balouek,Baptiste Jonglez*

Main category: cs.DC

TL;DR: 提出一个应用级可观测性框架，用于现代边缘到云系统，通过集成开发者驱动的仪器化和SLO感知反馈实现自主适应


<details>
  <summary>Details</summary>
Motivation: 现代边缘到云系统需要细粒度可观测性，以确保在异构动态环境中的自适应行为和性能目标合规性

Method: 集成OpenTelemetry、Prometheus、K3s和Chaos Mesh的应用级可观测性框架，结合开发者驱动的仪器化和SLO感知反馈，实现实时监控和自适应控制

Result: 视频处理用例展示应用级指标如何指导自动调整以维持目标帧率、延迟和检测精度；初步结果显示改进的可扩展性、容错性和响应性

Conclusion: 该框架为自适应、SLO合规的边缘到云应用提供了实用基础，通过应用级可观测性实现自主适应

Abstract: Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications.

</details>


<div id='Surajit Chaudhuri'></div>

# Surajit Chaudhuri [[Back]](#toc)

### [56] [HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.24562&hl=zh-CN&sa=X&d=6973965432322717332&ei=ZwFxad6NO4eUywS5-prpDw&scisig=AHkA5jTpXDEVflSll7UbcVuiQHsf&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=0&folt=rel)
*C Tong,Q Zhang,J Gao,L Jiang,Y Liu,N Sun*

Main category: Surajit Chaudhuri

TL;DR: LLMs在问答中常产生幻觉，研究通过内部不确定性信号检测幻觉，提出多视角不确定性估计方法提升检测效果


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在问答任务中表现出色，但经常产生包含事实错误或虚构内容的幻觉。基于内部不确定性信号的幻觉检测因其可扩展性和独立性而具有吸引力，但现有方法在检测准确性方面存在局限。

Method: 提出一种多视角不确定性估计方法，从不同角度（如语义、逻辑、事实一致性等）评估模型输出的不确定性，综合多个信号来检测幻觉。该方法利用LLMs的内部表示和生成过程来提取不确定性特征。

Result: 实验表明，该方法在多个基准数据集上显著优于现有幻觉检测方法，能够更准确地识别LLMs生成的幻觉内容，同时保持较高的召回率和精确率。

Conclusion: 通过多视角不确定性估计可以有效检测LLMs产生的幻觉，为构建更可靠的问答系统提供了实用方法。该方法不依赖外部知识库，具有较好的可扩展性。

Abstract: Large Language Models (LLMs) excel at question answering (QA) but often generate hallucinations, including factual errors or fabricated content. Detecting hallucinations from internal uncertainty signals is attractive due to its scalability and independence …

</details>


### [57] [Subspace Alignment for Vision-Language Model Test-time Adaptation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08139&hl=zh-CN&sa=X&d=1843137550369509079&ei=ZwFxad6NO4eUywS5-prpDw&scisig=AHkA5jSDrsjoIZVja2A1Lb7gmAm1&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=1&folt=rel)
*Z Zeng,W Bao,X Lin,R Qiu,T Wei,X Ning,Y Yan…*

Main category: Surajit Chaudhuri

TL;DR: 该论文研究视觉语言模型在分布偏移下的脆弱性，提出一种测试时自适应方法来解决这一问题


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型具有出色的零样本能力，但它们对分布偏移很脆弱。测试时自适应是适应未标记测试数据的主要策略，但现有方法存在局限性

Method: 论文提出一种新的测试时自适应方法，具体方法细节需要更多信息，但可能涉及对未标记测试数据的在线适应技术

Result: 提出的方法在分布偏移情况下提高了视觉语言模型的性能，但具体实验结果需要完整论文才能确定

Conclusion: 该研究为解决视觉语言模型在分布偏移下的脆弱性问题提供了有效的测试时自适应方案

Abstract: Vision-language models (VLMs), despite their extraordinary zero-shot capabilities, are vulnerable to distribution shifts. Test-time adaptation (TTA) emerges as a predominant strategy to adapt VLMs to unlabeled test data on the fly. However …

</details>


### [58] [CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03432&hl=zh-CN&sa=X&d=447000866206329208&ei=ZwFxad6NO4eUywS5-prpDw&scisig=AHkA5jTO4wJqx8btXz65AaB9gG94&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=2&folt=rel)
*D Brahman,M Mahoor*

Main category: Surajit Chaudhuri

TL;DR: 该论文指出当前LLM评估主要关注常识推理、语言理解和逻辑推理能力，而缺乏对专业领域模型（如数学或编程）的全面评估框架


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的评估体系存在局限性，主要关注通用能力而忽视了专业领域模型的评估需求，需要建立更全面的评估框架来准确衡量专业领域LLM的性能

Method: 论文未提供具体方法细节，但从摘要推断可能涉及分析现有评估方法的不足，并提出针对专业领域LLM的评估框架或指标体系

Result: 摘要未提供具体结果，但暗示需要建立专门针对数学、编程等专业领域LLM的评估方法，以弥补现有评估体系的不足

Conclusion: 需要开发更全面的评估框架来准确评估专业领域大语言模型的性能，超越当前主要关注通用能力的评估范式

Abstract: Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have …

</details>


### [59] [Efficiently Estimating Data Efficiency for Language Model Fine-tuning](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.24991&hl=zh-CN&sa=X&d=568655720932899245&ei=ZwFxad6NO4eUywS5-prpDw&scisig=AHkA5jQsNYaiVcRB2czVBMsa2TjL&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=3&folt=rel)
*GH Je,C Raffel*

Main category: Surajit Chaudhuri

TL;DR: 该论文研究了LLM微调的数据效率问题，提出了一个理论框架来量化不同任务的数据需求，并开发了预测模型来估计最优微调数据量。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在许多下游任务上表现出合理的零样本能力，但微调是提高其性能的常见做法。然而，不同任务的数据效率存在显著差异，目前缺乏系统性的理论框架来量化这种差异，导致实践中难以确定最优的微调数据量。

Method: 提出了一个理论框架来量化任务的数据效率，通过分析任务复杂度、模型容量和训练数据之间的相互作用来预测最优微调数据量。开发了基于任务特征和模型参数的预测模型，使用多种基准任务进行验证。

Result: 研究发现不同任务的数据效率差异可达数量级，提出的预测模型能够准确估计最优微调数据量，相比经验方法显著提高了微调效率。在多个基准任务上验证了框架的有效性。

Conclusion: 任务的数据效率是微调成功的关键因素，提出的理论框架为理解数据效率提供了系统方法，预测模型能够指导实践中的微调数据选择，提高资源利用效率。

Abstract: While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--ie, the number of fine-tuning …

</details>


### [60] [TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.20757&hl=zh-CN&sa=X&d=16608035220368283080&ei=ZwFxad6NO4eUywS5-prpDw&scisig=AHkA5jTelxshft5nXIL6ScSjDPYX&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=4&folt=rel)
*GS Altıntaş,M Ehghaghi,B Lester,F Liu,W Zhao…*

Main category: Surajit Chaudhuri

TL;DR: 该论文探讨了分词器对语言模型性能和行为的影响，指出当前对其作用理解不足，并提出了一种系统评估分词器影响的方法


<details>
  <summary>Details</summary>
Motivation: 分词器是语言模型处理文本的基础，但当前对其在模型性能和行为中的具体作用缺乏深入理解，这限制了语言模型的优化和发展

Method: 通过系统性的评估框架来分析不同分词策略对语言模型性能的影响，可能包括对比不同分词算法、词汇表大小、分词粒度等因素

Result: 研究发现分词器选择对语言模型的性能有显著影响，包括推理效率、泛化能力和下游任务表现等方面

Conclusion: 分词器是影响语言模型性能的关键因素，需要更系统的研究来优化分词策略，以提升语言模型的整体表现

Abstract: Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of …

</details>


### [61] [A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07291&hl=zh-CN&sa=X&d=4787924359217277882&ei=ZwFxad6NO4eUywS5-prpDw&scisig=AHkA5jTGqjX3LOf0uNmODrX0qXvF&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=5&folt=rel)
*Q Zheng,S Liu,Y Huang,S Jia,J Li,L Chen,J Chen…*

Main category: Surajit Chaudhuri

TL;DR: 该论文提出了一种名为Vision-Aware Watermarking (VAW)的新方法，用于大型视觉语言模型的内容溯源和知识产权保护，通过视觉感知机制将水印与视觉内容对齐，避免视觉无关标记的干扰。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型中使用的水印技术通常是视觉无关的，这会引入视觉上无关的标记，破坏视觉表示，并降低生成内容的质量。需要一种能够将水印与视觉内容对齐的方法，以保护知识产权同时保持生成质量。

Method: 提出视觉感知水印(VAW)方法，通过视觉感知机制将水印与视觉内容对齐。该方法包含视觉感知水印嵌入和提取模块，利用视觉特征指导水印生成过程，确保水印与图像内容语义相关。

Result: VAW方法在保持生成质量的同时实现了有效的水印检测，相比视觉无关水印方法，在视觉保真度和水印鲁棒性方面都有显著提升。实验表明该方法在各种攻击下仍能可靠检测水印。

Conclusion: 视觉感知水印为大型视觉语言模型提供了一种有效的知识产权保护方案，通过将水印与视觉内容对齐，解决了视觉无关水印的质量下降问题，在保护版权的同时保持了生成内容的质量。

Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks introduce visually irrelevant tokens and disrupt visual …

</details>


<div id='Alekh Jindal'></div>

# Alekh Jindal [[Back]](#toc)

### [62] [Serverless Computing Architectures for Business Process Automation](https://scholar.google.com/scholar_url?url=https://www.ijarcst.org/index.php/ijarcst/article/download/367/358&hl=zh-CN&sa=X&d=8224923610031408706&ei=ZwFxaZCEAvOlieoPkILrwAk&scisig=AHkA5jSKe0FXCLdZWO9ufPedvqbZ&oi=scholaralrt&hist=i6heNjgAAAAJ:4438704070979798767:AHkA5jShId-iWoaohWFlDvz-Dwie&html=&pos=1&folt=rel)
*D Ramya*

Main category: Alekh Jindal

TL;DR: 无服务器计算架构通过抽象基础设施管理和利用事件驱动的函数即服务模型来编排工作流，实现可扩展、成本效益高且敏捷的业务流程自动化


<details>
  <summary>Details</summary>
Motivation: 传统业务流程自动化面临基础设施管理复杂、扩展性有限、成本效率低等问题，需要更敏捷、可扩展且成本效益高的解决方案

Method: 采用无服务器计算架构，通过抽象基础设施管理，利用事件驱动的函数即服务模型来编排和自动化业务流程工作流

Result: 实现了可扩展、成本效益高且敏捷的业务流程自动化，简化了基础设施管理，提高了资源利用效率

Conclusion: 无服务器计算架构为业务流程自动化提供了有效的技术解决方案，能够显著提升自动化系统的可扩展性、成本效益和敏捷性

Abstract: Serverless Computing Architectures enable scalable, cost-efficient, and agile Business Process Automation by abstracting infrastructure management and leveraging event-driven, function-as-a-service models to orchestrate workflows …

</details>


### [63] [Scalable and Compliant Real-Time ETL Architectures: Converging Serverless Computing, Predictive Analytics, and Adaptive Data Governance](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Aremu-Oluwaferanmi-2/publication/399646082_Scalable_and_Compliant_Real-Time_ETL_Architectures_Converging_Serverless_Computing_Predictive_Analytics_and_Adaptive_Data_Governance/links/69625d4e5cc49c35ce7b0efb/Scalable-and-Compliant-Real-Time-ETL-Architectures-Converging-Serverless-Computing-Predictive-Analytics-and-Adaptive-Data-Governance.pdf&hl=zh-CN&sa=X&d=8644719013143563109&ei=ZwFxaZCEAvOlieoPkILrwAk&scisig=AHkA5jQThdKLKIuRgRRd4ynT0tb5&oi=scholaralrt&hist=i6heNjgAAAAJ:4438704070979798767:AHkA5jShId-iWoaohWFlDvz-Dwie&html=&pos=2&folt=rel)
*S Youseff,A Philip,F Hamzah,A Taofeek,B Barnanas…*

Main category: Alekh Jindal

TL;DR: 论文探讨了大数据时代下传统批处理ETL架构面临的挑战，并提出了现代化ETL系统的设计原则和解决方案


<details>
  <summary>Details</summary>
Motivation: 数据量、速度和多样性的指数级增长对传统批处理ETL架构构成了根本性挑战，需要重新思考ETL系统的设计和运营方式

Method: 分析传统ETL架构的局限性，提出现代化ETL系统的设计原则，可能包括实时处理、流式架构、云原生技术和自动化运维等解决方案

Result: 识别了传统ETL系统在大数据环境下的关键瓶颈，提出了适应现代数据特征的ETL架构转型路径

Conclusion: 组织需要从传统的批处理ETL架构向现代化、实时化、可扩展的ETL系统演进，以适应大数据时代的数据处理需求

Abstract: The exponential growth of data volume, velocity, and variety has fundamentally transformed how organizations design and operate Extract, Transform, and Load (ETL) systems. Traditional batch-oriented ETL architectures, originally designed for …

</details>


<div id='Zongheng Yang'></div>

# Zongheng Yang [[Back]](#toc)

### [64] [AgnoSVD: Dynamic resource allocation for serverless workloads using collaborative filtering](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S2590005625002899&hl=zh-CN&sa=X&d=16697047141436190765&ei=ZgFxaaKZKsm4ieoPof3esQc&scisig=AHkA5jRCTLtU_TN939W7OcbdNOxY&oi=scholaralrt&hist=i6heNjgAAAAJ:778719684493065653:AHkA5jQQvhnv-0XbvZE0RehBjafZ&html=&pos=0&folt=rel)
*MS Kabir,MA Adnan*

Main category: Zongheng Yang

TL;DR: 服务器无服务器计算中，由于云提供商对工作负载细节了解有限，确定最佳资源配置面临重大挑战


<details>
  <summary>Details</summary>
Motivation: 在无服务器计算环境中，云提供商缺乏对工作负载具体特性的深入了解，这导致难以确定最优资源配置，特别是在处理复杂工作负载时，这种挑战更加突出

Method: 论文可能提出了一种基于机器学习或自适应算法的资源配置优化方法，通过分析工作负载特征和性能指标来动态调整资源配置

Result: 预期该方法能够显著提高资源配置效率，降低资源浪费，提升工作负载性能，同时减少云提供商的运营成本

Conclusion: 通过智能化的资源配置优化方法，可以有效解决无服务器计算中资源配置的挑战，为云提供商和工作负载提供更好的性能和成本效益

Abstract: In serverless computing, determining the optimal resource configurations for workloads poses significant challenges, particularly due to the cloud provider's limited visibility into workload specifics. This complexity is amplified when dealing …

</details>


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [65] [Optimus: Deployable Query Optimization via Novel SQL Rewrites](https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DZVzYv7Qois&hl=zh-CN&sa=X&d=8376193066075901600&ei=ZwFxab_3KPOlieoPkILrwAk&scisig=AHkA5jS9_FfixoqCddZw8Pcingt6&oi=scholaralrt&hist=i6heNjgAAAAJ:8133205940682390297:AHkA5jQfTAaKkXi3kBQuJYouRHpx&html=&pos=0&folt=rel)
*R Lone*

Main category: Ziniu Wu

TL;DR: Optimus通过挖掘新颖的执行计划重写来扩展动作空间，学习在这些动作中选择，从而超越传统基于配置的查询优化器


<details>
  <summary>Details</summary>
Motivation: 传统学习型数据库查询优化器通常在一组预定义配置上进行优化，这限制了可达到的计划空间。为了突破这一限制，需要扩展动作空间本身，使优化器能够探索更广泛的执行计划可能性。

Method: Optimus通过挖掘新颖的执行计划重写来扩展动作空间，然后学习在这些扩展的动作中选择最优的执行计划。这种方法超越了传统的基于配置的优化方法。

Result: 通过扩展动作空间并学习选择机制，Optimus能够探索更广泛的执行计划空间，从而获得比传统基于配置的优化器更好的查询性能。

Conclusion: 通过挖掘执行计划重写来扩展动作空间，并结合学习选择机制，可以显著提升学习型查询优化器的性能，超越传统的基于配置的优化方法。

Abstract: Learned database query optimizers typically optimize over a set of configurations, which limits the attainable plan space. Optimus expands the action space itself by mining novel execution plan rewrites and learns to select among these actions …

</details>


### [66] [Searching the Web: From Keywords to Semantic Queries](https://scholar.google.com/scholar_url?url=https://www.academia.edu/download/125824338/INPRO--2009-045.pdf&hl=zh-CN&sa=X&d=12040717176933026319&ei=ZwFxab_3KPOlieoPkILrwAk&scisig=AHkA5jTrqV_e6cd9HSGWqA-F7bSV&oi=scholaralrt&hist=i6heNjgAAAAJ:8133205940682390297:AHkA5jQfTAaKkXi3kBQuJYouRHpx&html=&pos=1&folt=rel)
*A Illarramendi*

Main category: Ziniu Wu

TL;DR: 论文提出在语义网框架下，传统基于用户关键词的搜索引擎已不适用，需要开发基于关键词语义的新方法。


<details>
  <summary>Details</summary>
Motivation: 随着语义网的发展，传统基于关键词的搜索引擎无法充分理解和利用语义信息，需要更智能的搜索方法来处理语义层面的查询。

Method: 论文未在摘要中详细说明具体方法，但提到需要定义基于用户关键词语义的新方法，可能涉及语义分析、本体论、知识图谱等技术。

Result: 摘要未提供具体实验结果，但暗示基于语义的方法将比传统关键词搜索更有效。

Conclusion: 在语义网环境中，必须开发基于语义的新型搜索方法来替代传统关键词搜索引擎。

Abstract: Abstract Within the emergent Semantic Web framework, the use of traditional web search engines based on keywords provided by the users is not adequate anymore. Instead, new methods based on the semantics of user keywords must be defined to …

</details>


<div id='Matei Zaharia'></div>

# Matei Zaharia [[Back]](#toc)

### [67] [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.02076&hl=zh-CN&sa=X&d=1915482236404124982&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jSIfa7a_J_3aEFHUin_0Jpv&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=0&folt=rel)
*Y Shu,Y Tian,C Xu,Y Wang,H Chen*

Main category: Matei Zaharia

TL;DR: 扩散语言模型作为自回归模型的替代方案，通过并行文本生成提升效率，但现有块状方法存在上下文不连续问题，本文提出跨块注意力机制解决此问题


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）作为自回归模型的替代方案，能够实现并行文本生成，提高了推理效率。然而，现有的块状方法虽然兼容KV缓存，但存在块间上下文不连续的问题，影响了生成质量

Method: 提出跨块注意力机制，在块状扩散语言模型中引入跨块信息交互，解决上下文不连续问题，同时保持KV缓存兼容性和推理效率

Result: 跨块注意力机制显著改善了块状扩散语言模型的生成质量，在保持高效推理的同时，解决了上下文不连续问题，提升了文本生成的一致性

Conclusion: 跨块注意力机制是解决块状扩散语言模型中上下文不连续问题的有效方法，在保持推理效率的同时显著提升了文本生成质量，为扩散语言模型的实用化提供了重要改进

Abstract: Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based …

</details>


### [68] [ForgetMark: Stealthy Fingerprint Embedding via Targeted Unlearning in Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08189&hl=zh-CN&sa=X&d=10986440491085924953&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jThCjeMrogiSddIygMf3Z39&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=1&folt=rel)
*Z Xu,H Zhang,Z Wang,Q Liu,H Xu,W Xing,M Han*

Main category: Matei Zaharia

TL;DR: ForgetMark是一种新型的隐蔽后门攻击方法，通过低困惑度触发器和动态响应模式来规避现有检测机制


<details>
  <summary>Details</summary>
Motivation: 现有后门指纹存在高困惑度触发器易被过滤、固定响应模式易被启发式检测器发现、以及在良性输入上产生虚假激活等问题，需要开发更隐蔽的后门攻击方法

Method: ForgetMark采用低困惑度触发器设计，结合动态响应模式，避免固定的输出模式，同时减少在良性输入上的虚假激活

Result: ForgetMark能够有效规避现有后门检测机制，在保持攻击效果的同时显著提高了隐蔽性

Conclusion: ForgetMark为后门攻击提供了新的隐蔽性范式，揭示了现有防御机制的局限性，对AI安全领域具有重要意义

Abstract: Existing invasive (backdoor) fingerprints suffer from high-perplexity triggers that are easily filtered, fixed response patterns exposed by heuristic detectors, and spurious activations on benign inputs. We introduce\textsc {ForgetMark}, a stealthy …

</details>


### [69] [DIP: Dynamic In-Context Planner For Diffusion Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03199&hl=zh-CN&sa=X&d=12451250639277289841&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jRMF0otfo9Jh9GSqXw9_Qbo&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=2&folt=rel)
*Y Li,H Meng,C Wang,H Chen*

Main category: Matei Zaharia

TL;DR: 扩散语言模型（DLMs）在自然语言任务中表现出色，但随着上下文长度增加，其双向注意力机制导致计算成本显著上升


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在自然语言处理任务中展现出强大潜力，特别是在上下文学习方面。然而，由于其双向注意力机制，当上下文长度增加时，模型的计算成本会显著上升，这限制了DLMs在实际应用中的可扩展性和效率。

Method: 论文未提供具体方法细节，但从摘要推断，研究可能探讨了优化DLMs计算效率的技术，可能包括注意力机制改进、计算复杂度降低策略或针对长上下文场景的模型架构调整。

Result: 摘要未提供具体实验结果，但暗示了DLMs在长上下文场景下的计算效率问题，以及需要解决这一挑战以充分发挥DLMs在自然语言处理任务中的潜力。

Conclusion: 扩散语言模型虽然具有强大的自然语言处理能力，但其双向注意力机制导致的计算成本随上下文长度增加而显著上升，这是需要解决的关键技术挑战。

Abstract: Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases …

</details>


### [70] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.09865&hl=zh-CN&sa=X&d=2690202150983940264&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jRv1BcUPxtTPHZamoSkXvF8&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=3&folt=rel)
*J Sander,B Jalaian,VR Dasari*

Main category: Matei Zaharia

TL;DR: 该论文探讨了在资源受限的边缘设备上部署大型语言模型（LLMs）的挑战，并提出相应的优化方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理方面表现出色，但由于其高计算、内存和能耗需求，难以在资源受限的边缘设备上部署。需要开发优化技术来解决这一部署难题。

Method: 论文可能涉及模型压缩、量化、剪枝、知识蒸馏、硬件感知优化等方法来减少LLMs的计算和内存开销，使其适应边缘设备。

Result: 通过优化技术，能够在保持模型性能的同时显著降低计算复杂度、内存占用和能耗，使LLMs能够在边缘设备上有效运行。

Conclusion: 通过适当的优化策略，大型语言模型可以在资源受限的边缘设备上成功部署，为边缘计算环境中的自然语言处理应用开辟新可能性。

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires …

</details>


### [71] [Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.01966&hl=zh-CN&sa=X&d=11819346928659783213&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jT2SG6j62RgQcMBteFhxyED&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=4&folt=rel)
*B Yin,Q Li,R Yu,X Wang*

Main category: Matei Zaharia

TL;DR: 该论文研究指令调优中基于LLM的提示词改写方法，通过实例级审计分析改写对模型性能的影响


<details>
  <summary>Details</summary>
Motivation: 指令调优越来越依赖基于LLM的提示词改写，通过外部改写器选择性重写训练语料中的提示词以提高清晰度和指令对齐性。这促使进行实例级审计，分析改写对模型性能的影响

Method: 采用实例级审计方法，分析基于LLM的提示词改写过程，研究改写器如何选择性重写训练语料中的提示词，以及这种改写如何影响指令对齐和模型性能

Result: 研究发现提示词改写对模型性能有显著影响，但具体结果需要进一步分析改写策略、选择标准和最终性能指标

Conclusion: 基于LLM的提示词改写是指令调优的重要技术，需要进行系统性的实例级审计来理解改写对模型性能的影响机制

Abstract: Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit …

</details>


### [72] [Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08209&hl=zh-CN&sa=X&d=10988629267390072525&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jQK3aGEgbj3AZHidPrhyj9y&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=5&folt=rel)
*R Li,J Xu,X Chen,Y Yang,J Wang,X Chen,C Xie…*

Main category: Matei Zaharia

TL;DR: LLMs在生物医学、材料和金融等高风险领域部署时，需要注入私有、快速演变的领域特定知识，但这些知识在公开预训练中代表性不足且具有专有性。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如生物医学、材料、金融）部署大型语言模型时面临挑战：需要注入私有、快速演变的领域特定知识，但这些知识通常是专有的、在公开预训练数据中代表性不足，且模型需要保持对基础知识的掌握。

Method: 论文未在摘要中明确说明具体方法，但暗示需要开发能够有效注入私有领域知识的技术，同时保持模型对基础知识的掌握。

Result: 摘要未提供具体实验结果，但暗示需要解决LLMs在高风险领域部署时的知识注入挑战。

Conclusion: 在高风险领域成功部署LLMs需要解决私有、快速演变、代表性不足的领域特定知识的有效注入问题。

Abstract: In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining …

</details>


### [73] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.10416&hl=zh-CN&sa=X&d=1285779519890154633&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jR5rs8caZkgTxyzE2oN3v5Q&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=6&folt=rel)
*T Shen,R Mao,J Wang,H Sun,J Zhang,X Zhang…*

Main category: Matei Zaharia

TL;DR: 大语言模型与人类偏好对齐是关键挑战，传统微调方法计算成本高且不灵活，测试时对齐提供有前景的替代方案


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型与人类偏好对齐方法（如RLHF）存在计算成本高、不灵活的问题，需要更高效、灵活的对齐方法

Method: 测试时对齐方法，可能涉及无需微调的实时调整策略，具体方法未在摘要中详细说明

Result: 测试时对齐方法相比传统微调方法在计算效率和灵活性方面具有优势，但现有方法可能存在某些局限性

Conclusion: 测试时对齐是大语言模型与人类偏好对齐的有前景方向，需要进一步改进现有方法的局限性

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on …

</details>


### [74] [Multivector Reranking in the Era of Strong First-Stage Retrievers](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.05200&hl=zh-CN&sa=X&d=12062183794194738319&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jT_E695KkHuBuolXGqNscyA&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=7&folt=rel)
*S Martinico,FM Nardini,C Rulli,R Venturini*

Main category: Matei Zaharia

TL;DR: 论文提出了一种名为"Gather-and-Rerank"的两阶段检索方法，通过使用可学习的多向量表示来提高检索效率，避免对大规模语料库进行耗时的令牌级检索。


<details>
  <summary>Details</summary>
Motivation: 现代搜索系统中基于学习的多向量表示虽然检索效果好，但在实际应用中受到令牌级检索成本过高的限制。大多数系统采用两阶段方法，但现有方法在效率和效果之间存在权衡。

Method: 提出Gather-and-Rerank框架，第一阶段使用轻量级方法快速收集候选文档，第二阶段使用更强大的多向量表示对候选文档进行重新排序。该方法平衡了检索效率和效果。

Result: 该方法在多个基准测试中表现出色，相比现有方法在保持高检索效率的同时显著提升了检索效果，特别是在大规模语料库中表现优异。

Conclusion: Gather-and-Rerank框架为基于学习的多向量表示系统提供了一种实用的解决方案，有效解决了令牌级检索成本过高的问题，平衡了检索效率和效果。

Abstract: Learned multivector representations power modern search systems with strong retrieval effectiveness, but their real-world use is limited by the high cost of exhaustive token-level retrieval. Therefore, most systems adopt a\emph {gather-and …

</details>


### [75] [Inhibitory Attacks on Backdoor-based Fingerprinting for Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.04261&hl=zh-CN&sa=X&d=821913564505500249&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jSet5qUIX3dhJek-51hMUY4&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=8&folt=rel)
*H Fu,W Peng,Y Zhou,J Wu,J Wen,Y Xue*

Main category: Matei Zaharia

TL;DR: 该论文提出了一种基于后门的LLM指纹识别方法，用于保护大型语言模型的知识产权


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在商业和研究领域的广泛应用，对强大知识产权保护的需求日益增长。传统的保护方法可能不足以应对LLM的特殊性，需要专门针对LLM设计的知识产权保护方案。

Method: 采用基于后门的指纹识别技术。通过在LLM中嵌入特定的后门触发器或模式，创建独特的模型指纹，用于验证模型所有权和检测未经授权的使用。

Result: 基于后门的LLM指纹识别被证明是一种有前景的解决方案，能够有效保护LLM的知识产权，防止未经授权的复制和分发。

Conclusion: 后门指纹识别为LLM知识产权保护提供了可行的技术途径，有助于促进LLM技术的健康发展，同时保护模型开发者的合法权益。

Abstract: The widespread adoption of Large Language Model (LLM) in commercial and research settings has intensified the need for robust intellectual property protection. Backdoor-based LLM fingerprinting has emerged as a promising solution for this …

</details>


### [76] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.23126&hl=zh-CN&sa=X&d=5953785134874696717&ei=aAFxaZPfMKOi6rQPoMCXoAE&scisig=AHkA5jTaTbpgg2MiPGgpXUt21ysp&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=9&folt=rel)
*Y Li,T Lan,Z Qi*

Main category: Matei Zaharia

TL;DR: DPO及其变体存在两个基本限制：最优策略依赖于任意参考策略，且无法表达某些偏好类型，导致表达能力受限。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO及其变体因简单性和离线稳定性成为大语言模型对齐的标准方法，但研究者发现两个基本限制：1）最优策略依赖于任意参考策略，这在实际应用中存在问题；2）无法表达某些偏好类型，限制了其表达能力。

Method: 论文未在摘要中明确说明具体方法，但暗示需要解决DPO的这两个限制，可能提出新的对齐框架或改进方法。

Result: 摘要未提供具体实验结果，但指出DPO的局限性，暗示需要新的解决方案来克服这些限制。

Conclusion: DPO存在基本限制，需要新的对齐方法来解决参考策略依赖性和表达能力不足的问题。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary …

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [77] [Trajectory-Driven Multi-Product Influence Maximization in Billboard Advertising](https://arxiv.org/abs/2601.14737)
*Dildar Ali,Suman Banerjee,Rajibul Islam*

Main category: cs.DB

TL;DR: 该论文研究了多产品广告牌投放优化问题，提出了两种变体：共享时隙选择和互斥时隙选择，并设计了相应的近似算法。


<details>
  <summary>Details</summary>
Motivation: 广告牌广告作为一种户外广告技术，需要选择有限数量的广告位来最大化影响力。当商业公司需要推广多个产品，且每个产品都有特定的影响力需求时，如何高效选择广告位成为关键问题。

Method: 将第一个变体建模为多子模覆盖问题，基于连续贪心框架和随机舍入设计双准则近似算法；将第二个变体作为推广，提出基于采样的近似方法和高效的原对偶贪心算法来保证互斥性。

Result: 在真实世界轨迹和广告牌数据集上的大量实验表明，所提出的解决方案在效果和效率方面表现优异。

Conclusion: 论文成功解决了多产品广告牌投放优化问题，提出的算法框架能够有效处理共享时隙和互斥时隙两种场景，为实际应用提供了理论支持和实用工具。

Abstract: Billboard Advertising has emerged as an effective out-of-home advertising technique, where the goal is to select a limited number of slots and play advertisement content there, with the hope that it will be observed by many people and, effectively, a significant number of them will be influenced towards the brand. Given a trajectory and a billboard database and a positive integer $k$, how can we select $k$ highly influential slots to maximize influence? In this paper, we study a variant of this problem where a commercial house wants to make a promotion of multiple products, and there is an influence demand for each product. We have studied two variants of the problem. In the first variant, our goal is to select $k$ slots such that the respective influence demand of each product is satisfied. In the other variant of the problem, we are given with $\ell$ integers $k_1,k_2, \ldots, k_{\ell}$, the goal here is to search for $\ell$ many set of slots $S_1, S_2, \ldots, S_{\ell}$ such that for all $i \in [\ell]$, $|S_{i}| \leq k_i$ and for all $i \neq j$, $S_i \cap S_j=\emptyset$ and the influence demand of each of the products gets satisfied. We model the first variant of the problem as a multi-submodular cover problem and the second variant as its generalization. To solve the common-slot variant, we formulate the problem as a multi-submodular cover problem and design a bi-criteria approximation algorithm based on the continuous greedy framework and randomized rounding. For the disjoint-slot variant, we proposed a sampling-based approximation approach along with an efficient primal-dual greedy algorithm that enforces disjointness naturally. Extensive experiments with real-world trajectory and billboard datasets highlight the effectiveness and efficiency of the proposed solution approaches.

</details>


<div id='Xuanhe Zhou'></div>

# Xuanhe Zhou [[Back]](#toc)

### [78] [Accelerating dataset generation for machine learning using large language models: a pharmaceutical additive manufacturing case](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0378517326000359&hl=zh-CN&sa=X&d=12602310999167599204&ei=aQFxaYG2A6Oi6rQPoMCXoAE&scisig=AHkA5jRPoHDjfEPztd3bgInXW09K&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=0&folt=cit)
*P Carou*

Main category: Xuanhe Zhou

TL;DR: 开发了一个基于GPT-4的深度提示工程框架，用于自动化从专业文献中提取结构化数据集，应用于药物研究领域


<details>
  <summary>Details</summary>
Motivation: 在药物研究等专业领域，创建高质量机器学习训练数据集通常受限于从异构文献中手动提取和计算关键参数所需的大量人工努力，需要自动化解决方案来加速这一过程

Method: 开发了一个新颖的深度提示工程框架，将GPT-4转化为自动化生成结构化数据集的工具，采用多集提示策略，对70篇全文文献进行分析

Result: GPT-4成功转化为自动化生成结构化数据集的强大工具，能够从专业文献中提取关键参数，显著减少了手动工作量

Conclusion: 深度提示工程框架有效解决了专业领域数据集创建的瓶颈问题，为药物研究等领域的机器学习应用提供了高效的自动化数据提取解决方案

Abstract: Creating high-quality datasets for training machine learning models in specialized domains like pharmaceutical research is often constrained by the manual effort required to extract and compute critical parameters from heterogeneous literature. A novel deep prompt-engineering framework was developed to transform GPT-4 into a robust tool for automated and accelerated generation of structured datasets. Using a multi-set prompt strategy, GPT-4 analysed 70 full-text articles from literature on …

</details>


### [79] [UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.09215&hl=zh-CN&sa=X&d=16938448586551702807&ei=aAFxaf_YD-u7ieoPncyzoAs&scisig=AHkA5jRCp4Y-AebCwLX0mvDOUugz&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=1&folt=rel)
*F Zhang,S Li,C Zhang,Z Ma,J Xu,J Gao,J Hao,R He…*

Main category: Xuanhe Zhou

TL;DR: 用户模拟器作为智能体后训练的关键交互环境，理想情况下应具备跨领域泛化能力并能主动参与谈判（挑战或讨价还价），但现有方法存在两个主要问题


<details>
  <summary>Details</summary>
Motivation: 用户模拟器在智能体后训练中扮演关键角色，但现有方法在跨领域泛化和主动谈判能力方面存在不足，需要改进以更好地支持智能体训练

Method: 摘要未详细描述具体方法，但暗示需要解决当前用户模拟器存在的两个主要问题，可能涉及改进模拟器的泛化能力和主动谈判机制

Result: 摘要未提供具体实验结果，但指出了当前用户模拟器方法的局限性，为后续改进提供了方向

Conclusion: 需要开发更先进的用户模拟器，具备跨领域泛化能力和主动谈判能力，以更好地支持智能体的后训练过程

Abstract: User simulators serve as the critical interactive environment for agent post-training, and an ideal user simulator generalizes across domains and proactively engages in negotiation by challenging or bargaining. However, current methods exhibit two …

</details>


### [80] [WOC: Dual-Path Weighted Object Consensus Made Efficient](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.20485&hl=zh-CN&sa=X&d=9346853648129829929&ei=aAFxaf_YD-u7ieoPncyzoAs&scisig=AHkA5jSbo_xl-JIaV41FTMkcLGN3&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=2&folt=rel)
*T Fonseca,G Zhang*

Main category: Xuanhe Zhou

TL;DR: 论文指出现有共识协议无法同时优化节点异构性和工作负载独立性，提出新协议解决这一关键挑战


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统面临关键挑战：现有共识协议要么优化节点异构性（如Cabinet使用加权仲裁），要么优化工作负载独立性，但无法同时兼顾两者

Method: 未在摘要中明确说明具体方法，但暗示需要设计新协议来同时处理节点异构性和工作负载独立性

Result: 未在摘要中提供具体实验结果

Conclusion: 需要新的共识协议设计来同时优化节点异构性和工作负载独立性，以解决现代分布式系统的关键挑战

Abstract: Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but …

</details>


### [81] [EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03471&hl=zh-CN&sa=X&d=8873960001622105257&ei=aAFxaf_YD-u7ieoPncyzoAs&scisig=AHkA5jRlwZXalKtuijihThk2vtmk&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=3&folt=rel)
*M Wei,D Min,Z Liu,Y Xie,G Wu,C Yang,MSY Lau…*

Main category: Xuanhe Zhou

TL;DR: 该论文提出了一个用于流行病学推理的基准测试，旨在评估模型在综合研究证据以推断疾病负担、传播动态和干预效果方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的医学问答基准主要关注临床决策，缺乏对流行病学推理能力的评估。可靠的流行病学推理需要综合研究证据来推断人口层面的疾病负担、传播动态和干预效果，这对公共卫生决策至关重要。

Method: 论文提出了一个专门针对流行病学推理的基准测试，可能包括：1）收集和整理流行病学研究数据；2）设计评估模型综合研究证据能力的任务；3）建立评估指标来衡量模型在疾病负担估计、传播动态分析和干预效果评估等方面的表现。

Result: 论文可能展示了当前模型在流行病学推理任务上的局限性，并提供了基准测试的结果，显示了不同模型在综合研究证据、进行人口层面推断方面的性能差异。

Conclusion: 该研究强调了开发专门评估流行病学推理能力的基准测试的重要性，为改进模型在公共卫生领域的应用提供了基础，有助于提升疾病负担估计、传播动态分析和干预效果评估的可靠性。

Abstract: Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical …

</details>
