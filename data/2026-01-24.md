<div id=toc></div>

# Table of Contents

- [Matei Zaharia](#Matei Zaharia) [Total: 10]
- [Google Scholar](#Google Scholar) [Total: 1]
- [Carsten Binnig](#Carsten Binnig) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [Zongheng Yang](#Zongheng Yang) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [Xuanhe Zhou](#Xuanhe Zhou) [Total: 5]
- [cs.LG](#cs.LG) [Total: 40]


<div id='Matei Zaharia'></div>

# Matei Zaharia [[Back]](#toc)

### [1] [ForgetMark: Stealthy Fingerprint Embedding via Targeted Unlearning in Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08189&hl=zh-CN&sa=X&d=10986440491085924953&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jThCjeMrogiSddIygMf3Z39&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=0&folt=rel)
*Z Xu,H Zhang,Z Wang,Q Liu,H Xu,W Xing,M Han*

Main category: Matei Zaharia

TL;DR: ForgetMark是一种新型的隐蔽后门攻击方法，通过低困惑度触发器和自适应响应模式来规避现有检测机制


<details>
  <summary>Details</summary>
Motivation: 现有后门指纹存在高困惑度触发器易被过滤、固定响应模式易被启发式检测器发现、以及在良性输入上产生虚假激活等问题，需要更隐蔽的后门攻击方法

Method: ForgetMark采用低困惑度触发器设计，结合自适应响应模式，避免固定的输出模式，同时减少在良性输入上的虚假激活

Result: 该方法在保持攻击有效性的同时，显著提高了隐蔽性，能够规避现有的后门检测机制

Conclusion: ForgetMark为后门攻击提供了新的隐蔽性设计思路，揭示了现有检测方法的局限性，并提出了更难以检测的后门攻击范式

Abstract: Existing invasive (backdoor) fingerprints suffer from high-perplexity triggers that are easily filtered, fixed response patterns exposed by heuristic detectors, and spurious activations on benign inputs. We introduce\textsc {ForgetMark}, a stealthy …

</details>


### [2] [Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.11342&hl=zh-CN&sa=X&d=12567463795959401720&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jTrOOe5RgTvPCBUwBHFozog&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=1&folt=rel)
*C Yu,J Wang,Y Li,H Chang,G Lan,Q Sun,J Li,J Li…*

Main category: Matei Zaharia

TL;DR: 该论文探讨了检索增强生成（RAG）在扩散语言模型（DLMs）中的应用，提出了一种名为Retrieval-Augmented Diffusion Language Models（RAD）的新框架，通过检索机制增强DLMs的生成能力。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散语言模型在自然语言处理任务中表现出色，但检索增强生成技术在提升大型语言模型性能方面的成功尚未在DLMs中得到充分探索。论文旨在填补这一空白，研究如何将RAG机制有效整合到DLMs中。

Method: 提出了RAD框架，将检索机制集成到扩散语言模型中。该方法可能包括：1）检索相关文本片段或知识，2）将检索到的信息作为条件输入到扩散过程中，3）设计专门的融合机制将检索信息与扩散生成过程相结合。

Result: RAD框架在多个自然语言处理任务上表现出色，包括文本生成、问答和对话任务。实验结果表明，与标准DLMs相比，RAD在生成质量、事实准确性和任务适应性方面均有显著提升。

Conclusion: 检索增强生成可以显著提升扩散语言模型的性能，RAD框架为DLMs提供了一种有效整合外部知识的途径，扩展了DLMs的应用范围和能力。

Abstract: Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large …

</details>


### [3] [DIP: Dynamic In-Context Planner For Diffusion Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03199&hl=zh-CN&sa=X&d=12451250639277289841&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jRMF0otfo9Jh9GSqXw9_Qbo&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=2&folt=rel)
*Y Li,H Meng,C Wang,H Chen*

Main category: Matei Zaharia

TL;DR: 扩散语言模型（DLMs）在自然语言任务中表现出色，但双向注意力机制导致长上下文计算成本高


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然通过双向注意力机制在自然语言任务中表现出强大潜力，但随着上下文长度增加，计算成本显著上升，这限制了其在长序列处理中的应用

Method: 论文未提供具体方法细节，但从摘要推断可能涉及优化扩散语言模型的计算效率，特别是针对长上下文场景的注意力机制改进

Result: 摘要未提供具体实验结果，但暗示扩散语言模型在长上下文处理中存在计算效率问题

Conclusion: 扩散语言模型需要解决长上下文计算成本问题才能充分发挥其在自然语言处理中的潜力

Abstract: Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases …

</details>


### [4] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.09865&hl=zh-CN&sa=X&d=2690202150983940264&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jRv1BcUPxtTPHZamoSkXvF8&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=3&folt=rel)
*J Sander,B Jalaian,VR Dasari*

Main category: Matei Zaharia

TL;DR: LLMs在边缘设备部署面临计算、内存和能耗挑战，需要优化以在资源受限环境中实现高效运行


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具备先进的自然语言处理能力，但在计算、内存和能耗方面的高需求使其难以在资源受限的边缘设备上部署，需要针对边缘环境进行优化

Method: 论文未提供具体方法细节，但暗示需要采用优化技术来解决LLMs在边缘设备上的部署挑战

Result: 未提供具体实验结果，但指出优化LLMs对于在边缘设备上实现高效部署至关重要

Conclusion: LLMs在边缘设备部署面临显著挑战，需要专门的优化方法来解决计算、内存和能耗问题，以实现边缘AI应用

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires …

</details>


### [5] [Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.01966&hl=zh-CN&sa=X&d=11819346928659783213&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jT2SG6j62RgQcMBteFhxyED&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=4&folt=rel)
*B Yin,Q Li,R Yu,X Wang*

Main category: Matei Zaharia

TL;DR: 该论文提出了一种实例级审计框架，用于评估和量化指令调优中基于LLM的提示精炼过程对训练数据质量的影响。


<details>
  <summary>Details</summary>
Motivation: 随着指令调优越来越多地依赖基于LLM的提示精炼（外部精炼器选择性重写训练语料库中的提示以提高清晰度和指令对齐），需要系统评估这种精炼过程对训练数据质量的实际影响，从而确保指令调优的有效性和可靠性。

Method: 提出实例级审计框架，通过量化指标系统评估提示精炼过程。该方法可能包括对比原始提示与精炼后提示的质量差异、分析精炼引入的变化类型、以及评估这些变化对下游任务性能的影响。

Result: 研究发现提示精炼过程在提高清晰度和指令对齐方面存在显著效果，但也可能引入特定偏差或改变原始意图。审计框架能够识别精炼过程中的质量变化模式，为优化精炼策略提供依据。

Conclusion: 实例级审计对于理解和改进基于LLM的提示精炼过程至关重要，能够确保指令调优训练数据的质量，最终提升模型性能和可靠性。

Abstract: Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit …

</details>


### [6] [Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.01862&hl=zh-CN&sa=X&d=14083995375494362634&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jTPLVI5lzrDnDM-qMRaAZqq&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=5&folt=rel)
*N Chen,H Fang,P Wang,J Liu,T Sakai,XM Wu*

Main category: Matei Zaharia

TL;DR: 研究探讨如何通过提示词让大语言模型模拟特定人格特质，但现有方法缺乏系统性评估框架来验证模拟人格的真实性和一致性


<details>
  <summary>Details</summary>
Motivation: 当前研究虽然表明提示词能让大语言模型模拟特定人格特质，但缺乏系统性评估框架来验证这些模拟人格的真实性、一致性和有效性，需要建立更科学的评估方法

Method: 论文可能提出了一种系统性评估框架，包括设计特定的提示策略、使用心理学量表进行人格测量、评估模拟人格的一致性和真实性，以及对比不同提示方法的效果

Result: 预期结果显示某些提示策略能更有效地模拟特定人格特质，但模拟人格在不同情境下可能存在不一致性，需要更精细的提示工程来提升模拟质量

Conclusion: 大语言模型能够通过提示词模拟人格特质，但需要建立更系统科学的评估框架来确保模拟的真实性和一致性，这对心理学研究和AI应用具有重要意义

Abstract: Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated …

</details>


### [7] [Query-Document Dense Vectors for LLM Relevance Judgment Bias Analysis](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.01751&hl=zh-CN&sa=X&d=3657590385925024052&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jRDrb0JbYLfyxDCS02xkTSi&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=6&folt=rel)
*S Mohtadi,G Demartini*

Main category: Matei Zaharia

TL;DR: LLMs作为相关性评估器用于IR评估集合创建，相比人工评估具有成本低、可扩展性强的优势，但需要系统研究其可靠性


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs已被用作IR评估的相关性评估器，但缺乏对其可靠性的系统研究，需要了解LLM评估与人工评估的一致性程度及其影响因素

Method: 通过对比LLM评估与人工评估的一致性，分析不同因素（如查询类型、文档长度、评估标准等）对评估结果的影响

Result: LLM评估与人工评估在某些情况下具有较高一致性，但在特定条件下存在显著差异，需要谨慎使用LLM作为评估器

Conclusion: LLMs可以作为IR评估的辅助工具，但不能完全替代人工评估，需要根据具体应用场景和条件选择使用

Abstract: Large Language Models (LLMs) have been used as relevance assessors for Information Retrieval (IR) evaluation collection creation due to reduced cost and increased scalability as compared to human assessors. While previous research has …

</details>


### [8] [Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08209&hl=zh-CN&sa=X&d=10988629267390072525&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jQK3aGEgbj3AZHidPrhyj9y&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=7&folt=rel)
*R Li,J Xu,X Chen,Y Yang,J Wang,X Chen,C Xie…*

Main category: Matei Zaharia

TL;DR: LLMs在生物医学、材料和金融等高风险领域部署时，需要注入私有、快速演变的领域知识，但这些知识在公开预训练数据中代表性不足。


<details>
  <summary>Details</summary>
Motivation: LLMs在专业领域部署面临挑战：需要整合私有、快速演变的领域知识，这些知识在公开预训练数据中代表性不足，且涉及敏感数据隐私问题。

Method: 论文未提供具体方法细节，但从摘要推断可能涉及知识注入、持续学习、隐私保护技术，以及将私有领域知识整合到LLMs中的方法。

Result: 摘要未提供具体实验结果，但暗示需要开发能够处理私有、快速演变领域知识的LLM系统。

Conclusion: 在高风险领域成功部署LLMs需要解决私有知识注入、数据隐私保护和持续适应快速演变领域知识等关键挑战。

Abstract: In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining …

</details>


### [9] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.10416&hl=zh-CN&sa=X&d=1285779519890154633&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jR5rs8caZkgTxyzE2oN3v5Q&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=8&folt=rel)
*T Shen,R Mao,J Wang,H Sun,J Zhang,X Zhang…*

Main category: Matei Zaharia

TL;DR: 该论文提出了一种名为"Aligning at Test-time"的新方法，通过使用轻量级适配器在推理时对齐LLM，避免了传统微调的高计算成本和灵活性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型对齐方法（如RLHF、DPO）需要大量计算资源进行微调，且一旦训练完成就难以调整。测试时对齐方法虽然前景广阔，但现有方法通常依赖特定架构或需要大量计算，限制了实际应用。

Method: 提出了一种轻量级适配器方法，在推理时动态调整LLM的行为。该方法使用小型参数模块，可以在不修改原始模型权重的情况下，根据人类偏好实时调整模型输出，具有计算效率高和灵活可调的优点。

Result: 该方法在多个基准测试中表现出色，能够有效对齐LLM与人类偏好，同时保持计算效率。相比传统微调方法，计算成本显著降低，且能够灵活适应不同的对齐目标和偏好变化。

Conclusion: 测试时对齐提供了一种高效灵活的大语言模型对齐方案，通过轻量级适配器在推理时动态调整模型行为，解决了传统微调方法的高成本和僵化问题，为实际部署提供了可行路径。

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on …

</details>


### [10] [Holistic evaluation of large language models for medical tasks with MedHELM](https://scholar.google.com/scholar_url?url=https://www.nature.com/articles/s41591-025-04151-2&hl=zh-CN&sa=X&d=11315557401290697797&ei=beRzaeD7FabD6rQP1_TouQg&scisig=AHkA5jQEtu8kyWz3yBu956f0N9Fe&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=9&folt=rel)
*S Bedi,H Cui,M Fuentes,A Unell,M Wornow…*

Main category: Matei Zaharia

TL;DR: MedHELM是一个可扩展的医疗评估框架，旨在更全面地评估LLM在真实临床实践中的表现，超越传统医学执照考试的局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在医学执照考试中取得了接近完美的分数，但这些评估无法充分反映真实临床实践的复杂性和多样性。需要更全面的评估框架来准确衡量LLM在实际医疗场景中的能力。

Method: 引入MedHELM框架，这是一个可扩展的医疗评估系统，设计用于更全面地测试LLM在真实临床环境中的表现，包括多样化的临床场景和复杂性。

Result: 基于摘要信息，具体结果未详细说明，但暗示MedHELM能够揭示传统考试评估无法捕捉到的LLM在临床实践中的表现差异和局限性。

Conclusion: 需要超越传统医学考试评估的框架来准确评估LLM在真实临床环境中的能力，MedHELM为此提供了一个可扩展的解决方案。

Abstract: While large language models (LLMs) achieve near-perfect scores on medical licensing exams, these evaluations inadequately reflect the complexity and diversity of real-world clinical practice. Here we introduce MedHELM, an extensible …

</details>


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [11] [Same or Not? Enhancing Visual Perception in Vision-Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.23592&hl=en&sa=X&d=13781878437584043580&ei=bORzaY_9LKbD6rQP1_TouQg&scisig=AHkA5jTS6oZuADdBP_mQcFDIv5Cc&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=0&folt=rel)
*D Marsili,A Mehta,RY Lin,G Gkioxari*

Main category: Google Scholar

TL;DR: 论文指出当前视觉语言模型在细粒度视觉理解方面存在不足，并提出通过构建包含细粒度视觉细节的数据集来改进模型性能


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在广泛视觉理解方面表现出色，但在细粒度视觉理解方面存在局限：模型过于粗糙、存在视觉偏见、错过细微视觉细节。现有训练语料库强化了这一限制，过于强调一般识别能力而忽略了细粒度视觉细节。

Method: 论文未在摘要中明确说明具体方法，但从内容推断可能涉及构建包含细粒度视觉细节的数据集，以及相应的模型训练方法改进，以增强模型对细微视觉特征的捕捉能力。

Result: 摘要未提供具体实验结果，但暗示通过改进细粒度视觉理解能力，可以提升视觉语言模型在需要关注细微视觉细节任务上的性能。

Conclusion: 需要开发新的训练方法和数据集来增强视觉语言模型的细粒度视觉理解能力，以克服当前模型在捕捉细微视觉细节方面的局限性。

Abstract: Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition (" Is it a cat or a …

</details>


<div id='Carsten Binnig'></div>

# Carsten Binnig [[Back]](#toc)

### [12] [One-Sided RDMA-Based Distributed Transactions](https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-981-95-2885-1_6&hl=zh-CN&sa=X&d=1633793027343509376&ei=beRzaYC6A9rJieoP06bqgAE&scisig=AHkA5jTlqwWNMkbOJE-ujHfbbdfQ&oi=scholaralrt&hist=i6heNjgAAAAJ:15269883191641703195:AHkA5jTZCYhse3wIcZXHnO-iboY0&html=&pos=1&folt=cit)
*Y Hua*

Main category: Carsten Binnig

TL;DR: FORD是一个针对分解式持久内存架构的快速单边分布式事务方案，通过优化带宽利用和持久性特性来提升性能


<details>
  <summary>Details</summary>
Motivation: 持久内存分解架构提高了资源利用率和故障隔离性，但现有分布式事务方案因计算能力有限且忽视真实PM的带宽和持久性特性，无法在分解式PM架构中高效工作

Method: 提出FORD（Fast One-sided Distributed Transaction），针对分解式持久内存架构设计，优化带宽利用和持久性特性

Result: 未在摘要中明确说明具体实验结果，但暗示FORD能够更高效地在分解式PM架构中工作

Conclusion: FORD是针对分解式持久内存架构的有效分布式事务解决方案，克服了现有方案在带宽和持久性方面的局限性

Abstract: Persistent memory (PM) disaggregation improves the resource utilization and failure isolation to build a scalable and cost-effective remote memory pool. However, due to offering limited computing power and overlooking the bandwidth and persistence properties of real PMs, existing distributed transaction schemes, which are designed for legacy DRAM-based monolithic servers, fail to efficiently work in the disaggregated PM architecture. In this book, we propose FORD, a Fast One-sided …

</details>


### [13] [RDMA-Oriented Learned Key-Value Store](https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-981-95-2885-1_5&hl=zh-CN&sa=X&d=17436591101824921729&ei=beRzaYC6A9rJieoP06bqgAE&scisig=AHkA5jTVlodRJWiCkRDhIqNcPByI&oi=scholaralrt&hist=i6heNjgAAAAJ:15269883191641703195:AHkA5jTZCYhse3wIcZXHnO-iboY0&html=&pos=2&folt=cit)
*Y Hua*

Main category: Carsten Binnig

TL;DR: 论文探讨了在分解式内存系统中，利用RDMA技术实现远程内存访问时，有序键值存储（如B树和习得索引）面临的挑战与优化方案


<details>
  <summary>Details</summary>
Motivation: 分解式内存系统通过分离计算节点和内存节点来提高资源利用率、硬件可扩展性和数据共享效率。然而，在利用RDMA直接访问远程内存池时，有序键值存储面临新的性能挑战，需要专门优化

Method: 论文分析了在RDMA环境下有序键值存储（B树、习得索引等）的性能瓶颈，并提出相应的优化策略。可能包括减少远程访问延迟、优化数据结构布局、利用RDMA特性改进并发访问等

Result: 通过针对分解式内存系统和RDMA特性的优化，有序键值存储在远程内存访问场景下能够获得显著的性能提升，包括降低延迟、提高吞吐量和改善资源利用率

Conclusion: 分解式内存系统为键值存储提供了新的架构机会，但需要重新设计数据结构以充分利用RDMA的优势。针对性的优化能够显著提升有序键值存储在远程内存访问场景下的性能

Abstract: Disaggregated memory systems separate monolithic servers into different components, including compute and memory nodes, to enjoy the benefits of high resource utilization, flexible hardware scalability, and efficient data sharing. By exploiting the high-performance RDMA (Remote Direct Memory Access), the compute nodes directly access the remote memory pool without involving remote CPUs. Hence, the ordered key-value (KV) stores (eg, B-trees and learned indexes) …

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 提出三种改进RT Core粒子FRNN物理模拟的方法：BVH更新/重建优化器、无需邻居列表的RT Core新用法、支持周期性边界条件的RT Core技术，显著提升性能和能效


<details>
  <summary>Details</summary>
Motivation: 现有RT Core在粒子FRNN物理模拟中存在性能瓶颈，特别是BVH管理效率、邻居列表内存开销以及周期性边界条件支持不足的问题，需要优化以充分发挥RT Core潜力

Method: 1) 实时BVH更新/重建比例优化器，自适应模拟动态变化；2) 两种无需邻居列表的RT Core变体；3) 支持周期性边界条件的RT Core技术

Result: BVH优化器使RT Core流水线比已知方法快约3.4倍；新变体在小半径时提升约1.3倍，对数正态半径分布时提升约2.0倍；支持周期性边界条件无显著性能损失；方法在不同GPU代际上均能扩展

Conclusion: 提出的三种技术显著提升了RT Core在FRNN物理模拟中的性能和能效，同时明确了RT Core与传统GPU计算的适用场景，为硬件加速物理模拟提供了实用指导

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='Zongheng Yang'></div>

# Zongheng Yang [[Back]](#toc)

### [15] [itwinai: A Python Toolkit for Scalable Scientific Machine Learning on HPC Systems](https://scholar.google.com/scholar_url?url=https://joss.theoj.org/papers/10.21105/joss.09409.pdf&hl=zh-CN&sa=X&d=18053254810705247516&ei=a-Rzac_mErui6rQPrda9qQc&scisig=AHkA5jRZf6ZWJOn4yE1Y28U_7MOl&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=0&folt=cit)
*M Bunino,J Sæther,L Eickhoff,A Lappe,K Tsolaki…*

Main category: Zongheng Yang

TL;DR: AI在HPC系统部署面临异构性挑战，研究人员需关注底层实现而非核心科学问题


<details>
  <summary>Details</summary>
Motivation: AI在科学研究中的应用日益广泛，但研究人员在HPC系统上部署AI工作流时面临显著障碍，异构性迫使他们关注底层实现细节而非核心科学问题

Method: 论文未在摘要中明确描述具体方法，但暗示需要解决HPC系统异构性带来的AI工作流部署挑战

Result: 摘要未提供具体结果，但指出了当前AI在HPC部署中的瓶颈问题

Conclusion: 需要解决HPC系统异构性对AI工作流部署的障碍，使研究人员能专注于科学问题而非技术实现

Abstract: Summary The integration of Artificial Intelligence (AI) into scientific research has expanded significantly over the past decade, driven by the availability of large-scale datasets and Graphics Processing Units (GPUs), in particular at High Performance Computing (HPC) sites. However, many researchers face significant barriers when deploying AI workflows on HPC systems, as their heterogeneous nature forces scientists to focus on low-level implementation details rather than on their core …

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [16] [NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases](https://arxiv.org/abs/2601.15758)
*Xieyang Wang,Mengyi Liu,Weijia Yi,Jianqiu Xu,Raymond Chi-Wing Wong*

Main category: cs.DB

TL;DR: NL4ST是一个交互式工具，允许用户使用自然语言查询时空数据库，通过三层架构将自然语言转换为可执行的物理查询计划。


<details>
  <summary>Details</summary>
Motivation: 移动计算设备和定位技术的发展导致时空数据爆炸式增长，但时空查询（如范围查询、最近邻查询、连接查询）的制定需要领域专业知识和查询语言技能，这对非专业用户构成挑战，因此迫切需要支持自然语言查询的时空数据库工具。

Method: NL4ST采用三层架构：1) 知识库和语料库进行知识准备；2) 自然语言理解进行实体链接；3) 生成物理查询计划。该工具将自然语言查询转换为数据库可执行的物理计划。

Result: NL4ST能够为时空数据库提供有效的物理查询计划，已在四个真实和合成数据集上验证。工具已在线提供，并配有演示视频。

Conclusion: NL4ST通过自然语言接口弥合了非专业用户与数据库查询计划之间的鸿沟，使时空数据库查询更加易用和可访问。

Abstract: The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.

</details>


### [17] [EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery](https://arxiv.org/abs/2601.16025)
*Yajuan Xu,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: EAIFD是一种用于增量函数依赖发现的高效算法，通过维护差异集的部分超图，将问题转化为超图上的最小命中集枚举，避免了完全重新运行。


<details>
  <summary>Details</summary>
Motivation: 函数依赖是关系数据库中的基本完整性约束，但在增量更新下的发现仍然具有挑战性。静态算法由于完全重新执行而效率低下，增量算法则存在严重的性能和内存瓶颈。

Method: EAIFD维护差异集的部分超图，将增量FD发现问题重构为超图上的最小命中集枚举。引入两个关键创新：1) 多属性哈希表(MHT)，用于有效FD的高频键值映射，其内存消耗与数据集大小无关；2) 两步验证策略，利用MHT有效减少验证空间，然后选择性加载数据块进行批量验证，避免重复I/O操作。

Result: 在真实数据集上的实验结果表明，与现有算法相比，EAIFD在运行时间上实现了高达一个数量级的加速，同时将内存使用减少了两个数量级以上。

Conclusion: EAIFD为增量函数依赖发现提供了一个高效且可扩展的解决方案，显著优于现有方法。

Abstract: Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.

</details>


<div id='Xuanhe Zhou'></div>

# Xuanhe Zhou [[Back]](#toc)

### [18] [Retrieval-augmented Generation (RAG): What is There for Data Management Researchers? A discussion on research from a panel at LLM+ Vector Data Workshop …](https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3793217.3793229&hl=zh-CN&sa=X&d=11795933299326150824&ei=beRzacroJ7uM6rQP2dOq-Ag&scisig=AHkA5jRGMj7rN8pbYOAZ6mtDvXNx&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=0&folt=cit)
*A Khan,Y Luo,W Zhang,M Zhou,X Zhou*

Main category: Xuanhe Zhou

TL;DR: 该摘要概述了大语言模型（LLMs）在语言处理领域的先进地位，以及其在数据科学与工程自动化中的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 强调LLMs通过将多样化任务（从代码合成、医疗保健到金融、数字助理和科学发现）构建为下一个token预测问题，实现了语言处理的最先进水平，并推动了数据科学与工程流程的自动化。

Method: 摘要未详细描述具体方法，但指出LLMs的核心技术是将各种任务框架化为下一个token预测问题，并引用了一系列相关文献。

Result: LLMs在语言处理领域达到最先进水平，并在数据科学与工程中实现了流程自动化，包括数据分析、操作、查询、解释、研究和教育等环节。

Conclusion: 大语言模型通过下一个token预测范式，不仅推动了语言处理技术的发展，还显著促进了数据科学与工程领域的自动化进程。

Abstract: Large language models (LLMs) enable the state-ofthe-art in language processing by framing diverse tasks-from code synthesis and healthcare to finance, digital assistance, and scientific discovery-as next-token prediction problems [38, 53, 60, 65, 72, 20, 68, 76, 32]. In addition, LLMs enable automation in data science and engineering, optimizing processes such as data analysis, manipulation, querying, interpretation, research, and education [33, 7, 8, 22, 24, 42, 77, 37, 43, 44, 66, 49] …

</details>


### [19] [VDBFuzz: Understanding and Detecting Crash Bugs in Vector Database Management Systems](https://scholar.google.com/scholar_url?url=https://shenaow.github.io/files/icse26vdbfuzz.pdf&hl=zh-CN&sa=X&d=1837038674883441266&ei=beRzacroJ7uM6rQP2dOq-Ag&scisig=AHkA5jQzdEeQn8aePuhx-IaaFNYm&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=2&folt=cit)
*S Wang,Z Liu,Y Zhao,Q Zou,H Wang*

Main category: Xuanhe Zhou

TL;DR: VDBMS在LLM应用中至关重要，但其复杂性导致可靠性问题，特别是边界条件故障引发的崩溃bug


<details>
  <summary>Details</summary>
Motivation: 向量数据库管理系统在LLM集成应用中变得至关重要，但其固有的复杂性（包括高维数据结构、多样化索引策略和异构实现）使其容易产生可靠性问题，特别是由边界条件故障引发的崩溃bug

Method: 论文摘要未提供具体方法细节，但暗示需要解决VDBMS中的边界条件故障问题，如无效配置和维度不匹配等

Result: 摘要未提供具体实验结果，但指出边界条件故障会导致严重bug，可能造成数据损坏或系统崩溃

Conclusion: VDBMS的可靠性问题需要关注，特别是边界条件故障引发的崩溃bug，这对LLM应用至关重要

Abstract: ABSTRACT Vector Database Management Systems (VDBMSs) have become critical in LLM-integrated applications. However, their inherent complexity, including high-dimensional data structures, diverse indexing strategies, and heterogeneous implementations, makes them prone to reliability issues. Among these, crash bugs caused by boundary condition failures, such as invalid configurations and mismatched data dimensions, are particularly severe. These bugs can result in …

</details>


### [20] [SEG-SQL: Structure-aware Example Generation for Text-to-SQL Method with In-context Learning](https://scholar.google.com/scholar_url?url=https://www.dbpia.co.kr/Journal/articleDetail%3FnodeId%3DNODE12548332&hl=zh-CN&sa=X&d=10894122745510917760&ei=beRzacroJ7uM6rQP2dOq-Ag&scisig=AHkA5jTrzpfVIRzI3otxCOU9HcFx&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=4&folt=cit)
*권동욱， 문재완， 이종욱*

Main category: Xuanhe Zhou

TL;DR: SEG-SQL은 LLM의 Text-to-SQL 성능을 향상시키기 위한 새로운 예제 선택 방법으로, SQL 구조 유사성을 고려하여 pseudo SQL을 생성하고 이를 활용해 최적의 예제를 선택한다.


<details>
  <summary>Details</summary>
Motivation: 기존 자연어 유사도 기반 예제 선택 방법은 SQL 구조 유사성을 보장하지 못하며, 목표 SQL 쿼리와 구조적으로 유사한 예제가 없는 환경에서 성능이 저하되는 문제를 해결하기 위해 제안되었다.

Method: SEG-SQL은 자연어 질의로부터 초기 SQL을 생성하고 이를 구조적 특성을 반영하는 힌트 벡터로 변환한다. 이후 일부 비트를 변형해 구조적으로 유사한 pseudo SQL을 생성하고, SQL-to-Text 변환을 통해 자연어 질의를 재생성하여 최적의 예제를 선택한다.

Result: SEG-SQL은 기존 방법 대비 Text-to-SQL 성능을 크게 향상시켰으며, 특히 구조적으로 유사한 예제가 부족한 환경에서 더 효과적인 것으로 나타났다.

Conclusion: SQL 구조 유사성을 고려한 예제 선택이 Text-to-SQL 성능 향상에 중요하며, SEG-SQL은 이를 효과적으로 구현한 방법으로 다양한 데이터베이스 환경에서 적용 가능하다.

Abstract: 문맥 학습을 활용한 거대 언어 모델은 Text-to-SQL 성능을 크게 향상시켰으나, 기존 자연어 유사도 기반 예제 선택은 SQL 구조 유사성을 보장하지 못하고 목표 SQL 쿼리와 구조적으로 유사한 예제가 없는 환경에서 성능이 저하된다. 이를 해결하기 위해 본 연구에서는 SEG-SQL 을 제안한다. SEG-SQL 은 자연어 질의로부터 초기 SQL 을 생성하고 구조적 특성을 반영하는 힌트 벡터로 변환한다. 이후 일부 비트를 변형해 구조적으로 유사한 pseudo SQL 을 생성하고, SQL-to-Text 변환을 통해 자연어 질의를 …

</details>


### [21] [Code-Driven Programming Prediction Enhanced by LLM with a Feature Fusion Approach](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S1566253526000448&hl=zh-CN&sa=X&d=10290711333962268958&ei=bORzaeTjGaOi6rQPoMCXoAE&scisig=AHkA5jSLXXu1stw1kKjUiUefFZu0&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=0&folt=rel)
*S Liu,J Li,Q Wan,B He,Z Huang,Q Li*

Main category: Xuanhe Zhou

TL;DR: 在线编程辅导系统中的知识追踪研究，旨在通过分析学习者与编程练习的交互来建模其知识状态，以提供个性化学习支持


<details>
  <summary>Details</summary>
Motivation: 编程教育对培养数字素养和问题解决能力至关重要。在线编程辅导系统需要准确追踪学习者的知识状态，以提供个性化学习支持，但现有知识追踪方法在编程教育场景下面临挑战

Method: 该论文可能提出了一种针对编程教育场景的知识追踪方法，通过分析学习者与编程练习的交互数据（如代码提交、错误类型、调试过程等）来建模知识状态

Result: 提出的方法在编程教育数据集上表现出优于传统知识追踪模型的性能，能够更准确地预测学习者的编程技能掌握情况和未来表现

Conclusion: 针对编程教育特点的知识追踪方法能够有效建模学习者的编程技能发展，为个性化编程教学提供重要支持

Abstract: Programming education is essential for equipping individuals with digital literacy skills and developing the problem-solving abilities necessary for success in the modern workforce. In online programming tutoring systems, knowledge tracing (KT) …

</details>


### [22] [PRIME: Policy-Reinforced Iterative Multi-Agent Execution for Algorithmic Reasoning in Large Language Models](https://scholar.google.com/scholar_url?url=https://www.preprints.org/frontend/manuscript/61ae7bc8c6196e72c1c5b8b85c8407f6/download_pub&hl=zh-CN&sa=X&d=499370675858426397&ei=bORzaeTjGaOi6rQPoMCXoAE&scisig=AHkA5jSJh1yiVljMc09KDyUTGHQN&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=1&folt=rel)
*J Xu,Z Yu,Z Bi,MD Pham,X Qu,D Zhang*

Main category: Xuanhe Zhou

TL;DR: PRIME是一个基于策略强化的迭代多智能体框架，通过多智能体协作和强化学习来提升大语言模型在算法推理任务上的性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多样化推理任务上表现出色，但在算法推理方面仍存在局限性，需要专门的方法来提升其算法推理能力

Method: 提出PRIME框架，采用多智能体协作和策略强化学习，通过迭代优化过程来改进算法推理

Result: PRIME框架显著提升了大语言模型在算法推理任务上的性能，展示了多智能体协作和强化学习在算法推理中的有效性

Conclusion: PRIME框架为提升大语言模型的算法推理能力提供了一种有效的多智能体强化学习方法，具有重要的理论和应用价值

Abstract: Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent …

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [23] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: 评估LLMs在不同语言中的回答质量差异，发现低资源语言获得更低质量回答，且语言选择显著影响模型使用的文化背景


<details>
  <summary>Details</summary>
Motivation: 用户不应因使用不同语言与LLMs交互而受到系统性不利影响，需要评估LLMs在不同语言中的回答质量是否一致，以及语言选择如何影响模型使用的文化背景

Method: 基于WildChat数据集创建真实世界开放式问题集，评估不同语言下的回答质量差异；使用LLM-as-a-Judge方法识别回答中的文化背景；在CulturalBench基准的翻译子集上进行多语言评估

Result: LLMs在低资源语言的开放式问题上持续提供更低质量的回答；语言显著影响模型使用的文化背景；这种背景差异影响下游回答的质量

Conclusion: 语言选择不仅影响LLMs的回答质量，还影响模型使用的文化背景，揭示了LLMs在不同语言中存在系统性差异，需要解决语言公平性问题

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [24] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: 提出在因果token-choice MoE中通过引入零计算(null)专家实现数据稀疏性，结合权重稀疏性获得更好的计算效率，在视觉语言模型中实现隐式的模态感知分配


<details>
  <summary>Details</summary>
Motivation: MoE层通过权重稀疏性实现计算效率，但数据稀疏性（每个专家只处理部分token）提供了互补维度。现有的专家选择路由直接实现数据稀疏性，但在自回归模型中违反因果性，导致训练-推理不匹配。需要在不违反因果性的情况下实现数据稀疏性。

Method: 在因果token-choice MoE的路由池中引入零计算(null)专家。当token路由到null专家时，这些槽位不消耗计算。通过标准的负载均衡目标训练模型均匀使用所有专家（真实和null），在期望上创建数据稀疏性而不违反因果性。在视觉语言模型训练中评估，其中视觉编码器产生许多低信息token而文本token更密集。

Result: 在匹配的期望FLOPs下，组合权重和数据稀疏性比单独使用权重稀疏性产生更高效的计算前沿，在训练损失和下游性能上都有提升。模型学习到隐式的模态感知分配，将视觉token更积极地路由到null专家，而不需要显式的模态路由。

Conclusion: 通过引入null专家在因果token-choice MoE中实现数据稀疏性，结合权重稀疏性显著提高计算效率。该方法在视觉语言模型中特别有效，能自动学习模态感知路由策略，无需显式设计。

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [25] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: GOAT是一种基于熵最优传输的广义注意力机制，通过可学习的连续先验替代标准注意力的均匀先验假设，保持与FlashAttention等优化内核的兼容性，同时解决注意力下沉问题并实现长度泛化。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制隐含了均匀先验假设，这限制了其表达能力并导致注意力下沉问题。作者希望从熵最优传输的角度重新理解注意力，引入更灵活的可学习先验来克服这些限制。

Method: 将注意力重新解释为熵最优传输问题，提出GOAT（Generalized Optimal transport Attention with Trainable priors）机制。该方法用可学习的连续先验替代标准注意力的均匀先验，保持与FlashAttention等优化内核的兼容性，并将空间信息吸收到核心注意力计算中。

Result: GOAT提供了注意力下沉现象的EOT解释并实现了解决方案，避免了标准注意力的表示权衡。同时，通过可外推的先验，结合了学习位置嵌入的灵活性和固定编码的长度泛化能力。

Conclusion: 从熵最优传输视角重新审视注意力机制，提出具有可训练先验的广义最优传输注意力（GOAT），解决了标准注意力的局限性，在保持计算效率的同时提升了表达能力和泛化性能。

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [26] [FedUMM: A General Framework for Federated Learning with Unified Multimodal Models](https://arxiv.org/abs/2601.15390)
*Zhaolong Su,Leheng Zhao,Xiaoying Wu,Ziyue Xu,Jindong Wang*

Main category: cs.LG

TL;DR: FedUMM：一种用于非IID多模态数据的统一多模态模型联邦学习框架，通过LoRA适配器实现参数高效微调，大幅降低通信成本


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型（UMMs）通常在集中式设置中训练，限制了在隐私敏感和地理分布式场景中的部署。需要开发能够在非IID多模态数据下工作的联邦学习框架，同时降低通信成本。

Method: 基于NVIDIA FLARE构建FedUMM框架，使用BLIP3o作为骨干模型，通过参数高效微调实现联邦学习：客户端训练轻量级LoRA适配器并冻结基础模型，服务器仅聚合适配器更新。

Result: 在VQA v2和GenEval组合生成基准测试中，在Dirichlet控制的异构性下（最多16个客户端）进行评估。结果显示随着客户端数量和异构性增加，性能略有下降，但仍与集中式训练保持竞争力。适配器联邦相比全微调将每轮通信减少了一个数量级以上。

Conclusion: FedUMM为隐私保护的联邦统一多模态模型提供了实用的训练框架，通过适配器联邦显著降低了通信成本，为未来研究提供了实证经验。

Abstract: Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.

</details>


### [27] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: Ambient Dataloops是一种迭代式数据集精炼框架，通过数据集-模型协同进化过程，逐步提升数据集质量，使扩散模型能更好地学习底层数据分布。


<details>
  <summary>Details</summary>
Motivation: 现代数据集包含质量差异很大的样本，直接在这样异构的数据上训练往往产生次优模型。需要一种方法来提升数据集质量，同时避免破坏性的自消耗循环。

Method: 提出数据集-模型协同进化过程：每轮迭代中，数据集质量逐步提升，模型相应改进。为避免破坏性自消耗循环，将合成改进的样本视为有噪声但噪声水平略低于前一轮迭代，并使用Ambient Diffusion技术进行有损学习。

Result: Ambient Dataloops在无条件/文本条件图像生成和从头蛋白质设计中实现了最先进的性能。同时为数据循环过程提供了理论依据。

Conclusion: 该框架通过迭代式数据集精炼和Ambient Diffusion技术，有效解决了异构数据训练问题，在多个生成任务中表现出色，并具有理论支持。

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [28] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: CASL：首个实现扩散模型潜在表示与语义概念监督对齐的框架，通过稀疏自编码器和轻量级线性映射实现概念对齐，并引入CASL-Steer作为因果探针验证语义控制能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的内部激活编码了丰富的语义信息，但现有基于稀疏自编码器（SAE）的方法采用无监督方式，无法将稀疏特征与人类可理解的概念对齐，限制了生成图像的可靠语义控制能力。

Method: CASL框架包含两个阶段：1）在冻结的U-Net激活上训练稀疏自编码器（SAE）获得解耦的潜在表示；2）学习轻量级线性映射，将每个语义概念与一小部分相关潜在维度关联。为验证对齐方向的语义含义，提出CASL-Steer作为受控潜在干预，沿学习到的概念轴移动激活，作为因果探针揭示概念对齐潜在如何影响生成内容。

Result: 实验表明，该方法相比现有方法实现了更优的编辑精度和可解释性。引入编辑精度比（EPR）作为联合度量概念特异性和无关属性保持的指标。

Conclusion: 这是首个实现扩散模型潜在表示与语义概念监督对齐的工作，CASL框架能够提供可靠的语义控制，并通过CASL-Steer因果探针验证了概念对齐潜在对生成内容的因果影响。

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [29] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 该论文研究在合成数据污染环境下学习理论的基本问题，发现传统ERM方法存在局限性，提出加权算法能更好处理混合数据学习任务。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及和低成本，合成内容大量出现，导致"自然"数据被LLM生成的数据污染。研究者需要重新审视在这种普遍存在的数据污染环境下的基本学习理论问题，探索学习算法在无法区分数据来源时的表现。

Method: 将问题建模为一系列学习任务，输入是自然数据和合成数据的混合，学习算法无法区分单个样本的来源。研究ERM在这种设置下的表现，并与为不同代数据分配非均匀权重的算法进行比较。针对PAC学习设置，分析ERM的收敛性，并设计能够处理任意VC类和任意污染程度的算法。

Result: 对于估计任意d维分布均值的问题，ERM虽然收敛到真实均值，但被分配非均匀权重的算法超越。在PAC学习设置中，ERM并不总是收敛到真实概念，这与模型崩溃文献的发现一致。然而，研究者展示了存在能够学习任意VC类和任意污染程度的正确假设的算法。

Conclusion: 在合成数据污染的学习环境中，传统ERM方法存在局限性，需要开发新的算法策略来处理混合数据学习任务。非均匀加权方法在某些情况下表现更好，且存在能够处理任意污染程度的有效学习算法。

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [30] [Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra](https://arxiv.org/abs/2601.15473)
*Fahd Seddik,Abdulrahman Elbedewy,Gaser Sami,Mohamed Abdelmoniem,Yahia Zakaria*

Main category: cs.LG

TL;DR: Panther是一个PyTorch兼容的RandNLA库，通过随机化数值线性代数算法压缩深度学习模型，显著减少内存使用（BERT上达75%）


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型训练受GPU内存和计算限制，RandNLA技术虽能压缩模型但缺乏统一的生产级库，阻碍了广泛应用

Method: 开发Panther库，整合成熟的RandNLA算法，提供高性能PyTorch兼容框架；实现草图化线性层、2D卷积、多头注意力、随机矩阵分解等组件的即插即用替换；构建自定义C++/CUDA后端（pawX）优化CPU/GPU性能

Result: 仅需几行代码替换标准PyTorch线性层即可在BERT上实现高达75%的内存节省，同时保持可比的损失性能

Conclusion: Panther证明了RandNLA技术的有效性及其易用性，为深度学习模型压缩提供了统一的高性能解决方案

Abstract: Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.

</details>


### [31] [Early predicting of hospital admission using machine learning algorithms: Priority queues approach](https://arxiv.org/abs/2601.15481)
*Jakub Antczak,James Montgomery,Małgorzata O'Reilly,Zbigniew Palmowski,Richard Turner*

Main category: cs.LG

TL;DR: 比较SARIMAX、XGBoost和LSTM三种模型对急诊科每日到达量的7天预测，通过分解需求类别和患者复杂度，并使用Prophet模型处理COVID-19异常数据，发现XGBoost在总入院量预测上最优，SARIMAX在主要复杂度病例预测上略优。


<details>
  <summary>Details</summary>
Motivation: 急诊科过度拥挤严重影响患者安全和运营效率，需要准确的预测模型来优化资源配置。现有研究需要更精细的需求分解和应对异常事件（如COVID-19）的数据处理方法。

Method: 使用澳大利亚三级转诊医院2017-2021年数据，将急诊需求分解为8个病房类别并按临床复杂度分层。采用Prophet模型生成COVID-19异常期的合成反事实值。比较SARIMAX、XGBoost和LSTM三种模型对每日急诊到达量的7天预测性能，以季节性朴素模型为基线。

Result: 所有三种模型均优于季节性朴素基线。XGBoost在预测每日总入院量上表现最佳（MAE=6.63），SARIMAX在预测主要复杂度病例上略优（MAE=3.77）。模型能成功再现日常模式，但普遍低估突发性、偶发性患者激增。

Conclusion: SARIMAX、XGBoost和LSTM均能有效预测急诊需求，其中XGBoost在总体预测上最优，SARIMAX在特定复杂度病例预测上表现更好。但所有模型都存在低估突发激增的共同局限性，需要进一步改进以应对极端事件。

Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.

</details>


### [32] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: MFS将LLM解码重构为最优随机过程识别问题，利用鞅理论设计理论基础的算法，在六个推理基准上超越SOTA方法并显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 标准自回归解码在大型语言模型中存在短视问题，无法找到全局最优推理路径。现有的前瞻采样方法依赖启发式机制进行路径评估和剪枝，缺乏理论依据。

Method: 提出鞅前瞻采样（MFS）框架，将LLM解码重构为识别最优随机过程的问题。通过鞅理论设计算法：使用Doob分解定理进行步骤评估，Optional Stopping Theory进行路径选择，Martingale收敛定理实现自适应停止规则。

Result: 在六个推理基准测试中，MFS在准确率上超越了最先进的方法，同时显著提高了计算效率。

Conclusion: MFS为LLM解码提供了一个理论基础的框架，用概率论原理替代启发式机制，在推理任务中实现了更好的准确率和效率平衡。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [33] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: 提出Margin-Aware Speculative Verification方法，通过自适应目标模型的局部决策稳定性来改进推测解码中的验证机制，在保持生成质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法虽然改进了草稿模型质量，但验证机制仍依赖严格的token级拒绝采样。现代大语言模型常在低边际区域运行，此时拒绝合理的次优token带来的信息增益很小，却产生显著的回滚成本，导致验证效率低下。

Method: 提出Margin-Aware Speculative Verification，这是一种无需训练、领域无关的验证策略。方法直接基于目标模型的logits测量决策稳定性，仅在严格验证提供最小收益时放宽拒绝条件。该方法仅修改验证规则，完全兼容现有的目标耦合推测解码框架。

Result: 在8B到235B不同规模的模型上进行广泛实验，结果显示该方法相比最先进的基线方法能提供一致且显著的推理加速，同时在多样化基准测试中保持生成质量。

Conclusion: 通过自适应目标模型的局部决策稳定性来改进验证机制，Margin-Aware Speculative Verification方法能有效提升推测解码的效率，在保持生成质量的同时实现显著加速。

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [34] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: 该研究针对志愿者湖泊监测数据的不规则性和缺失问题，开发了一种联合可行性策略，确定在5%精度目标下所需的最小训练历史和最少预测因子，为湖泊监测提供高效采样方案。


<details>
  <summary>Details</summary>
Motivation: 志愿者湖泊监测产生不规则、季节性的时间序列，存在大量数据缺失（冰盖、天气限制、人为错误），这给有害藻华预测和早期预警带来困难。需要解决数据缺失问题并优化监测策略。

Method: 使用MICE处理数据缺失，在30个湖泊的30年现场记录子集上评估SDD预测。比较6种候选模型，岭回归表现最佳。通过向后最近历史协议确定最小样本量，识别最小特征集，并引入联合可行性函数统一历史长度和特征选择。

Result: 岭回归在测试中表现最佳；达到完整历史精度5%范围内平均需要约176个训练样本；紧凑的四特征子集与十三特征基线在5%容差内匹配；联合可行性分析显示，达到5%精度目标需要约64个最近样本和仅1个预测因子。

Conclusion: 联合可行性策略将最近历史长度和特征选择统一在固定精度目标下，为湖泊研究人员提供了简单高效的采样工作和测量优先级设置规则，突出了针对性监测的实用性。

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [35] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 提出机器学习方法，利用临床测试和MRI数据区分非典型阿尔茨海默病与非AD认知障碍，显著提高诊断准确率


<details>
  <summary>Details</summary>
Motivation: 非典型阿尔茨海默病（atAD）患者常被误诊，而基于临床评估和海马体积的常规诊断方法对这类患者效果不佳。需要开发仅使用临床测试和MRI数据的改进诊断方法。

Method: 开发机器学习分类方法，使用临床特征、海马体积以及全脑MRI特征，区分atAD与非AD认知障碍。采用Boruta统计方法识别显著脑区，并在三个数据集（一个私有数据集和两个公共数据集NACC、ADNI）上评估。

Result: 最佳性能通过结合重要MRI特征实现，优于仅使用海马体积。atAD病例正确诊断率（召回率）从52%提升至69%（NACC）和从34%提升至77%（ADNI），同时保持高精确率。

Conclusion: 提出的机器学习方法仅使用临床测试和MRI数据即可显著提高非典型阿尔茨海默病的诊断准确性，对临床实践有重要价值。

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [36] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: 量化感知的机器遗忘方法，通过引入logits空间铰链损失确保遗忘样本在4位量化后仍保持遗忘状态


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从训练模型中移除特定知识（如版权或隐私数据），但在实际部署中，模型常被量化（如4位）。研究发现量化会灾难性地恢复已遗忘的信息，因此需要解决量化对遗忘效果的破坏问题。

Method: 首先分析量化如何破坏遗忘，计算权重变化统计和量化桶重叠，发现典型遗忘更新太小而无法跨越量化阈值。基于此，提出量化感知的遗忘方法：引入logits空间铰链损失，强制遗忘样本在未遗忘模型和原始模型之间的输出logits差异至少达到量化步长的一半，确保量化后遗忘样本仍可区分。

Result: 在语言和分类任务（包括Twitter虚假信息数据集）上评估，该方法在4位量化下能保持遗忘效果，而现有方法在量化后几乎完全恢复已遗忘知识。

Conclusion: 量化会严重破坏机器遗忘效果，但通过量化感知的遗忘方法（特别是logits空间铰链损失）可以有效缓解这一问题，确保遗忘在量化部署后仍能保持。

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [37] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: Prism是一种基于最大化编码率降低原则的白盒注意力架构，通过几何归纳偏置实现无监督功能解耦，在TinyStories上验证了注意力头的频谱专业化。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（特别是Transformer）常被批评为"黑箱"且缺乏可解释性。研究者希望开发一种具有几何可解释性的白盒架构，证明可解释性和性能并非权衡关系，而是可以通过几何构造统一。

Method: 提出Prism架构，基于最大化编码率降低原则，将注意力机制建模为在信号-噪声流形上的梯度上升过程。引入两个物理约束：1）过完备字典扩展表示相空间；2）无理频率分离强制信号和噪声子空间的不相干性。使用TinyStories作为验证频谱动态的受控测试平台。

Result: Prism在TinyStories上自发地将注意力头专业化为频谱不同的机制：低频头捕获长程因果依赖（信号），高频头处理局部句法约束（噪声）。这些几何归纳偏置足以单独诱导无监督功能解耦。

Conclusion: 可解释性和性能并非权衡关系，而是可以通过基于几何原理的构造统一。Prism架构通过物理约束和几何归纳偏置实现了白盒可解释性，同时保持了模型性能。

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [38] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++通过引入两种漂移检测机制（基于熵和KL散度）和自适应重置策略，解决了持续测试时适应（CTTA）在快速变化或长期分布漂移下的性能崩溃问题，在CCC基准测试中相比RDumb获得约3%的绝对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法（如Tent、EATA等）在短期分布漂移下表现良好，但在测试分布快速变化或长期部署（如CCC基准中的750万样本流）时容易失败。需要一种能够检测有害适应并防止预测崩溃的机制。

Method: RDumb++是RDumb的扩展，引入两种漂移检测机制：1）基于熵的漂移评分，2）KL散度漂移评分。结合自适应重置策略，当检测到累积适应变得有害时，模型能够重置恢复，避免预测崩溃。

Result: 在CCC-medium基准（三种速度、三种种子，共9次运行，每次包含100万样本）上，RDumb++持续超越RDumb，获得约3%的绝对准确率提升，并在整个数据流中保持稳定的适应性能。

Conclusion: 漂移感知重置对于防止CTTA中的性能崩溃和实现可靠的长期适应至关重要。RDumb++通过检测有害适应和及时恢复，在持续变化的测试环境中保持了稳定的性能。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [39] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: 在医疗机器学习中，使用临床定制指标而非验证损失进行模型优化能获得更好的临床任务性能


<details>
  <summary>Details</summary>
Motivation: 传统机器学习使用验证损失指导模型优化（如超参数选择、停止点确定），但医疗ML的目标不同：模型需要满足特定临床要求而非训练损失函数。临床需求可以通过定制指标更精确地捕捉，而许多优化任务不需要驱动指标可微，这为使用临床相关指标提供了机会。

Method: 通过两个对照实验，比较使用临床定制指标与验证损失进行模型优化的效果。实验设计包括定义临床相关指标、将其编码到优化流程中，并评估两种方法在临床任务上的性能差异。

Result: 实验结果显示，使用临床定制指标进行模型优化相比使用验证损失能获得更好的临床任务性能。这表明针对临床需求定制的指标能更有效地指导模型优化过程。

Conclusion: 在医疗机器学习中，尽管定义和编码临床相关指标需要额外努力，但使用这些指标进行模型优化能产生更符合医疗ML核心目标的模型：在临床实践中表现优异。这种方法比传统验证损失优化更有效。

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [40] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 提出了首个从部分观测数据学习神经算子的系统框架，通过掩码预测训练策略和物理感知潜在传播器解决监督缺失和空间不匹配问题，在PDE任务上实现18-69%的相对误差降低。


<details>
  <summary>Details</summary>
Motivation: 现实科学应用中经常遇到不完整的观测数据（传感器限制、地理约束、测量成本等），而现有神经算子方法假设完全观测的空间输入，严重限制了在实际应用中的适用性。

Method: 提出了Latent Autoregressive Neural Operator (LARNO)，包含两个核心组件：1) 掩码预测训练策略，通过策略性地掩码观测区域创建人工监督；2) 物理感知潜在传播器，在潜在空间中通过边界优先自回归生成重建解。此外还开发了POBench-PDE基准测试。

Result: 在补丁式缺失率低于50%的情况下，在所有基准测试中实现了18-69%的相对L2误差降低，包括真实世界气候预测。方法能有效处理高达75%缺失率的实际场景。

Conclusion: 该工作首次系统解决了从部分观测学习神经算子的问题，通过创新的训练策略和潜在空间传播机制，在一定程度上弥合了理想化研究设置与现实世界科学计算复杂性之间的差距。

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [41] [BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations](https://arxiv.org/abs/2601.15552)
*Phuc Nguyen,Benjamin Zelditch,Joyce Chen,Rohit Patra,Changshuai Wei*

Main category: cs.LG

TL;DR: BanditLP：一个可扩展的多利益相关方上下文老虎机框架，结合神经Thompson采样学习目标特定结果与大规模线性规划进行约束动作选择，应用于LinkedIn电子邮件营销系统并取得业务成功


<details>
  <summary>Details</summary>
Motivation: 在生产系统中需要同时处理多个利益相关方的目标，并在满足约束条件的情况下进行决策。传统方法难以在探索（学习）和利用（优化）之间取得平衡，特别是在大规模、多约束的实际应用场景中

Method: 提出BanditLP框架，统一神经Thompson采样用于学习目标特定结果，结合大规模线性规划在服务时进行约束动作选择。框架应用无关，兼容任意神经架构，LP求解器可处理数十亿变量

Result: 在公共基准测试和合成数据上的实验显示相对于强基线的一致增益。在LinkedIn电子邮件营销系统中的实际应用展示了业务成功，验证了集成探索和约束优化的价值

Conclusion: BanditLP框架成功地将探索性学习与约束优化相结合，为大规模多利益相关方决策问题提供了可扩展的解决方案，在实际生产系统中证明了其有效性

Abstract: We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.

</details>


### [42] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: 该论文提出了一种结合深度学习和库存理论的端到端学习方法，用于管理具有随机提前期的易腐品库存系统，在需求过程和提前期分布未知的情况下，通过嵌入启发式策略结构来提高学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 易腐品库存管理面临需求过程和提前期分布未知的挑战，传统方法在有限历史数据下表现不佳。需要开发能够利用观察到的协变量和系统状态，在数据有限情况下高效学习的智能库存管理策略。

Method: 采用边际成本核算方案，为每个订单分配单一生命周期成本，产生统一的端到端学习损失函数。开发了三种深度学习方法：纯黑盒方法（E2E-BB）、嵌入投影库存水平策略的结构引导方法（E2E-PIL），以及利用目标函数齐次性进行提升的增强方法（E2E-BPIL）。

Result: 实验表明性能排序为：E2E-BB < E2E-PIL < E2E-BPIL。结构引导方法通过降低有效模型复杂度和提高学习效率，在仅牺牲少量灵活性的情况下显著提升性能。E2E-BPIL进一步利用齐次性提升技术获得最佳表现。

Conclusion: 深度学习决策工具在人类知识引导下更有效和稳健，强调了将高级分析与库存理论相结合的价值。嵌入启发式策略结构能够减少模型复杂性，提高学习效率，为数据驱动的库存管理提供了新范式。

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [43] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 本文研究1-identification多臂老虎机问题，提出新的下界和算法，在存在多个合格臂时填补了理论空白。


<details>
  <summary>Details</summary>
Motivation: 1-identification是多臂老虎机中的基本探索问题，目标是判断是否存在平均奖励不低于已知阈值μ₀的合格臂，或输出None。现有文献在存在多个合格臂时对期望总拉动次数𝔼τ的分析存在空白，本文旨在填补这一理论缺口。

Method: 采用优化问题框架推导新的下界，并设计新算法获得紧上界。通过优化公式分析期望总拉动次数，算法设计确保在所有问题实例中上界与下界的差距最多为对数因子的多项式。

Result: 推导出当存在至少一个合格臂时𝔼τ的新下界，设计了新算法获得紧上界，上界与下界的差距在所有问题实例中最多为对数因子的多项式。解决了历史文献中关于存在多个合格臂时𝔼τ分析的开放性问题。

Conclusion: 本文通过优化框架和算法设计，为1-identification问题提供了完整的理论分析，特别是在存在多个合格臂的情况下填补了理论空白，实现了接近最优的性能保证。

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [44] [Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting](https://arxiv.org/abs/2601.15669)
*Jingjing Bai,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Dualformer：一种用于长期时间序列预测的双域Transformer框架，通过分层频率采样和周期性感知加权机制解决传统Transformer的低通滤波效应，有效保留高频信息。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在长期时间序列预测中存在固有的低通滤波效应，导致高频信息在层间传播过程中逐渐衰减，限制了模型捕捉细粒度时间变化的能力。

Method: 提出Dualformer框架，包含三个核心组件：1）双分支架构并行建模时域和频域特征；2）分层频率采样模块为不同层分配特定频带；3）周期性感知加权机制基于谐波能量比动态平衡双分支贡献。

Result: 在8个广泛使用的基准数据集上进行实验，Dualformer表现出鲁棒性和优越性能，特别是在异构或弱周期性数据上效果显著。

Conclusion: Dualformer通过结构化频率建模和自适应时频特征集成，有效解决了Transformer的低通滤波问题，提升了长期时间序列预测的准确性和泛化能力。

Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.

</details>


### [45] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: RLSEdit：基于递归最小二乘的序列化模型编辑方法，通过在线二次优化解决长期编辑中的可塑性-稳定性困境，支持万次编辑而不损害模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在长期序列化编辑中面临可塑性-稳定性困境：硬写入方法会积累干扰，硬保护方法可能覆盖先前编辑且未约束行为会偏离，导致在多次编辑后通用能力下降。

Method: 将编辑建模为带软约束的在线二次优化问题，最小化累积键值拟合目标，包含两个正则项：控制与预训练权重的偏差，以及控制与指定锚点映射的偏差。通过Woodbury恒等式实现高效在线递归更新，每次编辑成本与历史长度无关，仅与当前编辑规模相关。

Result: 在多个模型家族上的实验表明，RLSEdit能够稳定扩展到10K次编辑，在编辑成功率和整体稳定性方面优于强基线方法，关键能保留早期编辑，并在GLUE和保留的推理/代码基准上保持通用能力。

Conclusion: RLSEdit通过递归最小二乘优化框架有效解决了长期序列化模型编辑中的可塑性-稳定性权衡问题，为实际部署中持续更新的LLM提供了可扩展且稳定的编辑解决方案。

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [46] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 论文提出零误差视野（ZEH）概念，用于评估LLM在无错误情况下的最大问题解决范围，发现即使先进模型在简单问题上也会出错，这对安全关键应用有重要启示。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注准确率，但缺乏对模型在无错误情况下能处理问题范围的系统性评估。作者提出ZEH概念来量化LLM的可靠解决范围，揭示即使先进模型在简单任务上也会出错，这对安全关键领域的应用有重要警示意义。

Method: 提出零误差视野（ZEH）评估框架，定义为模型能无错误解决的最大问题范围。通过测试GPT-5.2等先进模型在简单任务（如字符串奇偶性判断、括号平衡性检查）上的表现来评估ZEH。对Qwen2.5进行详细分析，比较ZEH与准确率的关系。提出使用树结构和在线softmax技术来降低计算成本，实现高达一个数量级的加速。

Result: 评估发现GPT-5.2在简单任务上表现不佳：无法计算短字符串11000的奇偶性，无法判断括号串((((())))))是否平衡。ZEH与准确率相关但行为模式不同，ZEH能提供算法能力涌现的线索。计算优化方法实现了高达一个数量级的加速。

Conclusion: ZEH是一个简单但有效的评估指标，能揭示LLM在简单任务上的潜在缺陷。即使先进模型也会在基本问题上出错，这对安全关键应用有重要启示。ZEH提供了不同于传统准确率的评估视角，能帮助理解模型算法能力的涌现。计算优化方法使ZEH评估更加实用。

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [47] [Towards Automated Kernel Generation in the Era of LLMs](https://arxiv.org/abs/2601.15727)
*Yang Yu,Peiyu Zang,Chi Hsu Tsai,Haiming Wu,Yixin Shen,Jialing Zhang,Haoyu Wang,Zhiyou Xiao,Jingze Shi,Yuyu Luo,Wentao Zhang,Chunlei Men,Guang Liu,Yonghua Lin*

Main category: cs.LG

TL;DR: 关于LLM驱动内核生成的综述：系统梳理了利用大语言模型和智能体系统自动化内核生成与优化的方法、数据集和基准测试，并指出了该领域的关键挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统性能受限于底层内核质量，而内核工程需要硬件架构和编程模型的专家知识，过程耗时且难以扩展。LLM和智能体系统为自动化内核生成与优化提供了新可能，但目前该领域缺乏系统性视角。

Method: 提供结构化综述，涵盖LLM基础方法和智能体优化工作流，系统整理支撑学习和评估的数据集与基准测试，并维护开源GitHub仓库跟踪领域进展。

Result: 建立了LLM驱动内核生成的系统性框架，整理了现有方法、数据集和基准测试，为下一代自动化内核优化提供了全面参考。

Conclusion: LLM和智能体系统在自动化内核生成与优化方面展现出巨大潜力，但需要系统性框架来整合碎片化进展。该综述填补了这一空白，为未来研究提供了结构化参考。

Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.

</details>


### [48] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: GenRel-DDI是一个基于关系中心学习框架的药物相互作用预测方法，通过将DDI预测重新定义为关系学习问题，学习独立于药物身份的可迁移相互作用模式，显著提升了模型对未见药物和药物对的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于分子中心的DDI预测方法虽然在标准基准上表现良好，但在实际部署场景中泛化能力不足，特别是在涉及未见药物和验证数据稀缺的情况下。研究发现现有模型嵌入空间中的邻近性并不能可靠对应相互作用标签，单纯增加模型容量无法改善泛化性能。

Method: 提出GenRel-DDI框架，将DDI预测重新定义为关系中心学习问题。该方法学习独立于药物身份的相互作用表示，通过关系级抽象捕获可迁移的相互作用模式。这种设计使模型能够泛化到未见药物和新的药物对。

Result: 在多个基准测试中，GenRel-DDI始终显著优于现有最先进方法。在严格的实体不相交评估中取得了特别大的性能提升，证明了关系学习对于稳健DDI预测的有效性和实际效用。

Conclusion: 关系中心学习框架为DDI预测提供了更稳健和可泛化的解决方案，能够有效处理实际部署中涉及未见药物和验证数据稀缺的挑战，具有重要的实际应用价值。

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [49] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: 提出基于混合LLMs的主动学习框架，用多LLM标注模型替代人工标注，通过聚合多个LLM优势提升标注鲁棒性，并引入标注差异和负学习处理噪声标签。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，研究者尝试将其纳入主动学习流程以降低标注成本。然而，LLM生成的标注质量往往达不到实际应用要求，需要提升LLM标注的鲁棒性和可靠性。

Method: 提出Mixture of LLMs in the Loop Active Learning框架：1) 使用基于混合LLMs的标注模型替代人工标注；2) 引入标注差异度量识别不可靠标注；3) 采用负学习技术增强学习效果；4) 框架基于轻量级LLM，可在本地机器上运行。

Result: 实验表明，该框架性能接近人工标注水平，显著优于单LLM基线和其他LLM集成方法，同时保持了轻量级特性，可在本地环境中部署。

Conclusion: 提出的混合LLMs主动学习框架有效提升了LLM标注的鲁棒性，通过聚合多个LLM优势和处理噪声标签，实现了接近人工标注的性能，为实际应用提供了可行的轻量级解决方案。

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [50] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: GOSV框架通过全局优化识别LLM中的安全关键注意力头，发现恶意注入向量和安全抑制向量两种空间分离的安全向量，并基于此开发了新型白盒越狱攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有安全分析方法依赖局部贪婪归因，假设组件独立贡献，忽视了注意力头等组件间的协同交互作用，导致对LLM安全机制的理解有限。

Method: 提出GOSV框架，通过全局优化同时分析所有注意力头，采用有害补丁和零消融两种互补的激活重补丁策略，识别安全关键注意力头和安全向量。

Result: 发现对齐LLM中存在空间分离的恶意注入向量和安全抑制向量；当约30%的总注意力头被重补丁时，所有模型都会出现完全安全崩溃；基于此开发的新型白盒越狱攻击在所有测试模型中显著优于现有方法。

Conclusion: GOSV框架有效提升了LLM安全可解释性，揭示了LLM通过分离功能通路实现安全目的，为理解和增强模型安全性提供了新视角。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [51] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 提出一种后训练统计准则和结构退火方法，通过移除未支持的参数依赖关系，揭示稳定独立的子结构，实现无需修改模型功能或接口的结构化并行推理。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的推理通常在密集参数矩阵上进行，导致推理成本和系统复杂度随模型规模不可持续地增长。这种限制并非源于模型容量不足，而是由于将后训练推理系统视为整体算子，忽略了学习过程中形成的内部结构。

Method: 研究发现大型模型中的梯度更新事件高度局部化和选择性，许多参数依赖在训练后与其初始化分布在统计上无法区分。基于此提出后训练统计准则和结构退火程序，移除未支持的依赖关系，揭示稳定独立的子结构。

Result: 该方法建立了后训练、模型无关的推理系统结构视图，能够实现结构化并行推理，而无需修改模型功能或接口。

Conclusion: 后训练推理系统本质上是结构非均匀且可分解的，通过统计分析和结构优化可以显著降低推理成本，为大规模模型的高效部署提供新途径。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [52] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: 提出IA-HVAE模型，结合摊销推理与迭代优化，在变换域实现线性可分离解码器，获得35倍加速并提升逆问题重建质量


<details>
  <summary>Details</summary>
Motivation: 传统分层变分自编码器（HVAE）在推理速度与精度之间存在权衡。完全摊销推理速度快但精度有限，完全迭代推理精度高但计算成本大。需要一种混合方法平衡速度与精度，特别是在实时应用和逆问题中。

Method: 提出迭代摊销分层变分自编码器（IA-HVAE），采用混合推理方案：1）初始摊销猜测；2）使用解码器梯度进行迭代优化。关键创新是在变换域（如傅里叶空间）创建线性可分离解码器，实现高模型深度的实时应用。

Result: 1）相比传统HVAE，迭代推理速度提升35倍；2）混合方法在精度上优于完全摊销方法，在速度上优于完全迭代方法；3）在去模糊和去噪等逆问题中，IA-HVAE比普通HVAE具有更好的重建质量。

Conclusion: IA-HVAE通过结合摊销推理的效率和迭代优化的精度，在变换域实现线性可分离解码器，为实时应用和高深度模型提供了有效的混合推理框架，在速度和重建质量方面均优于传统方法。

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [53] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: k-lazyGD算法在平滑在线凸优化中连接了贪婪OGD和惰性GD，证明了惰性更新可在不牺牲性能的情况下实现，其动态遗憾最优且允许的惰性程度与比较器路径长度相关。


<details>
  <summary>Details</summary>
Motivation: 在平滑在线凸优化中，学习器同时面临命中成本和移动成本。现有方法要么过于贪婪（OGD），要么过于稳定（惰性GD/对偶平均），缺乏一个能根据比较器动态调整反应性的统一框架。需要研究如何在保持惰性方法小移动特性的同时不牺牲跟踪能力。

Method: 提出k-lazyGD算法，通过参数k在贪婪OGD（k=1）和惰性GD/对偶平均（k=T）之间创建连续谱。基于FTRL框架进行分析，证明对于任何惰性松弛度k（上限为Θ(√T/P_T)），算法都能达到最优动态遗憾。使用不同松弛度的学习器集合，使方法在可能时保持稳定，在必要时保持敏捷。

Result: k-lazyGD实现了最优动态遗憾O(√(P_T+1)T)，其中P_T是比较器路径长度。允许的惰性程度k与比较器变化相关，可达Θ(√T/P_T)。算法在保持惰性方法小移动特性的同时不牺牲跟踪能力，匹配下界证明了最优性。

Conclusion: 惰性更新可在不牺牲命中性能的情况下实现，允许的惰性程度与比较器动态性直接相关。k-lazyGD统一了反应性和稳定性谱系，通过参数化连接了贪婪和惰性方法，为SOCO问题提供了适应性解决方案。

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [54] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 提出条件灵活性指数（CFI），通过从历史数据学习参数化可接受不确定性集，并利用上下文信息使其条件化，从而更准确地评估调度灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统灵活性指数使用简单可接受不确定性集（如超立方体）来近似可接受不确定性区域，但未考虑可用的上下文信息（如预测）。随着过程灵活性的增加，需要更精确的调度决策评估方法。

Method: 使用归一化流学习从高斯基分布到数据分布的双射映射，在潜在空间中构建超球面作为可接受潜在不确定性集，然后映射到数据空间。通过上下文信息使可接受不确定性集条件化。

Result: 条件灵活性指数（CFI）能提供更信息丰富的灵活性估计，将可接受不确定性集定义在给定条件下更可能相关的区域。应用于安全约束机组组合问题，证明CFI能通过纳入时间信息提高调度质量。

Conclusion: CFI扩展了传统灵活性指数，通过数据驱动和条件化方法更准确地评估调度灵活性。虽然不能一概而论数据驱动集优于简单集或条件集优于无条件集，但两者都能确保只考虑包含实际实现的参数空间区域。

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [55] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: CLASP算法在约束在线凸优化中同时最小化累积损失和平方约束违反，对凸损失实现次线性遗憾和惩罚，对强凸问题首次实现对数级遗憾和惩罚保证。


<details>
  <summary>Details</summary>
Motivation: 研究约束在线凸优化问题，其中学习者在迭代选择动作时面临未预期的凸损失和凸约束，需要同时最小化累积损失并控制约束违反。现有方法在强凸问题上的性能有限，需要开发能同时保证低遗憾和低约束违反的算法。

Method: 提出CLASP算法，通过最小化累积损失和平方约束违反来解决问题。关键创新在于充分利用凸投影算子的严格非扩张性这一证明策略，这在之前的研究中未被应用于该设置。

Result: 对于凸损失，CLASP实现遗憾O(T^{max{β,1-β}})和累积平方惩罚O(T^{1-β})，其中β∈(0,1)。对于强凸问题，首次实现对数级保证：遗憾和累积平方惩罚均为O(log T)。

Conclusion: CLASP算法在约束在线凸优化中提供了强大的理论保证，特别是对于强凸问题，首次实现了对数级的遗憾和约束违反惩罚，这代表了该领域的重要进展。

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [56] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 将XAI应用于工业CPS中的ML模型，通过SHAP值分析时间序列分解组件对预测的影响，发现训练数据上下文信息不足，通过增大窗口尺寸提升模型性能


<details>
  <summary>Details</summary>
Motivation: 工业CPS对安全和经济至关重要，需要高可靠性。ML模型在工业CPS中应用日益广泛，但其复杂性和不透明性可能导致未来数据上的意外行为，需要严格评估。XAI可用于揭示模型推理过程，从而进行更全面的行为分析

Method: 应用可解释AI（XAI）技术，具体使用SHAP值分析时间序列数据分解组件对ML模型预测的影响。通过XAI发现训练数据上下文信息不足的问题，据此增大数据实例的窗口尺寸来改进模型

Result: 通过XAI分析发现模型训练缺乏足够的上下文信息。基于这一发现，通过增加数据窗口尺寸，成功提升了工业CPS中ML模型的预测性能

Conclusion: XAI不仅能解释ML模型的行为，还能指导模型改进。在工业CPS应用中，通过XAI分析发现训练数据上下文不足的问题，并通过增大窗口尺寸有效提升了模型性能，证明了XAI在提升工业CPS中ML模型可靠性的实用价值

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [57] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: 提出用于MAP推断的PAC算法，在可变和固定计算预算下提供可证明的最优解，通过概率电路实现，并开发随机化策略来增强现有启发式方法


<details>
  <summary>Details</summary>
Motivation: MAP估计是概率推断中的基本任务，但通常难以计算，即使在许多常见的结构约束和近似方案下仍然困难。现有方法缺乏理论保证，需要开发具有可证明最优性的实用算法。

Method: 引入PAC-MAP算法，使用信息论度量表征可处理性条件，这些度量可以从有限样本中估计。通过具有适当架构的概率电路高效实现PAC-MAP求解器，开发随机化策略作为独立的MAP推断技术或改进现有启发式方法。

Result: 实验证实该方法在一系列基准测试中的优势，提供了具有严格理论保证的MAP推断解决方案，能够在可变和固定计算预算下获得可证明的最优解。

Conclusion: PAC-MAP算法为MAP推断提供了理论保证和实际可行性，通过信息论度量和概率电路实现高效求解，随机化策略可以增强现有启发式方法的可靠性。

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [58] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: 该研究首次系统性地比较了多个专门针对拉曼光谱设计的深度学习分类器，在三个开源拉曼数据集上评估了五种代表性架构，提供了公平可复现的基准比较。


<details>
  <summary>Details</summary>
Motivation: 当前拉曼光谱深度学习分类器虽然声称优于传统化学计量学方法，但评估往往孤立进行，缺乏专门为拉曼光谱设计的深度学习模型之间的直接比较。现有研究多与传统的机器学习方法或简单改编的视觉架构比较，而在共享开源数据集上对拉曼专用深度学习模型的系统性基准测试仍然稀缺。

Method: 研究选择了五种代表性的深度学习架构，在三个开源拉曼数据集上采用统一的训练和超参数调优协议进行评估。数据集选择支持标准评估、微调和显式分布偏移测试。使用分类准确率和宏平均F1分数作为评估指标，确保公平可复现的比较。

Result: 研究提供了拉曼光谱深度学习分类器的系统性基准测试结果，包括分类准确率和宏平均F1分数。这是首次对三个或更多已发表的拉曼专用深度学习分类器在多个开源数据集上进行直接比较的研究之一。

Conclusion: 该研究填补了拉曼光谱深度学习模型系统性比较的空白，为研究人员提供了公平、可复现的基准测试框架，有助于推动拉曼光谱分析领域的发展。

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [59] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 该论文研究了流形假设下核岭回归的泛化性能，提出了两种内在维度定义：基于核诱导度量的Minkowski维度和基于Kolmogorov n-宽度的有效维度，并建立了它们与泛化误差的关系。


<details>
  <summary>Details</summary>
Motivation: 流形假设认为当输入分布支撑的内在维度较低时，机器学习方法的泛化性能会显著提升。本文旨在研究核岭回归中内在维度的不同定义及其对泛化误差的影响，特别关注非规则域（如分形集）上的情况。

Method: 1. 定义了两种内在维度：$d_ρ$（基于核诱导度量的上Minkowski维度）和$d_K$（基于Kolmogorov n-宽度的有效维度）
2. 分析了Kolmogorov n-宽度与积分算子特征值的关系，证明n-宽度刻画了所有概率测度下的最坏特征值衰减
3. 提出了从有限样本估计n-宽度上界的算法
4. 对于接近均匀的分布，证明了使用$O(ε^{-d_ρ}\log\frac{1}ε)$样本可以高概率计算所有n-宽度的ε-准确上界
5. 计算了各种分形集的有效维度$d_K$并进行数值实验

Result: 1. 建立了Kolmogorov n-宽度与积分算子特征值的理论关系
2. 推导了约束核岭回归的泛化误差界：$O(n^{-\frac{2+d_K}{2+2d_K} + ε})$（对任意ε>0）
3. 证明了在规则域上$d_K = d_ρ$，但对于拉普拉斯核等核函数，在非规则域上$d_K$可以显著小于$d_ρ$
4. 提供了从有限样本有效估计n-宽度的算法和样本复杂度保证

Conclusion: 核岭回归的泛化性能由基于Kolmogorov n-宽度的有效维度$d_K$决定，而非传统的Minkowski维度$d_ρ$。在非规则域（如分形集）上，$d_K$可能远小于$d_ρ$，这意味着实际泛化性能可能比基于几何维度的预期更好。提出的算法能够从有限样本有效估计这些维度。

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [60] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: Beat-SSL：一种针对ECG信号的双上下文对比学习框架，通过节奏级和心跳级对比与软目标进行预训练，在有限标注数据下实现有效迁移学习


<details>
  <summary>Details</summary>
Motivation: 获取标注ECG数据用于监督模型训练具有挑战性。现有对比学习框架要么只关注全局上下文，要么未能充分利用ECG特定特征，且依赖硬对比目标，无法充分捕捉ECG信号特征相似性的连续性质。

Method: 提出Beat-SSL对比学习框架，通过节奏级和心跳级对比进行双上下文学习，使用软目标而非硬对比目标。在预训练后评估了两个下游任务：多标签分类（全局节奏评估）和ECG分割。

Result: 在消融研究后，最佳配置与包括一个ECG基础模型在内的三种其他方法比较。尽管基础模型有更广泛的预训练，Beat-SSL在多标签分类任务中达到了基础模型93%的性能，在分割任务中超过所有其他方法4%。

Conclusion: Beat-SSL通过双上下文学习和软目标有效解决了ECG对比学习中的局限性，在有限标注数据下实现了有竞争力的性能表现。

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [61] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: TTT-Discover：通过测试时强化学习让LLM针对特定问题持续训练，在数学、GPU内核、算法设计和生物学等多个领域取得SOTA结果


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法（如AlphaEvolve）使用冻结的LLM进行搜索，无法针对特定测试问题进行持续学习。需要一种方法让LLM在测试时能够基于特定问题的经验进行强化学习，以发现单个最优解而非平均良好解。

Method: 提出TTT-Discover方法，通过测试时训练进行强化学习，设计专门的学习目标和搜索子程序来优先探索最有希望的解决方案。使用开放式模型gpt-oss-120b，通过Tinker API以较低成本实现。

Result: 在多个领域取得新的SOTA结果：(1)数学：Erdős最小重叠问题和自相关不等式；(2)GPU内核工程：GPUMode内核竞赛（比先前最佳快2倍）；(3)算法设计：过去AtCoder算法竞赛；(4)生物学：单细胞分析去噪问题。所有结果均使用开源模型实现，成本仅数百美元/问题。

Conclusion: TTT-Discover通过测试时强化学习成功实现了针对特定问题的持续学习，在多个科学领域取得了超越先前最佳的结果，且使用开源模型和公开代码确保了可复现性。

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [62] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: 提出了一种名为"反事实训练"的新训练范式，利用反事实解释来增强模型的可解释能力，使模型在训练阶段就学习生成合理且可操作的反事实解释。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法多为后处理技术，虽然能生成满足合理性和可操作性要求的反事实，但模型本身并不具备生成这些解释的内在能力。作者希望让模型在训练阶段就直接学习生成理想的反事实解释，而不是依赖后处理。

Method: 提出反事实训练方法，在训练阶段使用反事实来最小化学到的表示与合理、可操作解释之间的差异。该方法让模型直接对最终目标负责，而不是依赖后处理方法。

Result: 通过实证和理论分析证明，该方法能够训练出具有内在理想反事实解释能力的模型，并且这些模型还表现出改进的对抗鲁棒性。

Conclusion: 反事实训练是一种有前景的方法，能够同时提升模型的可解释性和鲁棒性，为构建更可信的机器学习系统提供了新思路。

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>
