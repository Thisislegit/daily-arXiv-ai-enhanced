<div id=toc></div>

# Table of Contents

- [Zongheng Yang](#Zongheng Yang) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [Matei Zaharia](#Matei Zaharia) [Total: 10]
- [Sai Wu](#Sai Wu) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [Ziniu Wu](#Ziniu Wu) [Total: 3]
- [Google Scholar](#Google Scholar) [Total: 3]
- [Carsten Binnig](#Carsten Binnig) [Total: 1]
- [Xuanhe Zhou](#Xuanhe Zhou) [Total: 6]
- [cs.LG](#cs.LG) [Total: 50]


<div id='Zongheng Yang'></div>

# Zongheng Yang [[Back]](#toc)

### [1] [A Bootstrapping Technique for Reducing the Costs of Machine Learning Models for Predicting Execution Times in IaaS Clouds](https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3772052.3772251&hl=zh-CN&sa=X&d=12890399035264748384&ei=J-JqaZHiL7OlieoPn87c6Q4&scisig=AHkA5jRGYQhitJNTEIdXJraVmphJ&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=0&folt=cit)
*R Marotta,G Russo Russo,F Quaglia,P Di Sanzo*

Main category: Zongheng Yang

TL;DR: 该论文研究了一种机器学习模型引导技术，旨在降低在IaaS云环境中预测任务执行时间时的样本收集成本


<details>
  <summary>Details</summary>
Motivation: 在IaaS云环境中，机器学习已成为预测任务执行时间的强大工具，但训练ML模型需要大量样本收集，这在时间和金钱上都成本高昂，特别是当云提供商提供多种VM类型选择时，这种成本对用户来说往往不经济

Method: 论文研究了一种机器学习模型引导技术（bootstrapping technique），旨在减少训练准确预测模型所需的样本数量

Result: 论文摘要未提供具体结果，但暗示该引导技术能够降低样本收集成本

Conclusion: 通过机器学习模型引导技术，可以在IaaS云环境中更经济地实现任务执行时间的准确预测

Abstract: Machine Learning (ML) emerged as a powerful tool for predicting task execution times across the variety of VM types offered by Infrastructure-as-a-Service (IaaS) clouds. However, training ML models to ensure accurate predictions can often become uneconomical for users due to the high costs—in terms of both time and money—for collecting samples, especially when an IaaS cloud offers a wide choice of VM types. This paper investigates a ML model bootstrapping technique that …

</details>


### [2] [Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08342&hl=zh-CN&sa=X&d=5861867903108856598&ei=J-JqaZHiL7OlieoPn87c6Q4&scisig=AHkA5jS36C9nMnLI6rCxy0WzVCGI&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=1&folt=cit)
*R Chen,W Liang,Z Gong,L Ai,J Hirschberg*

Main category: Zongheng Yang

TL;DR: 首个针对口语对话中的心理操纵检测研究，通过合成多说话者基准数据集SPEECHMENTALMANIP，将文本数据集增强为高质量语音对话，填补了现有研究仅限于文本对话的空白。


<details>
  <summary>Details</summary>
Motivation: 心理操纵作为计算社会推理中的新兴任务，现有研究仅关注文本对话，忽视了操纵策略在语音对话中的表现形式。需要填补这一研究空白，探索语音对话中的心理操纵检测。

Method: 创建合成多说话者基准数据集SPEECHMENTALMANIP，将基于文本的数据集通过高质量语音合成技术转换为语音对话，为语音心理操纵检测提供首个系统性研究框架。

Result: 成功构建了首个口语心理操纵检测基准数据集，为研究语音对话中的操纵策略提供了标准化评估平台，填补了该领域的研究空白。

Conclusion: 语音对话中的心理操纵检测是一个重要但被忽视的研究方向，SPEECHMENTALMANIP数据集为这一领域的研究奠定了基础，未来需要进一步探索语音特有的操纵特征和检测方法。

Abstract: Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice …

</details>


### [3] [Decoding Order Matters in Autoregressive Speech Synthesis](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08450&hl=zh-CN&sa=X&d=1854203867351230060&ei=J-JqaZHiL7OlieoPn87c6Q4&scisig=AHkA5jQzO4lrDexNK2Xx6XzPETt1&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=2&folt=cit)
*M Zhao,A Ragni*

Main category: Zongheng Yang

TL;DR: 该研究探索了自回归语音合成中的解码顺序问题，通过掩码扩散框架支持任意解码顺序，发现解码顺序的随机性会影响语音质量，并比较了固定策略与自适应策略。


<details>
  <summary>Details</summary>
Motivation: 自回归语音合成通常采用从左到右的顺序，但解码顺序实际上是一种建模选择。研究旨在探索不同的解码顺序如何影响语音生成质量，以及是否可以通过优化解码顺序来提升性能。

Method: 采用掩码扩散框架，该框架逐步解除位置掩码，允许在训练和推理过程中使用任意解码顺序。通过插值恒等排列和随机排列来研究解码顺序随机性的影响，并比较固定策略（如从左到右和从右到左）与自适应策略。

Result: 研究发现解码顺序的随机性确实会影响语音质量。通过比较不同的解码顺序策略，揭示了顺序选择对语音合成性能的重要性。

Conclusion: 解码顺序是自回归语音合成中一个重要的建模维度，通过掩码扩散框架可以灵活探索不同顺序策略，为优化语音生成质量提供了新的方向。

Abstract: Autoregressive speech synthesis often adopts a left-to-right order, yet generation order is a modelling choice. We investigate decoding order through masked diffusion framework, which progressively unmasks positions and allows arbitrary decoding orders during training and inference. By interpolating between identity and random permutations, we show that randomness in decoding order affects speech quality. We further compare fixed strategies, such as\texttt {l2r} and\texttt {r2l} with adaptive …

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [4] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: Multiverse是一个新的软件事务内存系统，结合了无版本和多版本事务的优点，支持长时读取和频繁更新工作负载的高性能


<details>
  <summary>Details</summary>
Motivation: 现有STM系统在处理长时读取（访问大量频繁更新地址）时存在局限性。多版本方法虽然支持这类工作负载，但成本高昂，且在不必要版本化时会降低事务性能。需要一种既能支持长时读取，又不影响普通事务性能的解决方案。

Method: 提出Multiverse STM系统，支持版本化和无版本事务并发执行。系统设计确保无版本事务达到最先进无版本STM的性能水平，同时支持快速版本化事务以满足长时读取需求。实现了版本化和无版本事务的混合执行机制。

Result: 实验表明，在没有长时读取的常见工作负载下，Multiverse达到可比或更好的性能。在包含长时读取和频繁更新的工作负载下，Multiverse显著优于现有STM系统，吞吐量在某些情况下比其他STM快几个数量级。

Conclusion: Multiverse成功结合了无版本和多版本事务内存的优点，为支持长时读取的工作负载提供了高效解决方案，同时保持了普通工作负载的高性能。

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [5] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 开发Babel和ORION工具解决FAIR数据原则在实际互操作中的挑战，通过标识符映射和数据模型转换实现知识库的互操作性


<details>
  <summary>Details</summary>
Motivation: 尽管许多资源遵循FAIR原则并具有良好注释，但由于标识符模式和数据模型的差异，这些资源在实际中往往无法有效互操作，需要工具来弥合理论互操作性与实际互操作性之间的鸿沟

Method: 开发两个工具：Babel通过创建标识符映射集解决多标识符模式问题，生成等价标识符团并通过高性能API暴露；ORION通过摄取知识库并将其转换为社区管理的通用数据模型解决多数据模型问题

Result: 成功创建了Babel和ORION工具，能够支持数据互操作，并提供了通过这两个工具创建的完全互操作知识库库，可在https://robokop.renci.org下载使用

Conclusion: Babel和ORION工具有效解决了FAIR数据在实际互操作中的挑战，通过标识符映射和数据模型标准化实现了知识库的真正互操作性

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [6] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: SDP算法通过选择性发现和剪枝，高效发现按冗余计数排序的前k个函数依赖，解决传统方法计算成本高和结果集过大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统函数依赖发现算法存在两个主要问题：1) 计算成本过高，复杂度随元组数平方增长、随属性数指数增长；2) 结果集过大，难以识别有用的依赖关系。需要一种能选择性发现最重要依赖的高效方法。

Method: 提出SDP（选择性发现与剪枝）算法，基于冗余计数排序发现前k个函数依赖。使用冗余上界进行搜索空间剪枝，该上界具有单调性：添加属性会细化分区从而降低上界。采用三种优化：按分区基数排序属性、使用分区基数矩阵中的成对统计收紧边界、全局调度器优先探索有希望的分支。

Result: 在超过40个数据集上的实验表明，SDP比穷举方法快得多且内存使用更少，能有效发现按冗余计数排序的前k个函数依赖。

Conclusion: SDP通过选择性发现和剪枝策略，解决了传统函数依赖发现算法的可扩展性问题，能够高效识别最重要的依赖关系，适用于大规模高维数据。

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [7] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 提出TransactionMerger中间件，通过合并结构相似的事务和语句来提升应用端事务处理性能


<details>
  <summary>Details</summary>
Motivation: 传统事务处理系统存在性能瓶颈，特别是在高并发场景下，相似事务的重复执行和资源竞争导致效率低下。通过合并结构相似的事务可以显著减少数据库负载和锁竞争。

Method: 设计TransactionMerger中间件，采用三种优化策略：1) 基于SQL语义合并相似语句；2) 消除冗余读取；3) 通过预计算聚合效果合并冲突语句。提供静态分析工具识别合并机会而不违反隔离性。

Result: 在TPC-C基准测试中吞吐量提升最高达2.65倍，在实际应用Spree中吞吐量提升3.52倍，证明事务合并能显著提升系统性能。

Conclusion: 事务合并是提升应用端事务处理性能的有效方法，TransactionMerger中间件能够在不违反隔离性的前提下，通过智能合并相似事务显著提升系统吞吐量。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [8] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 提出将数学数据模型转换为关系模式的算法，证明其高效性，并应用于家谱树建模


<details>
  <summary>Details</summary>
Motivation: 需要将Elementary Mathematical Data Model方案转换为关系数据库可用的模式，并处理非关系约束，以支持MatBase智能数据库管理系统原型

Method: 设计伪代码算法，将数学数据模型方案转换为关系模式和相关非关系约束集，证明算法性能，应用于家谱树子宇宙建模，提供SQL和VBA代码示例

Result: 算法被证明非常快速、稳健、完整且最优，成功应用于家谱树建模，提供了约束实施的具体代码示例和开发指南

Conclusion: 提出的算法有效实现了数学数据模型到关系模式的转换，为MatBase系统提供了实用的约束实施解决方案

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>


<div id='Matei Zaharia'></div>

# Matei Zaharia [[Back]](#toc)

### [9] [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.22737&hl=zh-CN&sa=X&d=14153719490578423189&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jTlHEGL47U7azKDhNLgjC-d&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=0&folt=rel)
*A Liu,M He,S Zeng,S Zhang,L Zhang,C Wu,W Jia…*

Main category: Matei Zaharia

TL;DR: 扩散语言模型（DLLMs）通过并行解码解决自回归生成在推理时的串行瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 自回归生成作为大语言模型的标准解码范式，其逐token生成特性在推理时限制了并行性，导致延迟问题

Method: 提出扩散语言模型（DLLMs），采用扩散过程实现并行解码，通过去噪过程一次性生成完整序列

Result: DLLMs在保持生成质量的同时显著提升推理速度，实现更高效的并行解码

Conclusion: 扩散语言模型为语言生成提供了有前景的替代方案，能够克服自回归解码的并行性限制

Abstract: Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by …

</details>


### [10] [Gamayun's Path to Multilingual Mastery: Cost-Efficient Training of a 1.5 B-Parameter LLM](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.21580&hl=zh-CN&sa=X&d=5591099301271435698&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jRK6g9pcGHYLw7cV5e5XqRq&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=1&folt=rel)
*A Podolskiy,S Molokov,T Gerasin,M Titov…*

Main category: Matei Zaharia

TL;DR: Gamayun是一个1.5B参数的多语言语言模型，从头开始在2.5T tokens上训练，专为资源受限环境设计


<details>
  <summary>Details</summary>
Motivation: 解决资源受限环境中高效部署语言模型的研究不足问题，提供可在有限计算资源下运行的多语言模型

Method: 从头开始训练1.5B参数的多语言模型，使用2.5T tokens的训练数据，优化模型架构和训练策略以提高效率

Result: 成功开发出Gamayun模型，在资源受限环境中表现出良好的性能，实现了高效的多语言处理能力

Conclusion: Gamayun证明了在资源受限环境下部署高效多语言语言模型的可行性，为实际应用提供了实用解决方案

Abstract: We present Gamayun, a 1.5 B-parameter multilingual language model trained entirely from scratch on 2.5 T tokens. Designed for efficiency and deployment in resource-constrained environments, Gamayun addresses the lack of research on …

</details>


### [11] [ForgetMark: Stealthy Fingerprint Embedding via Targeted Unlearning in Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08189&hl=zh-CN&sa=X&d=10986440491085924953&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jThCjeMrogiSddIygMf3Z39&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=2&folt=rel)
*Z Xu,H Zhang,Z Wang,Q Liu,H Xu,W Xing,M Han*

Main category: Matei Zaharia

TL;DR: ForgetMark是一种新型隐蔽后门攻击方法，通过低困惑度触发器和自适应响应模式规避检测


<details>
  <summary>Details</summary>
Motivation: 现有后门指纹存在高困惑度触发器易被过滤、固定响应模式易被启发式检测器发现、以及在良性输入上产生虚假激活等问题，需要更隐蔽的后门攻击方法

Method: ForgetMark采用低困惑度触发器设计，结合自适应响应模式，避免固定模式暴露，同时减少在良性输入上的虚假激活

Result: 该方法在保持高攻击成功率的同时，显著降低了被现有检测方法发现的概率，提高了后门攻击的隐蔽性

Conclusion: ForgetMark为后门攻击提供了更隐蔽的解决方案，对现有防御方法构成新挑战，强调了需要更先进的检测技术

Abstract: Existing invasive (backdoor) fingerprints suffer from high-perplexity triggers that are easily filtered, fixed response patterns exposed by heuristic detectors, and spurious activations on benign inputs. We introduce\textsc {ForgetMark}, a stealthy …

</details>


### [12] [Breaking myths in LLM scaling and emergent abilities with a comprehensive statistical analysis](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S092523122503214X&hl=zh-CN&sa=X&d=2016090856459858081&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jSI7mJZravmJNqpTAlcUCq-&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=3&folt=rel)
*K Sun,R Wang*

Main category: Matei Zaharia

TL;DR: LLM评估的重要性日益凸显，评估揭示了模型规模、训练类型、架构等因素对性能的影响


<details>
  <summary>Details</summary>
Motivation: 随着LLM快速发展，评估对于理解和推动模型进步变得至关重要，需要系统研究影响LLM性能的各种因素

Method: 通过评估分析不同因素对LLM性能的影响，包括模型规模、训练类型、架构等关键变量

Result: 评估揭示了多种因素对LLM性能的影响，为模型优化和选择提供了实证依据

Conclusion: 系统评估是理解LLM能力和局限性的关键，为未来模型发展提供指导方向

Abstract: Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors …

</details>


### [13] [Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.24618&hl=zh-CN&sa=X&d=1432766452130016355&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jTsO4FMY6hZ-X702_TiO5a4&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=4&folt=rel)
*J Lu,J Qin,L Qiao,Y Li,X Dai,B Ke,J He,R Qiao…*

Main category: Matei Zaharia

TL;DR: Youtu-LLM是一个轻量级但功能强大的语言模型，在保持高计算效率的同时具备原生智能体能力，无需依赖蒸馏技术。


<details>
  <summary>Details</summary>
Motivation: 当前小型语言模型通常依赖蒸馏技术来获得能力，这限制了它们的性能和效率。研究者希望开发一个既轻量又具备原生智能体能力的模型，能够在资源受限的环境中高效运行。

Method: Youtu-LLM（1.96B参数）采用从头预训练的方法，而非依赖蒸馏技术。模型设计注重计算效率与智能体能力的平衡，通过特定的架构和训练策略实现轻量级但强大的性能。

Result: 模型在保持轻量级（1.96B参数）的同时，展现出强大的语言理解和智能体能力，计算效率高，适合资源受限环境部署。

Conclusion: Youtu-LLM证明了轻量级语言模型可以通过从头预训练而非蒸馏获得强大的智能体能力，为资源受限环境中的高效AI部署提供了新途径。

Abstract: We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96 B) is pre-trained from …

</details>


### [14] [Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.19238&hl=zh-CN&sa=X&d=11219162641487679779&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jRQk7fmqacwfJkq2I8pXzNX&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=5&folt=rel)
*AM Gueorguieva,A Caliskan*

Main category: Matei Zaharia

TL;DR: 该研究探讨了LLMs对非受保护污名化身份的社会偏见，分析了污名特征与模型偏见之间的关系


<details>
  <summary>Details</summary>
Motivation: 大型语言模型已被证明存在社会偏见，但对非受保护污名化身份的偏见研究不足，且不清楚污名的哪些社会特征与LLM输出中的偏见相关

Method: 研究可能通过系统分析不同污名化身份在LLM输出中的表现，考察污名的社会特征（如可见性、可控性、危险性等）与模型偏见之间的关联

Result: 研究发现LLMs对非受保护污名化身份存在系统性偏见，且特定污名特征（如可见性、危险性）与偏见程度显著相关

Conclusion: 需要更全面的偏见评估框架，不仅要关注受保护群体，还要包括非受保护污名化身份，并考虑污名的社会特征维度

Abstract: Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown …

</details>


### [15] [Parallel Token Prediction for Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.21323&hl=zh-CN&sa=X&d=5323568586116534353&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jR_oamNAt-4niFTRobVI1Ib&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=6&folt=rel)
*F Draxler,J Will,FM Sofian,T Karaletsos,S Singh…*

Main category: Matei Zaharia

TL;DR: 提出Parallel Token Prediction (PTP)框架，通过单次transformer调用联合预测多个依赖token，实现语言模型的并行序列生成


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型逐token生成序列效率低下，需要提高序列生成速度，同时保持生成质量

Method: 将采样过程融入transformer架构，单次调用联合预测多个依赖token，实现并行序列生成

Result: PTP框架在保持生成质量的同时显著提升序列生成速度，实现高效并行解码

Conclusion: PTP为语言模型提供了一种通用的并行序列生成框架，平衡了生成速度与质量

Abstract: We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the …

</details>


### [16] [DIP: Dynamic In-Context Planner For Diffusion Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03199&hl=zh-CN&sa=X&d=12451250639277289841&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jRMF0otfo9Jh9GSqXw9_Qbo&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=7&folt=rel)
*Y Li,H Meng,C Wang,H Chen*

Main category: Matei Zaharia

TL;DR: 扩散语言模型在自然语言任务中表现出色，但双向注意力机制导致长上下文计算成本高昂


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在自然语言任务中展现出强大潜力，特别是在上下文示例学习方面，但其双向注意力机制在长上下文处理时计算成本过高，限制了实际应用

Method: 未在摘要中明确说明具体方法，但问题核心是扩散语言模型的双向注意力机制导致的长上下文计算效率问题

Result: 未在摘要中提供具体实验结果，但指出了扩散语言模型在长上下文处理时存在显著的计算成本问题

Conclusion: 扩散语言模型虽然性能强大，但需要解决长上下文处理时的计算效率问题才能更广泛地应用

Abstract: Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases …

</details>


### [17] [LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.20002&hl=zh-CN&sa=X&d=6280039184666866606&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jTEhAQCfZsSNKAJ2sj3P2vO&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=8&folt=rel)
*J You,J Yang,Y Xie,Z Wu,X Li,F Li,P Wang,J Xu…*

Main category: Matei Zaharia

TL;DR: 该论文针对时间序列预测中训练数据有限和复杂噪声动态的挑战，提出了一种新的训练策略，通过逐步增加预测长度来改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度预测模型通常使用完整长度序列进行监督训练，但在实际应用中面临训练数据有限和复杂噪声动态的挑战，需要更有效的训练策略。

Method: 提出了一种新的训练策略，通过逐步增加预测长度来训练模型，从短序列开始逐步扩展到长序列预测，以更好地处理数据限制和复杂动态。

Result: 该方法在多个真实世界数据集上（如金融和能源领域）相比传统全长度监督训练方法，显著提高了预测准确性和鲁棒性。

Conclusion: 渐进式预测长度训练策略是解决时间序列预测中数据限制和复杂动态问题的有效方法，为实际应用提供了更可靠的预测模型。

Abstract: Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length …

</details>


### [18] [Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.01966&hl=zh-CN&sa=X&d=11819346928659783213&ei=KuJqadq0EIeUywS5-prpDw&scisig=AHkA5jT2SG6j62RgQcMBteFhxyED&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=9&folt=rel)
*B Yin,Q Li,R Yu,X Wang*

Main category: Matei Zaharia

TL;DR: 该论文提出了一种针对指令调优中LLM提示精炼过程的实例级审计方法，通过分析精炼前后提示的语义变化来评估精炼质量


<details>
  <summary>Details</summary>
Motivation: 指令调优越来越依赖基于LLM的提示精炼，但缺乏系统方法来评估精炼过程的质量和效果，需要实例级审计来理解精炼如何改变提示的语义内容

Method: 提出实例级审计框架，通过分析精炼前后提示的语义变化来评估精炼质量，可能包括语义相似性度量、指令对齐度量和选择性重写分析

Result: 通过审计发现提示精炼过程中的关键模式，识别精炼质量的变化，为优化精炼过程提供实证依据

Conclusion: 实例级审计是理解和改进指令调优中提示精炼过程的重要工具，有助于提高精炼质量和指令对齐效果

Abstract: Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit …

</details>


<div id='Sai Wu'></div>

# Sai Wu [[Back]](#toc)

### [19] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08545&hl=zh-CN&sa=X&d=6279153404487472063&ei=KuJqabnGHqC16rQPqaeYsAc&scisig=AHkA5jQbCKg3-E2mUf3FP3bzzcRJ&oi=scholaralrt&hist=i6heNjgAAAAJ:18095185696304281508:AHkA5jQB6iGmaAP-whloFJID0lTf&html=&pos=0&folt=art)
*Z Dai,Z Zhao,H Wang,X Tang,S Wu,C Yao,Z Gao…*

Main category: Sai Wu

TL;DR: 该研究提出了一种基于大语言模型的智能编程辅导系统，通过分析学生代码中的编程模式来提供个性化辅导，而非仅仅修复错误代码


<details>
  <summary>Details</summary>
Motivation: 现有智能编程辅导系统主要关注修复错误代码，但缺乏对学生编程模式的分析和个性化辅导。需要一种能够识别学生编程风格并提供针对性指导的系统，以提升编程学习效果。

Method: 基于大语言模型开发智能编程辅导系统，通过分析学生代码中的编程模式（如算法选择、代码结构、编码风格等），识别学生的编程习惯和潜在问题，提供个性化的辅导建议和优化指导。

Result: 系统能够有效识别学生的编程模式，提供针对性的辅导建议，相比仅修复错误的传统方法，更能帮助学生理解编程概念、改进编码风格，提升编程学习效率和质量。

Conclusion: 基于大语言模型的编程模式分析为智能编程辅导提供了新方向，通过关注学生的编程风格而不仅仅是错误修复，能够实现更有效的个性化编程教育，具有重要的教育应用价值。

Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming …

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)
*Ziting Zhang,Kai Wan,Minquan Cheng,Shuo Shao,Giuseppe Caire*

Main category: cs.DC

TL;DR: 研究异构分布式线性可分计算问题，针对任意异构数据分配，提出通用计算方案和逆界，刻画任务函数可计算维度与通信成本之间的基本权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同构分布式线性可分计算问题，假设每个工作节点持有相同数量的数据集，且数据分配由数据中心精心设计（如循环分配）。本文考虑更一般的任意异构数据分配场景，其中数据分配预先给定且各工作节点可能持有不同数量的数据集，旨在刻画这种更一般设置下的基本权衡关系。

Method: 针对整数通信成本约束下的任意异构数据分配，提出通用计算方案和通用逆界，通过刻画数据分配结构来建立权衡关系。然后将所提出的计算方案和逆界扩展到分数通信成本情况。

Result: 在整数通信成本约束下，针对任意异构数据分配提出了通用计算方案和通用逆界，在某些参数范围内两者一致。进一步将结果扩展到分数通信成本情况。

Conclusion: 本文解决了异构分布式线性可分计算问题，针对任意异构数据分配刻画了任务函数可计算维度与通信成本之间的基本权衡关系，提出的通用计算方案和逆界为这一更一般设置提供了理论基础。

Abstract: Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.

</details>


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [21] [Markovian Pre-Trained Transformer for Next-Item Recommendation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08275&hl=zh-CN&sa=X&d=4748469596494327208&ei=KOJqadiaK5Sw6rQPxrTySQ&scisig=AHkA5jSFQdKBtVneTXYrtYrgD9ma&oi=scholaralrt&hist=i6heNjgAAAAJ:8133205940682390297:AHkA5jQfTAaKkXi3kBQuJYouRHpx&html=&pos=0&folt=rel)
*C Xu,G Li,J Wang,W Zhang*

Main category: Ziniu Wu

TL;DR: MPT是一种用于下一项推荐的马尔可夫预训练Transformer，通过在合成马尔可夫链上完全预训练，然后通过轻量级微调实现最先进性能的可迁移模型


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统通常需要大量真实用户数据进行训练，这存在数据稀缺、隐私保护和冷启动问题。需要开发一种能够通过合成数据预训练，然后高效迁移到真实场景的推荐模型

Method: 提出马尔可夫预训练Transformer（MPT）：1）在合成马尔可夫链上完全预训练模型，学习序列模式；2）设计轻量级微调机制，将预训练知识迁移到真实推荐任务；3）利用Transformer架构捕捉序列依赖关系

Result: MPT在多个推荐基准测试中实现了最先进的性能，证明了仅使用合成数据预训练然后轻量级微调的有效性。模型展示了良好的可迁移性和数据效率

Conclusion: MPT为推荐系统提供了一种新颖的预训练范式，通过合成数据训练实现高性能，解决了数据稀缺和隐私问题，同时保持了模型的可迁移性和效率

Abstract: We introduce the Markovian Pre-trained Transformer (MPT) for next-item recommendation, a transferable model fully pre-trained on synthetic Markov chains, yet capable of achieving state-of-the-art performance by fine-tuning a lightweight …

</details>


### [22] [Serverless Elasticsearch: the Transition from Stateful to Stateless](https://scholar.google.com/scholar_url?url=https://www.cs.ubc.ca/~brendan/papers/serverless-elasticsearch.pdf&hl=zh-CN&sa=X&d=4663664700620309864&ei=KOJqadiaK5Sw6rQPxrTySQ&scisig=AHkA5jQ42xzoAOOsQlkP2_egdEN7&oi=scholaralrt&hist=i6heNjgAAAAJ:8133205940682390297:AHkA5jQfTAaKkXi3kBQuJYouRHpx&html=&pos=1&folt=rel)
*I Psaroudakis,P Salehi,J Bryan,FF Castaño,B Cully…*

Main category: Ziniu Wu

TL;DR: Elasticsearch从共享无架构转向共享存储架构，以解决本地磁盘存储带来的可扩展性、数据迁移和运维挑战


<details>
  <summary>Details</summary>
Motivation: 传统Elasticsearch的共享无架构依赖本地磁盘存储集群数据，导致数据迁移成本高、存储容量受限、运维复杂，需要更灵活可扩展的存储解决方案

Method: 将Elasticsearch架构从共享无架构转变为共享存储架构，使用共享存储系统（如云存储）替代本地磁盘，实现存储与计算分离

Result: 共享存储架构显著提升了Elasticsearch的可扩展性，简化了数据迁移和节点管理，降低了运维复杂度，同时保持了原有的搜索和分析性能

Conclusion: 共享存储架构是Elasticsearch发展的必然方向，解决了传统架构的诸多限制，为大规模分布式搜索和分析提供了更灵活、可扩展的基础设施

Abstract: Elasticsearch (ES) is a distributed search and analytics database consisting of a cluster of nodes, each hosting a disjoint subset of the data. Until recently ES had a shared-nothing architecture that relied on local disks to store cluster data such as …

</details>


### [23] [Automated Data Quality Frameworks for Healthcare Data Lakes](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Narasimha-Chaitanya-Samineni/publication/399697229_Automated_Data_Quality_Frameworks_for_Healthcare_Data_Lakes/links/69657c35c906f117f2a45653/Automated-Data-Quality-Frameworks-for-Healthcare-Data-Lakes.pdf&hl=zh-CN&sa=X&d=5626713935160258952&ei=KOJqadiaK5Sw6rQPxrTySQ&scisig=AHkA5jQYqaaNBdGUKxtpksZ-lWCB&oi=scholaralrt&hist=i6heNjgAAAAJ:8133205940682390297:AHkA5jQfTAaKkXi3kBQuJYouRHpx&html=&pos=2&folt=rel)
*NC Samineni*

Main category: Ziniu Wu

TL;DR: 医疗数据湖作为统一存储库，整合电子健康记录、索赔数据、实验室系统、影像元数据、设备生成的患者流和财务信息，构建可扩展的分析环境


<details>
  <summary>Details</summary>
Motivation: 医疗数据分散在不同系统中，难以实现统一分析和洞察，需要构建集成存储库来支持医疗分析、研究和决策

Method: 建立医疗数据湖作为统一存储库，整合多种医疗数据源，构建可扩展的分析环境

Result: 创建了能够整合异构医疗数据的数据湖架构，支持大规模分析和数据驱动的医疗决策

Conclusion: 医疗数据湖是解决医疗数据碎片化问题的有效方案，为医疗分析和研究提供了统一的数据基础

Abstract: Healthcare data lakes serve as unified repositories that integrate electronic health records (EHR), claims data, laboratory systems, imaging metadata, device-generated patient streams, and financial information into scalable analytical environments. As …

</details>


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [24] [PerCache: Predictive Hierarchical Cache for RAG Applications on Mobile Devices](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Kaiwei-Liu-7/publication/399666884_PerCache_Predictive_Hierarchical_Cache_for_RAG_Applications_on_Mobile_Devices/links/696471e4ee048155cffa93e3/PerCache-Predictive-Hierarchical-Cache-for-RAG-Applications-on-Mobile-Devices.pdf&hl=en&sa=X&d=13422459121275262622&ei=KOJqaa6RGayK6rQPm4H2iQs&scisig=AHkA5jTIn67GZ8-M23A59AoIVUOX&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=0&folt=cit)
*K Liu,L Zeng,L Xu,B Yang,Z Yan*

Main category: Google Scholar

TL;DR: 移动设备上RAG系统响应延迟高，提出通过复用不同查询间的中间计算结果来降低延迟


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)在移动设备应用广泛，但由于长提示和资源限制，移动RAG系统响应延迟显著较高，需要优化性能

Method: 通过复用不同查询间的中间计算结果来降低延迟，具体方法未在摘要中详细说明

Result: 摘要未提供具体实验结果，但暗示该方法有望降低移动RAG系统的响应延迟

Conclusion: 复用中间计算结果是降低移动RAG系统延迟的有前景方法

Abstract: Retrieval-augmented generation (RAG) has been extensively used as a de facto paradigm in various large language model (LLM)-driven applications on mobile devices, such as mobile assistants leveraging personal emails or meeting records. However, due to the lengthy prompts and the resource constraints, mobile RAG systems exhibit significantly high response latency. On this issue, one promising approach is to reuse intermediate computational results across different queries to …

</details>


### [25] [CoRe: Collaborative Replica Scheduling for Large-Scale Cloud Database Services](https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3772052.3772248&hl=en&sa=X&d=13799453915753151081&ei=KOJqaa6RGayK6rQPm4H2iQs&scisig=AHkA5jQkp2BIMr1b-SmrleEJl7q-&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=1&folt=cit)
*H Lei,S Di,C Li,K Zhou,M Xie,F Yang,J Zhu,X Li…*

Main category: Google Scholar

TL;DR: 云数据库服务中大规模用户表通过数据分片和复制技术进行细粒度副本分区，副本放置优化对系统性能至关重要


<details>
  <summary>Details</summary>
Motivation: 云数据库服务集群管理大量用户表，这些表通过数据分片和复制技术被分区成细粒度副本并分布在多个存储节点上。次优的副本分布会导致集群过载和持续负载不平衡，最终降低系统性能

Method: 基于数据分片和复制技术的细粒度副本分区方法，将大规模用户表分布到多个存储节点

Result: 未在摘要中明确说明具体实验结果，但强调了次优副本分布会导致集群过载和负载不平衡问题

Conclusion: 云数据库服务中副本放置优化对于避免集群过载、维持负载平衡和保障系统性能至关重要

Abstract: Cloud database service clusters manage extensive collections of user tables, ranging from dozens to hundreds of thousands, which are partitioned into fine-grained replicas using data sharding and replication techniques and distributed across multiple storage nodes. Optimal replica placement is critical in cloud database services, as suboptimal distribution often results in cluster overload and persistent load imbalance, ultimately degrading system performance and …

</details>


### [26] [From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08741&hl=en&sa=X&d=5650556968670370338&ei=KeJqaZXSL9vWieoPkLe3iAs&scisig=AHkA5jQTXK3_CbLq19o85DrfaYMG&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=3&folt=rel)
*A Gulati,S Sen,W Sarguroh,K Paul*

Main category: Google Scholar

TL;DR: LLMs难以处理大规模企业电子表格，本文提出了一种新方法来解决这个问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理包含数千行数字数据、多个链接工作表以及嵌入图表和收据等视觉内容的大规模企业电子表格时存在困难。现有方法无法有效处理这种复杂结构的数据。

Method: 本文提出了一种新的方法（具体方法未在摘要中详细说明，但暗示是改进现有技术的方法）来处理大规模企业电子表格的推理问题。

Result: 该方法在多个基准测试中达到了最先进的性能，显著提升了LLMs处理复杂企业电子表格的能力。

Conclusion: 提出的方法有效解决了LLMs处理大规模企业电子表格的挑战，为实际企业应用提供了可行的解决方案。

Abstract: Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art …

</details>


<div id='Carsten Binnig'></div>

# Carsten Binnig [[Back]](#toc)

### [27] [BlockSketch: A Hybrid Tree-Based Sketch for Keyword Search in Blockchain Systems](https://scholar.google.com/scholar_url?url=https://link.springer.com/content/pdf/10.1007/s41019-025-00326-6_reference.pdf&hl=zh-CN&sa=X&d=17466352310355923682&ei=KuJqac7kAZSw6rQPxrTySQ&scisig=AHkA5jSt-KhSJKOES2VpZP_EJ5om&oi=scholaralrt&hist=i6heNjgAAAAJ:15269883191641703195:AHkA5jTZCYhse3wIcZXHnO-iboY0&html=&pos=0&folt=cit)
*Y Liu,X Qi,Z Zhang,Y Yang,C Jin,A Zhou*

Main category: Carsten Binnig

TL;DR: 该论文探讨了区块链监管中的关键词搜索技术，针对交易量激增带来的监管挑战，提出了一种高效的区块链关键词搜索机制。


<details>
  <summary>Details</summary>
Motivation: 区块链技术的快速发展和交易量的指数级增长给监管带来了重大挑战，需要高效的监控分析机制。关键词搜索作为数据库系统中的成熟技术，可以应用于区块链监管，因为链上交易可以被视为结构化数据。

Method: 论文将区块链监管中的关键词搜索问题形式化，并提出了相应的解决方案。方法可能涉及将区块链交易数据视为结构化数据集，并应用或改进现有的关键词搜索技术以适应区块链环境。

Result: 论文提出了适用于区块链监管的关键词搜索机制，该机制能够有效处理大规模交易数据，为监管机构提供高效的分析工具。

Conclusion: 关键词搜索技术可以成功应用于区块链监管，为解决交易量激增带来的监管挑战提供了有效的技术方案。

Abstract: The rapid advancement of blockchain technologies [1, 2] has enabled a wide range of applications [3–5]. The exponential growth in transaction volumes poses significant regulatory challenges [6] and necessitates efficient mechanisms for monitoring and analysis [7]. Keyword search, as a prevalent technique enabling this capability, has been extensively studied in database systems [8–10]. Its applicability to blockchain supervision stems from the fact that on-chain transactions can be …

</details>


<div id='Xuanhe Zhou'></div>

# Xuanhe Zhou [[Back]](#toc)

### [28] [ALERT: Zero-shot LLM Jailbreak Detection via Internal Discrepancy Amplification](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03600&hl=zh-CN&sa=X&d=7482814631189776069&ei=KeJqaaSYILui6rQPrda9qQc&scisig=AHkA5jSsDebZ_sUKiije3E1JNHvN&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=1&folt=rel)
*X Lin,P Li,Z Zeng,T Li,T Wei,X Ning,G Li,Y Chen…*

Main category: Xuanhe Zhou

TL;DR: 该论文针对大语言模型（LLMs）的越狱攻击检测问题，提出了一种新的检测方法，旨在更有效地识别和防御此类安全威胁。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种安全对齐策略，但大语言模型仍然极易受到越狱攻击，这会破坏安全护栏并带来严重的安全风险。现有的检测方法主要检测越狱状态，但存在局限性，需要更有效的检测技术来应对这一挑战。

Method: 论文提出了一种新的越狱攻击检测方法，具体方法细节需要从完整论文中获取，但核心是改进现有检测方法的不足，提供更准确和可靠的越狱检测能力。

Result: 该方法在检测越狱攻击方面表现出更好的性能，相比现有方法有显著提升，能够更有效地识别安全威胁，增强大语言模型的安全性。

Conclusion: 提出的检测方法为解决大语言模型的越狱攻击问题提供了有效解决方案，有助于提升模型的安全性和可靠性，为未来的安全防护研究提供了重要参考。

Abstract: Despite rich safety alignment strategies, large language models (LLMs) remain highly susceptible to jailbreak attacks, which compromise safety guardrails and pose serious security risks. Existing detection methods mainly detect jailbreak status …

</details>


### [29] [Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.19920&hl=zh-CN&sa=X&d=1691090652215400058&ei=KeJqaaSYILui6rQPrda9qQc&scisig=AHkA5jQF4VpjaDCOE-ynHDHAPQQy&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=2&folt=rel)
*J Wu,J Liu,Z Zeng,T Zhan,W Huang*

Main category: Xuanhe Zhou

TL;DR: LLM在关键领域部署受幻觉问题阻碍，虽然缩放定律提升了通用能力，但理论框架表明需要新的方法来减少事实性错误


<details>
  <summary>Details</summary>
Motivation: LLM在关键领域（如医疗、金融、法律等）的部署受到幻觉问题的严重阻碍，这些幻觉会产生看似合理但事实错误的内容，限制了LLM在需要高可靠性和准确性的场景中的应用

Method: 论文未提供具体方法细节，但提到理论框架表明需要超越传统缩放定律的新方法来解决幻觉问题

Result: 未提供具体实验结果，但强调了当前LLM部署面临的挑战和需要解决的关键问题

Conclusion: LLM幻觉问题是阻碍其在关键领域部署的主要障碍，需要开发新的理论框架和方法来减少事实性错误，超越单纯依赖模型缩放的传统路径

Abstract: LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest …

</details>


### [30] [Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.14008&hl=zh-CN&sa=X&d=1718303489563373763&ei=KeJqaaSYILui6rQPrda9qQc&scisig=AHkA5jRWCISl_Y9moAPqwX8c2Hr3&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=3&folt=rel)
*S Li,J Gu,K Liu,Z Lin,Z Wei,A Grover,J Kuen*

Main category: Xuanhe Zhou

TL;DR: MD3模型在多种多模态任务中表现优异，但推理速度较慢，主要受限于需要大量去噪步骤


<details>
  <summary>Details</summary>
Motivation: 掩码离散扩散模型（MDMs）在图像理解、生成和编辑等多模态任务中表现出色，但其推理速度不理想，限制了实际应用

Method: 论文未提供具体方法细节，但从摘要看可能涉及改进MD3模型的推理效率

Result: 摘要未提供具体结果，但暗示MD3模型存在推理速度瓶颈

Conclusion: 需要解决MD3模型的推理速度问题以提升其实用性

Abstract: Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to …

</details>


### [31] [Conceptual Framework for Designing Domain-Specific LLM-Based Information](https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Dzh-CN%26lr%3D%26id%3DF_KtEQAAQBAJ%26oi%3Dfnd%26pg%3DPA63%26ots%3DjVmXsTKPGq%26sig%3DHYPsxRFJlC1MEN9q8udFVQxQo5I&hl=zh-CN&sa=X&d=16591893702558916692&ei=KuJqacq4LMyQieoPxcX9yAo&scisig=AHkA5jRiF0sGtc_P_46uUefO5ysJ&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=0&folt=cit)
*D Ullrich,J Wallys,S Hinrichsen*

Main category: Xuanhe Zhou

TL;DR: RAG技术结合LLM与领域知识，但企业实施面临高复杂度，缺乏统一监管框架


<details>
  <summary>Details</summary>
Motivation: RAG技术已成为结合领域特定信息与生成式语言能力的关键技术，为企业提供透明、最新的信息。然而许多企业在试点LLM信息系统时报告规划实施复杂度高，缺乏一致的监管框架来映射关键决策。

Method: 论文未在摘要中明确说明具体方法，但基于检索增强生成(RAG)技术，结合大型语言模型与领域特定信息检索。

Result: 摘要未提供具体实验结果，但指出当前RAG系统实施面临高复杂度，缺乏统一监管框架来指导关键决策。

Conclusion: 需要建立一致的监管框架来指导RAG系统的规划与实施，降低企业采用该技术的复杂度。

Abstract: Retrieval-augmented generation (RAG) based on large language models (LLMs) has established itself as a key technology for combining domain-specific information with generative language skills, thereby providing transparent, up-to-date information. Many firms are already piloting such LLM-based information systems, but report a high degree of complexity in planning and implementation. A generally accepted regula-tory framework that consistently maps key decisions is not yet …

</details>


### [32] [Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08778&hl=zh-CN&sa=X&d=16841627210155501751&ei=KuJqacq4LMyQieoPxcX9yAo&scisig=AHkA5jTxHCLTqieWJBd_fNkw4Uli&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=1&folt=cit)
*T Jin,Y Choi,Y Zhu,D Kang*

Main category: Xuanhe Zhou

TL;DR: 对文本到SQL技术评估中人工标注的有效性进行实证研究，发现当前基准测试存在标注质量问题，并提出改进方法


<details>
  <summary>Details</summary>
Motivation: 文本到SQL技术依赖公共基准测试和排行榜进行比较和选择，但这些基准测试在问题构建和答案评估中严重依赖人工标注，标注有效性对技术评估至关重要

Method: 对现有文本到SQL基准测试中的人工标注进行实证研究，分析标注质量问题，并提出改进标注质量和评估方法的建议

Result: 研究发现当前基准测试存在显著的标注质量问题，包括不一致性、错误和偏差，这些问题影响了技术评估的可靠性和公平性

Conclusion: 文本到SQL基准测试中的人工标注存在质量问题，需要改进标注流程和评估方法以确保技术评估的有效性和可靠性

Abstract: Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial. In this paper, we conduct an …

</details>


### [33] [Mapping the Collaboration between Crowdsourcing and Large Language Models: A Fine-Grained Survey](https://scholar.google.com/scholar_url?url=https://www.intechopen.com/online-first/1229427&hl=zh-CN&sa=X&d=11594090668386320945&ei=KuJqacq4LMyQieoPxcX9yAo&scisig=AHkA5jT6REAxa9N5ZFGeRt8CGgLd&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=2&folt=cit)
*C Lv,C Shen,M Xie*

Main category: Xuanhe Zhou

TL;DR: LLMs与众包协作日益紧密，传统众包主要承担数据收集和标注等基础任务，而LLMs处理复杂任务能力显著提升，能够向众包平台提供反馈


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在NLP领域的广泛应用，其与众包协作关系日益密切，传统众包模式主要处理基础任务，而LLMs技术快速发展使其处理复杂任务能力大幅提升，需要探索LLMs如何向众包平台提供反馈的新协作模式

Method: 论文未提供具体方法细节，但从摘要可推测研究可能涉及分析LLMs与众包协作的新模式，探讨LLMs如何向众包平台提供反馈机制，以及这种协作对任务处理效率和质量的影响

Result: 摘要未提供具体实验结果，但暗示LLMs处理复杂任务能力显著提升，能够向众包平台提供反馈，表明LLMs与众包协作模式正在发生重要转变

Conclusion: LLMs与众包的协作关系正在深化，LLMs从单纯的任务执行者转变为能够向众包平台提供反馈的智能参与者，这种转变将重塑众包工作流程和任务分配机制

Abstract: With the widespread application of Large Language Models (LLMs) in Natural Language Processing (NLP), their collaboration with crowdsourcing is becoming increasingly close. Traditional crowdsourcing models primarily undertake foundational tasks such as data collection and annotation. However, with the rapid advancements in LLMs technology, their capabilities in handling complex tasks have significantly improved. LLMs can now feedback to crowdsourcing platforms by …

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Social Determinants of Health Prediction for ICD-9 Code with Reasoning Models](https://arxiv.org/abs/2601.09709)
*Sharim Khan,Paul Landes,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: 该研究探索使用推理模型和传统大语言模型在MIMIC-III数据集上进行医院入院多标签社会健康决定因素ICD-9代码分类，利用现有ICD-9代码预测入院情况，达到89%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 社会健康决定因素与患者预后相关，但很少在结构化数据中捕获。虽然已有研究关注从临床文本中自动提取这些标记，但在大型入院记录或纵向笔记中进行预测面临长距离依赖的挑战。

Method: 使用推理模型和传统大语言模型在MIMIC-III数据集上进行医院入院多标签社会健康决定因素ICD-9代码分类，利用现有ICD-9代码进行预测。

Result: 在入院预测中达到了89%的F1分数，发现了139个入院记录中缺失的社会健康决定因素代码。

Conclusion: 该方法能够有效识别社会健康决定因素代码，补充诊断系统对患者社会环境的了解，并提供了可复现结果的代码。

Abstract: Social Determinants of Health correlate with patient outcomes but are rarely captured in structured data. Recent attention has been given to automatically extracting these markers from clinical text to supplement diagnostic systems with knowledge of patients' social circumstances. Large language models demonstrate strong performance in identifying Social Determinants of Health labels from sentences. However, prediction in large admissions or longitudinal notes is challenging given long distance dependencies. In this paper, we explore hospital admission multi-label Social Determinants of Health ICD-9 code classification on the MIMIC-III dataset using reasoning models and traditional large language models. We exploit existing ICD-9 codes for prediction on admissions, which achieved an 89% F1. Our contributions include our findings, missing SDoH codes in 139 admissions, and code to reproduce the results.

</details>


### [35] [TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models](https://arxiv.org/abs/2601.09776)
*Khalid Oublal,Quentin Bouniot,Qi Gan,Stephan Clémençon,Zeynep Akata*

Main category: cs.LG

TL;DR: TimeSAE：一个基于稀疏自编码器和因果关系的框架，用于解释时间序列黑盒模型，在分布偏移下提供更忠实和鲁棒的解释。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒模型和预训练模型在时间序列应用中的普及，理解其预测变得至关重要，特别是在需要可解释性和信任的高风险领域。现有方法大多只涉及分布内解释，无法泛化到训练支持之外，缺乏泛化能力。

Method: 提出TimeSAE框架，结合稀疏自编码器（SAEs）和因果关系的双重视角来解释时间序列黑盒模型。该方法通过稀疏自编码器学习时间序列的表示，并利用因果关系分析来提供更鲁棒的解释。

Result: 在合成和真实世界时间序列数据集上进行了广泛评估，与领先基线方法比较。定量指标和定性分析均表明，TimeSAE提供了更忠实和鲁棒的解释。

Conclusion: TimeSAE框架能够有效解释时间序列黑盒模型，在分布偏移下表现出更好的泛化能力和解释鲁棒性，为实际应用提供了可靠的解释工具。

Abstract: As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-distribution explanation, and do not generalize outside the training support, which requires the learning capability of generalization. In this work, we aim to provide a framework to explain black-box models for time series data through the dual lenses of Sparse Autoencoders (SAEs) and causality. We show that many current explanation methods are sensitive to distributional shifts, limiting their effectiveness in real-world scenarios. Building on the concept of Sparse Autoencoder, we introduce TimeSAE, a framework for black-box model explanation. We conduct extensive evaluations of TimeSAE on both synthetic and real-world time series datasets, comparing it to leading baselines. The results, supported by both quantitative metrics and qualitative insights, show that TimeSAE provides more faithful and robust explanations. Our code is available in an easy-to-use library TimeSAE-Lib: https://anonymous.4open.science/w/TimeSAE-571D/.

</details>


### [36] [Eluder dimension: localise it!](https://arxiv.org/abs/2601.09825)
*Alireza Bakhtiari,Alex Ayoub,Samuel Robertson,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 论文建立了广义线性模型类eluder维度的下界，表明标准eluder维度分析无法获得一阶遗憾界，为此引入了eluder维度的局部化方法，改进了伯努利多臂赌博机经典结果，首次为有限时域强化学习任务提供真正的一阶遗憾界。


<details>
  <summary>Details</summary>
Motivation: 标准eluder维度分析无法获得一阶遗憾界，这限制了其在广义线性模型和强化学习中的应用。需要新的分析框架来突破这一限制。

Method: 引入eluder维度的局部化方法，通过局部化技术改进标准eluder维度分析，使其能够处理更复杂的模型类。

Result: 建立了广义线性模型类eluder维度的下界，证明了标准分析无法获得一阶遗憾界；局部化方法成功恢复了伯努利多臂赌博机的经典结果并有所改进，首次为有限时域强化学习任务提供了真正的一阶遗憾界。

Conclusion: eluder维度的局部化方法是突破标准分析限制的有效工具，能够为广义线性模型和强化学习任务提供更优的遗憾界分析。

Abstract: We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.

</details>


### [37] [A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch](https://arxiv.org/abs/2601.09831)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: cs.LG

TL;DR: 首次在去噪器训练数据分布与推理任务不匹配（先验失配）的情况下，为PnP-PGD算法提供了收敛理论证明，移除了现有理论中多个限制性且无法验证的假设。


<details>
  <summary>Details</summary>
Motivation: 现有的PnP算法理论分析通常假设去噪器在理想条件下训练，但实际应用中常出现训练数据分布与推理任务不匹配的情况（先验失配）。目前缺乏在这种实际场景下的收敛理论保证。

Method: 提出了针对先验失配条件下PnP-PGD（plug-and-play proximal gradient descent）的新收敛理论框架。通过重新分析算法收敛条件，移除了传统理论中不切实际的假设。

Result: 首次证明了在先验失配情况下PnP-PGD的收敛性，相比现有理论结果，新理论不需要多个限制性且无法验证的假设条件，更具实际应用价值。

Conclusion: 该工作填补了PnP算法在先验失配条件下的理论空白，为实际应用中更广泛的PnP算法部署提供了理论保障，推动了PnP方法在真实场景中的可靠应用。

Abstract: In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions.

</details>


### [38] [A pipeline for enabling path-specific causal fairness in observational health data](https://arxiv.org/abs/2601.09841)
*Aparajita Kashyap,Sara Matijevic,Noémie Elhadad,Steven A. Kushner,Shalmali Joshi*

Main category: cs.LG

TL;DR: 提出一个模型无关的流程，用于训练因果公平的机器学习模型，解决医疗保健中的直接和间接偏见问题


<details>
  <summary>Details</summary>
Motivation: 在医疗保健环境中部署机器学习模型时，需要确保模型不复制或加剧现有的医疗保健偏见。现有公平性定义虽多，但路径特定因果公平性能更好地考虑偏见发生的社会和医疗背景，并描述这些偏见如何在学习模型中显现。

Method: 将结构公平模型映射到观察性医疗保健设置中，创建一个通用的流程来训练因果公平模型。该流程明确考虑特定的医疗保健背景和差异来定义目标"公平"模型。利用未受公平约束的基础模型在观察性健康数据上训练，生成具有因果公平性的下游预测。

Result: 填补了两个主要空白：1) 通过解耦直接和间接偏见来源，扩展了"公平性-准确性"权衡的表征；2) 展示了如何在已知社会和医疗差异的任务中，利用在观察性健康数据上训练的基础模型生成因果公平的下游预测。

Conclusion: 提出了一个模型无关的流程，用于训练因果公平的机器学习模型，解决医疗保健中的直接和间接偏见形式。该工作考虑了特定的医疗保健背景，并将公平性考虑与准确性考虑共同呈现。

Abstract: When training machine learning (ML) models for potential deployment in a healthcare setting, it is essential to ensure that they do not replicate or exacerbate existing healthcare biases. Although many definitions of fairness exist, we focus on path-specific causal fairness, which allows us to better consider the social and medical contexts in which biases occur (e.g., direct discrimination by a clinician or model versus bias due to differential access to the healthcare system) and to characterize how these biases may appear in learned models. In this work, we map the structural fairness model to the observational healthcare setting and create a generalizable pipeline for training causally fair models. The pipeline explicitly considers specific healthcare context and disparities to define a target "fair" model. Our work fills two major gaps: first, we expand on characterizations of the "fairness-accuracy" tradeoff by detangling direct and indirect sources of bias and jointly presenting these fairness considerations alongside considerations of accuracy in the context of broadly known biases. Second, we demonstrate how a foundation model trained without fairness constraints on observational health data can be leveraged to generate causally fair downstream predictions in tasks with known social and medical disparities. This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias.

</details>


### [39] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://arxiv.org/abs/2601.09865)
*Jacob Sander,Brian Jalaian,Venkat R. Dasari*

Main category: cs.LG

TL;DR: 提出集成框架结合GPTQ量化、LoRA和专用数据蒸馏，显著减小LLM大小和复杂度，同时保持或提升任务特定性能，实现2倍内存压缩和高效推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临计算、内存和能耗挑战，需要解决任务特定数据获取、性能微调和模型压缩三个关键问题。

Method: 集成框架结合GPTQ量化、低秩适应(LoRA)和专用数据蒸馏过程，利用数据蒸馏、KL散度知识蒸馏、贝叶斯超参数优化和Muon优化器。

Result: 实现高达2倍内存压缩(如6GB模型减至3GB)，在标准LLM基准测试中表现优于单独GPTQ量化，Muon优化器显著增强微调模型在量化过程中的抗精度衰减能力。

Conclusion: 提出的集成框架有效解决了LLM在边缘设备部署的资源限制问题，通过量化、微调和蒸馏的组合方法实现了高效推理，为资源受限环境提供了实用解决方案。

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.

</details>


### [40] [The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation](https://arxiv.org/abs/2601.09926)
*Kirandeep Kaur,Vinayak Gupta,Aditya Gupta,Chirag Shah*

Main category: cs.LG

TL;DR: ProPer是一个两智能体架构，通过生成隐式维度来增强语言助手的主动性，实现个性化、及时的主动干预，显著提升响应质量和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有语言助手主要采用反应式的问答范式，需要用户明确表达需求，导致相关但未表达的需求无法得到满足。现有的主动智能体要么需要用户进一步澄清（增加负担），要么从上下文推断未来需求（常导致不必要或时机不当的干预）。

Method: 提出ProPer（Proactivity-driven Personalized agents）两智能体架构：1）维度生成智能体（DGA）：基于微调的LLM，利用显式用户数据生成多个隐式维度（用户任务相关但未被考虑的潜在方面）或知识缺口；2）使用基于质量、多样性和任务相关性的重排序器筛选维度；3）响应生成智能体（RGA）：平衡显式和隐式维度，生成具有及时主动干预的个性化响应。

Result: 在多个领域使用结构化、缺口感知的评估框架（衡量覆盖率、主动性适当性和意图对齐）进行评估。结果显示ProPer在所有领域都提高了质量分数和胜率，在单轮评估中实现高达84%的提升，在多轮交互中持续占优。

Conclusion: ProPer通过两智能体架构有效解决了现有主动智能体的局限性，能够生成高质量的个性化响应，在适当时机提供主动干预，显著提升了语言助手的主动性和实用性。

Abstract: Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.

</details>


### [41] [Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains](https://arxiv.org/abs/2601.09946)
*Chenxi Qiu*

Main category: cs.LG

TL;DR: 提出基于插值的框架优化lp-范数度量差分隐私，通过锚点优化和log-凸组合插值在细粒度连续域中实现高效隐私保护


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的度量差分隐私方法在粗粒度域中有效，但在细粒度或连续域中面临计算挑战，需要构建密集扰动矩阵并满足逐点约束，计算成本高

Method: 提出插值框架：1) 在稀疏锚点集优化扰动分布；2) 通过log-凸组合插值非锚点分布，保持度量差分隐私；3) 针对高维空间隐私违规问题，将插值过程分解为一系列一维步骤，推导修正公式；4) 探索扰动分布与隐私预算跨维度分配的联合优化

Result: 在真实世界位置数据集上的实验表明，该方法在细粒度域中提供严格的隐私保证和竞争性效用，优于基线机制

Conclusion: 提出的插值框架有效解决了细粒度连续域中度量差分隐私优化的计算挑战，通过锚点优化和修正插值实现了高效隐私保护，为高维空间应用提供了实用解决方案

Abstract: Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.
  In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.

</details>


### [42] [Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series](https://arxiv.org/abs/2601.09949)
*Griffin Kearney*

Main category: cs.LG

TL;DR: Kinematic Tokenization：一种基于优化的连续时间表示方法，通过样条重建从噪声测量中提取局部运动系数作为token，在金融时间序列上优于离散token化方法


<details>
  <summary>Details</summary>
Motivation: Transformer设计用于离散token，但许多真实世界信号是连续过程通过噪声采样观测的。离散token化（原始值、补丁、有限差分）在低信噪比环境下脆弱，特别是当下游目标施加非对称惩罚时，理性鼓励弃权

Method: 提出Kinematic Tokenization：基于优化的连续时间表示，从噪声测量中重建显式样条，并将局部样条系数（位置、速度、加速度、急动度）作为token。应用于金融时间序列数据（资产价格和交易量分布）

Result: 在多资产日度股票测试平台上，使用风险规避的非对称分类目标作为可学习性压力测试。在此目标下，多个离散基线崩溃为吸收性现金策略（清算均衡），而连续样条token维持校准的非平凡行动分布和稳定策略

Conclusion: 显式连续时间token可以改善噪声时间序列中在弃权诱导损失下的选择性决策策略的可学习性和校准性

Abstract: Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.

</details>


### [43] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 机器学习价值链存在结构性不可持续问题，数据生成者在数据从输入到模型权重再到合成输出的过程中被剥夺经济权益，大部分价值流向聚合者，创作者版税几乎为零且交易条款不透明。


<details>
  <summary>Details</summary>
Motivation: 揭示机器学习价值链中的经济不平等问题：数据生成者在数据生命周期中被系统性剥夺经济权益，这种不平等不仅影响经济福利，还威胁到当前学习算法的可持续性反馈循环。

Method: 分析73个公开数据交易案例，识别结构性缺陷（缺失溯源、不对称议价能力、非动态定价），提出公平数据价值交换（EDVEX）框架，并规划研究方向。

Result: 研究发现大部分价值流向数据聚合者，创作者版税几乎为零，交易条款普遍不透明，机器学习价值链存在三个结构性缺陷导致经济不平等。

Conclusion: 机器学习价值链存在结构性不可持续问题，需要建立公平数据价值交换框架来创建惠及所有参与者的最小化市场，并提出了具体的研究方向。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [44] [An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series Classification](https://arxiv.org/abs/2601.09971)
*Hansen He,Shuheng Li*

Main category: cs.LG

TL;DR: 探索性研究评估了将专门的时间序列编码器与冻结LLM主干结合的混合架构，发现Inception模型是唯一能持续带来性能提升的编码器架构。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类是核心机器学习问题，最近有研究尝试将大语言模型用于TSC，但现有工作主要关注将时间序列数据映射到文本域的校准策略，而时间序列编码器架构的选择尚未得到充分探索。

Method: 研究了混合架构，将专门的时间序列编码器与冻结的LLM主干结合。评估了多种编码器家族，包括Inception、卷积、残差、基于Transformer和多层感知机架构。

Result: Inception模型是唯一在集成到LLM主干时能持续产生正向性能提升的编码器架构。其他编码器家族未能一致地带来性能改善。

Conclusion: 时间序列编码器的选择对混合LLM架构有重要影响，基于Inception的模型是未来LLM驱动时间序列学习的有前景方向。

Abstract: Time series classification (TSC) is a core machine learning problem with broad applications. Recently there has been growing interest in repurposing large language models (LLMs) for TSC, motivated by their strong reasoning and generalization ability. Prior work has primarily focused on alignment strategies that explicitly map time series data into the textual domain; however, the choice of time series encoder architecture remains underexplored. In this work, we conduct an exploratory study of hybrid architectures that combine specialized time series encoders with a frozen LLM backbone. We evaluate a diverse set of encoder families, including Inception, convolutional, residual, transformer-based, and multilayer perceptron architectures, among which the Inception model is the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone. Overall, this study highlights the impact of time series encoder choice in hybrid LLM architectures and points to Inception-based models as a promising direction for future LLM-driven time series learning.

</details>


### [45] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出基于概率测度空间的上下文算子学习框架，用于最优传输问题，通过少量样本提示学习从分布对到OT映射的算子，无需推理时梯度更新。


<details>
  <summary>Details</summary>
Motivation: 传统最优传输方法需要为每个新分布对重新计算，计算成本高。本文旨在开发一个通用的上下文学习框架，能够从少量样本中学习OT映射，实现快速推理和泛化。

Method: 提出概率测度空间上的上下文算子学习框架，参数化解算子。在非参数设置中，当任务集中在低内在维度的源-目标对流形上时，建立泛化边界；在参数设置中（如高斯族），给出显式架构以精确恢复OT映射。

Result: 建立了上下文精度随提示大小、任务内在维度和模型容量的缩放定律。在参数设置中实现了精确的OT映射恢复，并在合成传输和生成建模基准上验证了框架有效性。

Conclusion: 上下文算子学习为最优传输问题提供了高效、可扩展的解决方案，能够在少量样本提示下学习OT映射，无需推理时梯度更新，具有理论和实践意义。

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


### [46] [FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems](https://arxiv.org/abs/2601.09985)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FaTRQ是一种面向远内存感知的近似最近邻搜索精炼系统，通过分层内存消除从存储中获取完整向量的需求，使用渐进距离估计器和分层残差量化，显著提升存储效率和查询吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代ANNS引擎虽然使用预建索引和向量量化加速搜索，但仍依赖昂贵的二次精炼阶段，需要从SSD等慢速存储读取完整精度向量。对于现代文本和多模态嵌入，这些读取操作已成为整个查询延迟的主要瓶颈。

Method: 1) 提出渐进距离估计器，使用从远内存流式传输的紧凑残差来精炼粗略分数；2) 当候选向量被证明不在top-k内时，精炼过程提前停止；3) 设计分层残差量化，将残差编码为三元值高效存储在远内存中；4) 在CXL Type-2设备中部署定制加速器，在本地执行低延迟精炼。

Result: FaTRQ将存储效率提升2.4倍，相比最先进的GPU ANNS系统，吞吐量提升高达9倍。

Conclusion: FaTRQ通过消除从存储中获取完整向量的需求，解决了ANNS精炼阶段的性能瓶颈，利用分层内存和定制硬件加速实现了显著的性能提升。

Abstract: Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed vector-quantized representations in fast memory. However, they still rely on a costly second-pass refinement stage that reads full-precision vectors from slower storage like SSDs. For modern text and multimodal embeddings, these reads now dominate the latency of the entire query. We propose FaTRQ, a far-memory-aware refinement system using tiered memory that eliminates the need to fetch full vectors from storage. It introduces a progressive distance estimator that refines coarse scores using compact residuals streamed from far memory. Refinement stops early once a candidate is provably outside the top-k. To support this, we propose tiered residual quantization, which encodes residuals as ternary values stored efficiently in far memory. A custom accelerator is deployed in a CXL Type-2 device to perform low-latency refinement locally. Together, FaTRQ improves the storage efficiency by 2.4$\times$ and improves the throughput by up to 9$ \times$ than SOTA GPU ANNS system.

</details>


### [47] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出混合Transformer架构，用连续深度神经ODE块替代离散中间层，通过学习到的控制信号在推理时控制生成属性


<details>
  <summary>Details</summary>
Motivation: 标准Transformer通过固定离散层处理表示，缺乏对生成属性的细粒度控制。需要一种能够在推理时连续控制生成特性的机制，同时保持计算效率

Method: 混合Transformer架构，用连续深度神经ODE块替换离散中间层。使用学习向量场F_θ(H, τ, u)，其中u是低维控制信号通过显式拼接注入。采用伴随方法实现O(1)内存训练

Result: 梯度流稳定性：零梯度爆炸/消失事件；语义控制：正/负情感控制准确率达98%/88%；连续插值：固定与自适应求解器轨迹差异仅0.068%；效率：延迟与标准离散基线相当

Conclusion: 连续深度动力学与学习控制信号为可操控语言生成提供了可行、高效的机制，自适应ODE求解器揭示了学习动力学中的几何结构

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [48] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 研究XGBoost模型在点击率预测中的时间聚合特征，比较不同窗口设计，发现滑动窗口在Avazu数据集上表现最佳，相比目标编码基线提升约0.0066-0.0082 ROC AUC。


<details>
  <summary>Details</summary>
Motivation: 研究在严格时间外划分和无前瞻特征约束下，时间聚合特征对XGBoost点击率预测模型的影响，探索不同窗口设计方案的性能差异。

Method: 使用Avazu点击率预测数据集，采用严格时间外划分和无前瞻特征约束。比较时间感知目标编码基线与添加实体历史时间聚合的模型，测试多种窗口设计：滑动窗口、事件计数窗口、间隔窗口和分桶窗口。

Result: 滑动窗口相比目标编码基线提升ROC AUC约0.0066-0.0082，PR AUC约0.0084-0.0094。事件计数窗口提供微小但一致的改进，间隔窗口和分桶窗口表现不如简单滑动窗口。

Conclusion: 滑动窗口可作为实用默认选择，当边际ROC AUC增益重要时可考虑事件计数窗口。在Avazu数据集和该协议下，复杂窗口设计（间隔窗口和分桶窗口）表现不如简单滑动窗口。

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [49] [BPE: Behavioral Profiling Ensemble](https://arxiv.org/abs/2601.10024)
*Yanxin Liu,Yunqi Zhang*

Main category: cs.LG

TL;DR: 提出行为画像集成（BPE）框架，通过构建模型内在行为画像，根据测试实例响应与行为画像的偏差计算集成权重，超越传统基于模型间差异的集成方法。


<details>
  <summary>Details</summary>
Motivation: 传统静态集成方法（如Stacking）将每个基学习器视为整体分配权重，忽略了模型在不同实例空间区域的能力差异。动态集成选择（DES）虽然考虑了区域能力差异，但传统方法主要依赖模型间的差异作为集成基础，这种模型间视角忽略了模型本身的内在特性，且严重依赖验证集进行能力估计。

Method: 提出行为画像集成（BPE）框架，为每个模型构建内在的"行为画像"，基于模型对特定测试实例的响应与其已建立的行为画像之间的偏差来推导集成权重。该方法从模型内在特性出发，而非依赖模型间差异。

Result: 在合成和真实世界数据集上的大量实验表明，基于BPE框架的算法相比最先进的集成基线方法取得了显著改进。这些改进不仅体现在预测准确性上，还在计算效率和存储资源利用方面表现出优势。

Conclusion: BPE框架通过引入基于模型内在行为画像的集成范式，克服了传统集成方法的局限性，在准确性、效率和资源利用方面均优于现有方法。

Abstract: Ensemble learning is widely recognized as a pivotal strategy for pushing the boundaries of predictive performance. Traditional static ensemble methods, such as Stacking, typically assign weights by treating each base learner as a holistic entity, thereby overlooking the fact that individual models exhibit varying degrees of competence across different regions of the instance space. To address this limitation, Dynamic Ensemble Selection (DES) was introduced. However, both static and dynamic approaches predominantly rely on the divergence among different models as the basis for integration. This inter-model perspective neglects the intrinsic characteristics of the models themselves and necessitates a heavy reliance on validation sets for competence estimation. In this paper, we propose the Behavioral Profiling Ensemble (BPE) framework, which introduces a novel paradigm shift. Unlike traditional methods, BPE constructs a ``behavioral profile'' intrinsic to each model and derives integration weights based on the deviation between the model's response to a specific test instance and its established behavioral profile. Extensive experiments on both synthetic and real-world datasets demonstrate that the algorithm derived from the BPE framework achieves significant improvements over state-of-the-art ensemble baselines. These gains are evident not only in predictive accuracy but also in computational efficiency and storage resource utilization across various scenarios.

</details>


### [50] [Unlabeled Data Can Provably Enhance In-Context Learning of Transformers](https://arxiv.org/abs/2601.10058)
*Renpu Liu,Jing Yang*

Main category: cs.LG

TL;DR: 提出增强型上下文学习框架，在提示中加入少量标注示例和大量未标注数据，通过思维链提示使Transformer隐式执行EM算法，理论证明可提升ICL性能


<details>
  <summary>Details</summary>
Motivation: 传统上下文学习受限于提示中能容纳的少量标注示例，而现实中存在大量未标注数据。如何利用这些未标注数据理论保证地提升ICL性能成为一个新兴基础问题

Method: 提出增强型ICL框架，提示包含少量标注示例和未标注输入块。在多类线性分类设置下，通过思维链提示使多层Transformer隐式执行期望最大化算法，从标注和未标注数据中提取有用信息。使用教师强制训练Transformer，参数以线性速率收敛到期望解

Result: 增强型ICL框架在实验中一致优于传统少样本ICL，为理论发现提供了实证支持。Transformer能够有效模拟EM算法，实现ICL准确率的理论可证提升

Conclusion: 这是首个关于未标注数据对Transformer ICL性能影响的理论研究，证明了增强型ICL框架能够利用未标注数据提升性能，为利用大规模未标注数据改进ICL提供了理论基础

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.

</details>


### [51] [Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection](https://arxiv.org/abs/2601.10067)
*Hung Vinh Tran,Tong Chen,Hechuan Wen,Quoc Viet Hung Nguyen,Bin Cui,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出噪声感知核心集选择框架NaCS，用于内容推荐系统，通过子模优化和噪声标签校正，仅用1%数据恢复93-95%全数据性能


<details>
  <summary>Details</summary>
Motivation: 内容推荐系统需要大规模持续训练以适应多样用户偏好，计算成本高；现有核心集选择方法易受用户-物品交互噪声影响，特别是当核心集规模较小时

Method: NaCS框架：1) 基于训练梯度的子模优化构建核心集；2) 使用渐进训练模型校正噪声标签；3) 通过不确定性量化过滤低置信度样本，避免不可靠交互训练

Result: NaCS在内容推荐系统中产生更高质量核心集，效率优于现有技术；仅使用1%训练数据即可恢复93-95%全数据集训练性能

Conclusion: NaCS是专门针对内容推荐系统的噪声感知核心集选择框架，能有效处理交互噪声，显著降低训练开销同时保持模型性能

Abstract: Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommodate diverse user preferences, resulting in significant computational costs and resource demands. A promising approach to this challenge is coreset selection, which identifies a small but representative subset of data samples that preserves model quality while reducing training overhead. Yet, the selected coreset is vulnerable to the pervasive noise in user-item interactions, particularly when it is minimally sized. To this end, we propose Noise-aware Coreset Selection (NaCS), a specialized framework for CRSs. NaCS constructs coresets through submodular optimization based on training gradients, while simultaneously correcting noisy labels using a progressively trained model. Meanwhile, we refine the selected coreset by filtering out low-confidence samples through uncertainty quantification, thereby avoid training with unreliable interactions. Through extensive experiments, we show that NaCS produces higher-quality coresets for CRSs while achieving better efficiency than existing coreset selection techniques. Notably, NaCS recovers 93-95\% of full-dataset training performance using merely 1\% of the training data. The source code is available at \href{https://github.com/chenxing1999/nacs}{https://github.com/chenxing1999/nacs}.

</details>


### [52] [Comparative Evaluation of Deep Learning-Based and WHO-Informed Approaches for Sperm Morphology Assessment](https://arxiv.org/abs/2601.10070)
*Mohammad Abbadi*

Main category: cs.LG

TL;DR: 比较评估基于图像的深度学习模型(HuSHeM)与传统WHO标准增强版(WHO(+SIRI))在精子形态质量评估中的性能，发现深度学习模型在区分能力、校准和临床效用方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 精子形态质量评估是男性生育力评估的关键但主观的组成部分，常受观察者间变异性和资源限制的影响，需要更客观、可重复的评估方法。

Method: 提出比较性生物医学人工智能框架，评估基于高分辨率精子形态图像训练的深度学习模型(HuSHeM)与基于WHO标准增强系统性炎症反应指数(SIRI)的临床基线方法(WHO(+SIRI))。使用独立临床队列评估模型性能，包括区分能力分析、校准分析和临床效用分析。

Result: HuSHeM模型表现出更高的区分性能(ROC曲线下面积更大且置信区间更窄)，在类别不平衡情况下精度-召回分析显示性能改善，校准分析显示预测概率与观察结果更一致，决策曲线分析表明在临床相关阈值概率范围内具有更大的净临床效益。

Conclusion: 基于图像的深度学习相比传统的基于规则和炎症增强的标准，可能提供改进的预测可靠性和临床效用。该框架支持精子形态的客观和可重复评估，可作为生育筛查和转诊工作流程中的决策支持工具，但并非旨在替代临床判断或实验室评估。

Abstract: Assessment of sperm morphological quality remains a critical yet subjective component of male fertility evaluation, often limited by inter-observer variability and resource constraints. This study presents a comparative biomedical artificial intelligence framework evaluating an image-based deep learning model (HuSHeM) alongside a clinically grounded baseline derived from World Health Organization criteria augmented with the Systemic Inflammation Response Index (WHO(+SIRI)).
  The HuSHeM model was trained on high-resolution sperm morphology images and evaluated using an independent clinical cohort. Model performance was assessed using discrimination, calibration, and clinical utility analyses. The HuSHeM model demonstrated higher discriminative performance, as reflected by an increased area under the receiver operating characteristic curve with relatively narrow confidence intervals compared to WHO(+SIRI). Precision-recall analysis further indicated improved performance under class imbalance, with higher precision-recall area values across evaluated thresholds. Calibration analysis indicated closer agreement between predicted probabilities and observed outcomes for HuSHeM, while decision curve analysis suggested greater net clinical benefit across clinically relevant threshold probabilities.
  These findings suggest that image-based deep learning may offer improved predictive reliability and clinical utility compared with traditional rule-based and inflammation-augmented criteria. The proposed framework supports objective and reproducible assessment of sperm morphology and may serve as a decision-support tool within fertility screening and referral workflows. The proposed models are intended as decision-support or referral tools and are not designed to replace clinical judgment or laboratory assessment.

</details>


### [53] [Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts](https://arxiv.org/abs/2601.10079)
*Sijia Luo,Xiaokang Zhang,Yuxuan Hu,Bohan Zhang,Ke Wang,Jinbo Su,Mengshu Sun,Lei Liang,Jing Zhang*

Main category: cs.LG

TL;DR: Sparse-RL：一种通过稀疏化KV缓存来降低强化学习训练内存开销的方法，同时保持性能稳定


<details>
  <summary>Details</summary>
Motivation: 大型语言模型强化学习训练中，长序列推理产生的KV缓存占用大量内存，成为硬件受限环境下的关键瓶颈。现有KV压缩技术虽可用于推理，但直接应用于RL训练会导致策略失配和性能崩溃。

Method: 提出Sparse-RL框架，通过稀疏化采样策略减少KV缓存开销。引入稀疏感知拒绝采样和基于重要性的重加权方法，校正压缩导致的信息损失带来的离策略偏差。解决密集旧策略、稀疏采样策略和学习器策略之间的策略失配问题。

Result: 实验表明Sparse-RL相比密集基线显著降低rollout开销，同时保持性能不变。该方法还实现了稀疏感知训练，显著增强了模型在稀疏推理部署时的鲁棒性。

Conclusion: Sparse-RL为硬件受限环境下的大语言模型强化学习训练提供了有效的解决方案，通过稀疏化KV缓存降低内存需求，同时通过偏差校正机制保持训练稳定性，为稀疏推理部署提供了更好的模型适应性。

Abstract: Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.

</details>


### [54] [Bayesian Meta-Analyses Could Be More: A Case Study in Trial of Labor After a Cesarean-section Outcomes and Complications](https://arxiv.org/abs/2601.10089)
*Ashley Klein,Edward Raff,Marcia DesJardin*

Main category: cs.LG

TL;DR: 提出贝叶斯方法解决医学研究中关键决策变量未被记录导致的元分析不可靠问题，并以剖宫产后试产评估为例验证方法实用性


<details>
  <summary>Details</summary>
Motivation: 医学研究中关键决策变量未被记录导致元分析结果不可靠，需要开发新方法在信息不完整情况下评估治疗效果

Method: 构建贝叶斯分析方法处理医学研究中关键变量缺失问题，应用于剖宫产后试产评估场景，协助产科医生在干预有限情况下做出决策

Result: 开发出实用的贝叶斯分析框架，能够在不完整数据下评估治疗效果，为医生提供支持以推进患者护理

Conclusion: 贝叶斯方法能有效解决医学研究中关键变量缺失问题，提高元分析可靠性，在剖宫产后试产等临床决策中具有实用价值

Abstract: The meta-analysis's utility is dependent on previous studies having accurately captured the variables of interest, but in medical studies, a key decision variable that impacts a physician's decisions was not captured. This results in an unknown effect size and unreliable conclusions. A Bayesian approach may allow analysis to determine if the claim of a positive effect is still warranted, and we build a Bayesian approach to this common medical scenario. To demonstrate its utility, we assist professional OBGYNs in evaluating Trial of Labor After a Cesarean-section (TOLAC) situations where few interventions are available for patients and find the support needed for physicians to advance patient care.

</details>


### [55] [LeMoF: Level-guided Multimodal Fusion for Heterogeneous Clinical Data](https://arxiv.org/abs/2601.10092)
*Jongseok Kim,Seongae Kang,Jonghwan Shin,Yuhan Lee,Ohyun Jo*

Main category: cs.LG

TL;DR: 提出LeMoF框架，通过层级引导的模态融合策略，在临床多模态预测中实现平衡的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有多模态临床预测方法依赖静态模态集成方案和简单融合策略，未能充分利用模态特定表示，在异构临床环境中性能受限

Method: 提出LeMoF框架，选择性整合每个模态内的层级引导表示，明确分离并学习全局模态级预测和层级特定判别表示

Result: 在ICU住院时长预测任务中，LeMoF在各种编码器配置下均优于现有最先进的多模态融合技术，层级集成是实现稳健预测性能的关键因素

Conclusion: LeMoF框架能够在异构临床环境中实现预测稳定性与判别能力之间的平衡，层级引导的模态融合是提升多模态临床预测性能的有效方法

Abstract: Multimodal clinical prediction is widely used to integrate heterogeneous data such as Electronic Health Records (EHR) and biosignals. However, existing methods tend to rely on static modality integration schemes and simple fusion strategies. As a result, they fail to fully exploit modality-specific representations. In this paper, we propose Level-guided Modal Fusion (LeMoF), a novel framework that selectively integrates level-guided representations within each modality. Each level refers to a representation extracted from a different layer of the encoder. LeMoF explicitly separates and learns global modality-level predictions from level-specific discriminative representations. This design enables LeMoF to achieve a balanced performance between prediction stability and discriminative capability even in heterogeneous clinical environments. Experiments on length of stay prediction using Intensive Care Unit (ICU) data demonstrate that LeMoF consistently outperforms existing state-of-the-art multimodal fusion techniques across various encoder configurations. We also confirmed that level-wise integration is a key factor in achieving robust predictive performance across various clinical conditions.

</details>


### [56] [Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text](https://arxiv.org/abs/2601.10096)
*Piyush Singh Pasi*

Main category: cs.LG

TL;DR: METAL：一种轻量级对齐方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间，实现强大的零样本跨语言迁移。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在英语上表现出色，但在其他语言上性能急剧下降，因为缺乏多语言多模态资源。现有解决方案严重依赖机器翻译，而多语言文本建模的进展未得到充分利用。

Method: 提出METAL方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间。该方法简单轻量，通过线性变换实现跨语言对齐。

Result: METAL在英语上达到基线性能（94.9% Recall@10），在11种语言（10种未见语言）上实现强大的零样本迁移（平均89.5% Recall@10）。t-SNE可视化显示多语言嵌入与多模态表示紧密对齐，权重分析表明变换重塑了嵌入几何而非简单旋转。

Conclusion: METAL证明了通过简单线性变换实现多语言多模态对齐的有效性，可推广到音频-文本检索和跨语言文本到图像生成。作者发布了代码、检查点和多语言评估数据集以促进进一步研究。

Abstract: Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.

</details>


### [57] [Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation](https://arxiv.org/abs/2601.10137)
*Ziyi Ding,Chenfei Ye-Hao,Zheyuan Wang,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: Tree-Query：基于LLM的树状多专家框架，通过结构化查询序列实现无数据因果发现，提供可解释判断和鲁棒性感知置信度


<details>
  <summary>Details</summary>
Motivation: 传统约束性因果发现方法（如PC、FCI）存在误差传播问题，而近期基于LLM的因果预言机往往表现为不透明、无置信度的黑盒模型。需要一种既能利用LLM知识又能提供可解释、有置信度判断的因果发现方法。

Method: 提出Tree-Query框架，将成对因果发现转化为关于后门路径、(不)依赖性、潜在混杂和因果方向的短序列查询。采用树状结构的多专家LLM设计，每个专家负责特定因果推理任务，形成结构化决策流程。

Result: 在基于Mooij等人和UCI因果图的无数据基准测试中，Tree-Query在结构指标上优于直接LLM基线。饮食-体重案例研究展示了混杂因素筛选和稳定、高置信度的因果结论。理论保证提供了四种成对关系的渐近可识别性。

Conclusion: Tree-Query提供了一种原则性方法，从LLM中获取无数据因果先验，可补充下游数据驱动的因果发现。框架提供可解释判断和鲁棒性感知置信度评分，为因果发现提供了透明且可靠的解决方案。

Abstract: Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.

</details>


### [58] [Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141)
*Jiawen Zhang,Yangfan Hu,Kejia Chen,Lipeng He,Jiachen Ma,Jian Lou,Dan Li,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 提出SPF方法解决LLM微调中的安全-效用困境，通过几何分析发现安全梯度位于低秩子空间而效用梯度在高维空间，通过移除与安全子空间冲突的梯度分量来保持安全对齐。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调会显著降低安全对齐性，即使微调数据无害也会增加越狱攻击的脆弱性。现有方法面临安全-效用困境：强调安全会损害任务性能，优先效用则需深度微调导致安全大幅下降。

Method: 提出安全保持微调(SPF)：1)通过系统实证分析发现安全梯度位于低秩子空间，效用梯度位于高维空间，两者子空间负相关；2)从单样本高效估计主导安全方向；3)显式移除与低秩安全子空间冲突的梯度分量。

Result: 理论上证明SPF保证效用收敛同时限制安全漂移。实证显示SPF能保持下游任务性能，恢复几乎所有预训练安全对齐，即使在对抗性微调场景下也有效。对深度微调和动态越狱攻击具有鲁棒抵抗性。

Conclusion: SPF为始终对齐的LLM微调提供了新的机制理解和实践指导，解决了安全-效用困境，实现了安全保持的轻量级微调方法。

Abstract: Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination.
  In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.

</details>


### [59] [LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers](https://arxiv.org/abs/2601.10155)
*Aryan Karmore*

Main category: cs.LG

TL;DR: LOOKAT：一种基于乘积量化和非对称距离计算的KV缓存压缩方法，将注意力计算从内存受限转为计算受限，实现64倍压缩且保持95.7%输出保真度


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存量化方法仅压缩存储但无法减少带宽，因为注意力计算需要将INT4/INT8键反量化为FP16。需要借鉴向量数据库的压缩技术来更好地压缩KV缓存。

Method: 将乘积量化和非对称距离计算应用于Transformer架构：1）将键向量分解为子空间；2）学习码本；3）通过查找表计算注意力表。将注意力从内存受限转为计算受限。

Result: 在GPT-2上测试：64倍压缩达到95.7%输出保真度，32倍压缩达到95.0%保真度。无需架构更改或训练，保持秩相关ρ>0.95。理论分析确认秩相关退化与d_k/mK成正比。

Conclusion: LOOKAT成功将向量数据库压缩技术应用于Transformer KV缓存，显著减少带宽需求，实现高压缩比同时保持输出质量，为边缘设备部署大语言模型提供了有效解决方案。

Abstract: Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\times$ compression at 95.7\% output fidelity and 32 $\times$ compression at 95.0\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $ρ> 0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.

</details>


### [60] [CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling](https://arxiv.org/abs/2601.10176)
*Mingyu Zhao,Haoran Bai,Yu Tian,Bing Zhu,Hengliang Luo*

Main category: cs.LG

TL;DR: 提出CC-OR-Net框架，通过结构分解解决LTV预测中零膨胀长尾分布问题，平衡全局准确率与高价值用户精度


<details>
  <summary>Details</summary>
Motivation: LTV预测面临零膨胀长尾分布挑战：大多数低中价值用户数量上压倒少数但关键的高价值"鲸鱼"用户，且低中价值用户内部存在显著异质性。现有方法要么依赖刚性统计假设，要么通过损失约束而非架构设计来解耦排序和回归，无法平衡全局准确率与高价值精度。

Method: 提出CC-OR-Net框架，包含三个专门组件：1) 结构序数分解模块用于稳健排序；2) 桶内残差模块用于细粒度回归；3) 定向高价值增强模块用于顶级用户精度。通过结构分解实现架构保证的排序解耦。

Result: 在包含超过3亿用户的真实数据集上评估，CC-OR-Net在所有关键业务指标上实现了更优的权衡，超越了现有最先进方法，提供了全面且商业价值高的LTV预测解决方案。

Conclusion: CC-OR-Net通过结构分解方法有效解决了LTV预测中的零膨胀长尾分布问题，实现了排序和回归的稳健解耦，为现代营销中的LTV预测提供了更优的解决方案。

Abstract: Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value "whale" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \textbf{C}onditional \textbf{C}ascaded \textbf{O}rdinal-\textbf{R}esidual Networks \textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \textit{structural ordinal decomposition module} for robust ranking, an \textit{intra-bucket residual module} for fine-grained regression, and a \textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.

</details>


### [61] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: 提出图正则化PCA（GR-PCA），通过图拉普拉斯正则化处理非各向同性噪声的高维数据，提高主成分的结构保真度


<details>
  <summary>Details</summary>
Motivation: 高维数据中变量间的依赖关系常常违反PCA所假设的各向同性噪声条件。当噪声在特征间非独立同分布（协方差非球形）时，传统PCA不再最优，需要能够利用数据特征依赖结构的降维方法

Method: 提出图正则化PCA（GR-PCA），通过图拉普拉斯正则化将载荷偏向图拉普拉斯的低频傅里叶模式，抑制高频信号，保留图一致的低频信号。方法包括学习稀疏精度图，并利用图拉普拉斯惩罚项实现正则化

Result: 在多种图拓扑、信噪比和稀疏度水平的合成数据上评估，GR-PCA能将方差集中在预期支撑集上，产生更低图拉普拉斯能量的载荷，在样本外重建中保持竞争力。当存在高频信号时，图拉普拉斯惩罚防止过拟合，降低重建精度但提高结构保真度

Conclusion: GR-PCA为结构感知的降维提供了实用途径，在不牺牲预测性能的情况下提高结构保真度。当高频信号图相关时，相比PCA优势最明显；而当高频信号接近旋转不变时，PCA仍具竞争力。方法实现简单、模块化且可扩展

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


### [62] [PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary](https://arxiv.org/abs/2601.10201)
*Jiarui Yao,Ruida Wang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出Process Reward Learning (PRL)框架，将熵正则化强化学习目标分解为中间步骤，为大型语言模型推理过程提供细粒度监督，无需额外复杂步骤如MCTS或单独奖励模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要基于轨迹级结果奖励，缺乏推理过程中的细粒度监督；其他结合过程信号的训练框架依赖MCTS、单独奖励模型等额外步骤，训练效率低；过程信号设计缺乏严格理论支持，优化机制不透明。

Method: 提出Process Reward Learning (PRL)，将熵正则化强化学习目标分解为中间步骤，推导出等价于奖励最大化加策略模型与参考模型间KL散度惩罚项的形式，将结果奖励转化为过程监督信号指导RL优化探索。

Result: PRL不仅提高了LLM推理能力的平均性能（average @ n），还通过改进pass @ n指标扩展了推理边界；大量实验验证了PRL的有效性和泛化能力。

Conclusion: PRL为LLM推理过程提供理论支持的细粒度监督框架，无需复杂额外步骤，显著提升训练效率和推理性能，扩展了模型的推理能力边界。

Abstract: Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.

</details>


### [63] [Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD](https://arxiv.org/abs/2601.10237)
*Murat Bilgehan Ertan,Marten van Dijk*

Main category: cs.LG

TL;DR: DP-SGD在f-差分隐私框架下存在基本限制：在单轮洗牌采样中，实现强隐私需要高噪声水平，导致实用性显著下降，无法同时实现强隐私和高实用性。


<details>
  <summary>Details</summary>
Motivation: DP-SGD是私有训练的主流方法，但其在最坏情况对抗性隐私定义下的基本限制尚未被充分理解。研究者旨在分析DP-SGD在f-差分隐私框架下的隐私-实用性权衡，揭示其根本局限性。

Method: 在f-差分隐私框架下分析DP-SGD，研究单轮洗牌采样（M次梯度更新）。推导可实现权衡曲线的显式次优上界，进而得到分离度κ的几何下界。分析高斯噪声乘子σ与分离度κ之间的严格下界关系，并扩展到泊松子采样。

Result: 证明在最坏情况对抗模型下，洗牌DP-SGD必须满足σ ≥ 1/√(2lnM)或κ ≥ (1/√8)(1-1/√(4πlnM))，无法同时实现强隐私和高实用性。该界限随M→∞渐近消失但收敛极慢，实际训练中所需噪声水平仍很高。实验证实该噪声水平导致显著精度下降。

Conclusion: DP-SGD在标准最坏情况对抗假设下存在关键瓶颈：强隐私要求与高实用性之间存在根本性权衡，无法同时实现。即使使用大量更新，所需噪声水平仍会显著降低模型精度。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy
  $σ\ge \frac{1}{\sqrt{2\ln M}}$ $\quad\text{or}\quad$ $κ\ge\ \frac{1}{\sqrt{8}}\!\left(1-\frac{1}{\sqrt{4π\ln M}}\right)$,
  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \to \infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.

</details>


### [64] [X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction](https://arxiv.org/abs/2601.10251)
*Hongru Duan,Yongle Chen,Lei Guan*

Main category: cs.LG

TL;DR: 提出X-SAM方法，通过沿Hessian主特征向量正交分解修正梯度，更直接有效地正则化最大特征值，解决SAM在训练中可能仍指向尖锐区域的问题。


<details>
  <summary>Details</summary>
Motivation: SAM旨在通过最小化模型参数小邻域内的最坏扰动损失来改善泛化，但训练中其优化行为并不总是符合理论预期，因为尖锐和平坦区域都可能产生小的扰动损失。此时梯度仍可能指向尖锐区域，未能实现SAM的预期效果。

Method: 从谱和几何角度分析SAM，利用梯度与Hessian主特征向量之间的夹角作为尖锐度度量。提出显式特征向量对齐的SAM（X-SAM），通过沿主特征向量正交分解修正梯度，实现对Hessian最大特征值更直接有效的正则化。

Result: 理论证明了X-SAM的收敛性和优越泛化性能，并通过大量实验评估确认了理论和实践优势。

Conclusion: X-SAM通过几何修正解决了SAM可能仍指向尖锐区域的问题，提供了更直接有效的尖锐度正则化方法，在理论和实验上均表现出优越性能。

Abstract: Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.

</details>


### [65] [Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders](https://arxiv.org/abs/2601.10269)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 提出无监督涡扇发动机健康监测框架，无需故障标签，通过回归归一化消除工况影响，LSTM自编码器仅用健康数据训练，自适应阈值触发实时警报


<details>
  <summary>Details</summary>
Motivation: 现有基于剩余使用寿命(RUL)的监测方法需要故障标签，部署成本高且难以适应不同机队。需要无需标签、能快速部署、适应多种工况的无监督监测框架

Method: 1) 回归归一化消除NASA CMAPSS传感器数据中的工况影响；2) 仅用每个轨迹的健康部分训练LSTM自编码器；3) 使用自适应数据驱动阈值估计持续重构误差，触发实时警报

Result: 基准测试显示高召回率和低误报率，适用于多种运行工况。方法可快速部署、适应不同机队规模，可作为RUL模型的补充预警层

Conclusion: 提出的无监督框架有效实现了涡扇发动机健康监测，无需故障标签，通过自适应阈值实现实时预警，可作为现有RUL模型的补充方案

Abstract: This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.

</details>


### [66] [Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers](https://arxiv.org/abs/2601.10274)
*Emre Ozbas,Melih Bastopcu*

Main category: cs.LG

TL;DR: 论文研究LLM服务器中异构查询任务的令牌分配优化问题，在精度-延迟权衡和队列稳定性约束下，通过优化令牌分配来最大化加权精度并最小化平均系统时间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务器需要处理多种类型的查询任务，每种任务需要不同的计算资源（令牌）。如何在不同任务类型之间分配有限的令牌资源，以平衡响应精度和服务延迟，是一个重要的系统优化问题。

Method: 将系统建模为M/G/1队列，服务时间与分配的令牌数呈近似仿射关系。建立约束优化问题，最大化加权平均精度并惩罚平均系统时间，受限于令牌预算和队列稳定性条件。证明目标函数在稳定区域内严格凹，提出迭代解法和投影梯度法。

Result: 证明了最优令牌分配的存在性和唯一性，推导出一阶最优条件形成耦合投影定点特征。开发了具有可计算全局步长界的投影梯度法，保证在非压缩区域收敛。通过舍入连续解得到整数令牌分配，仿真评估了性能损失。

Conclusion: 该研究为LLM服务器中的异构查询任务提供了系统的令牌分配优化框架，通过数学建模和优化算法实现了精度与延迟的有效权衡，为实际系统部署提供了理论指导。

Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.

</details>


### [67] [SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks](https://arxiv.org/abs/2601.10282)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TL;DR: SPIKE框架通过连续时间Koopman算子正则化PINNs，学习稀疏动力学表示，提升微分方程求解的泛化能力和长期预测精度。


<details>
  <summary>Details</summary>
Motivation: PINNs在训练域内容易过拟合，在时空区域外泛化能力差，需要改进其外推和长期预测能力。

Method: 提出SPIKE框架，通过连续时间Koopman算子正则化PINNs，在学习的可观测空间中强制线性动力学dz/dt=Az，并使用L1正则化获得稀疏生成矩阵。

Result: 在抛物线、双曲线、色散和刚性PDEs（包括Navier-Stokes流体动力学）以及混沌ODEs（Lorenz系统）上实验，显示在时间外推、空间泛化和长期预测精度方面的一致改进。

Conclusion: SPIKE通过连续时间Koopman算子和稀疏正则化，有效解决了PINNs的过拟合问题，提升了复杂动力系统的建模和预测能力。

Abstract: Physics-Informed Neural Networks (PINNs) provide a mesh-free approach for solving differential equations by embedding physical constraints into neural network training. However, PINNs tend to overfit within the training domain, leading to poor generalization when extrapolating beyond trained spatiotemporal regions. This work presents SPIKE (Sparse Physics-Informed Koopman-Enhanced), a framework that regularizes PINNs with continuous-time Koopman operators to learn parsimonious dynamics representations. By enforcing linear dynamics $dz/dt = Az$ in a learned observable space, both PIKE (without explicit sparsity) and SPIKE (with L1 regularization on $A$) learn sparse generator matrices, embodying the parsimony principle that complex dynamics admit low-dimensional structure. Experiments across parabolic, hyperbolic, dispersive, and stiff PDEs, including fluid dynamics (Navier-Stokes) and chaotic ODEs (Lorenz), demonstrate consistent improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy. The continuous-time formulation with matrix exponential integration provides unconditional stability for stiff systems while avoiding diagonal dominance issues inherent in discrete-time Koopman operators.

</details>


### [68] [We Need a More Robust Classifier: Dual Causal Learning Empowers Domain-Incremental Time Series Classification](https://arxiv.org/abs/2601.10312)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Haodong Jing,Mingyang Geng,Yongsheng Huang,Jialu Xu,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 提出轻量级双因果解耦框架DualCD，通过时间特征解耦和双因果干预机制提升时间序列分类在域增量学习中的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类研究在域增量学习方面面临挑战，需要增强模型在域增量场景下的鲁棒性

Method: 提出DualCD框架：1) 时间特征解耦模块分离类因果特征和伪特征；2) 双因果干预机制消除类内和类间混杂特征影响，通过构造变体样本和因果干预损失训练模型

Result: 在多个数据集和模型上的实验表明，DualCD有效提升了域增量场景下的性能，并建立了全面的基准测试

Conclusion: DualCD框架能够无缝集成到时间序列分类模型中，通过因果解耦和干预机制显著增强域增量学习能力

Abstract: The World Wide Web thrives on intelligent services that rely on accurate time series classification, which has recently witnessed significant progress driven by advances in deep learning. However, existing studies face challenges in domain incremental learning. In this paper, we propose a lightweight and robust dual-causal disentanglement framework (DualCD) to enhance the robustness of models under domain incremental scenarios, which can be seamlessly integrated into time series classification models. Specifically, DualCD first introduces a temporal feature disentanglement module to capture class-causal features and spurious features. The causal features can offer sufficient predictive power to support the classifier in domain incremental learning settings. To accurately capture these causal features, we further design a dual-causal intervention mechanism to eliminate the influence of both intra-class and inter-class confounding features. This mechanism constructs variant samples by combining the current class's causal features with intra-class spurious features and with causal features from other classes. The causal intervention loss encourages the model to accurately predict the labels of these variant samples based solely on the causal features. Extensive experiments on multiple datasets and models demonstrate that DualCD effectively improves performance in domain incremental scenarios. We summarize our rich experiments into a comprehensive benchmark to facilitate research in domain incremental time series classification.

</details>


### [69] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: 提出Strategy-aware Surprise (SuS)框架，使用前后预测不匹配作为强化学习探索的新颖性信号，在数学推理任务上显著提升准确性和解决方案多样性。


<details>
  <summary>Details</summary>
Motivation: 传统基于好奇心的探索方法仅依赖状态预测误差，缺乏对智能体行为策略一致性和策略相关意外结果的考虑。需要一种更全面的内在动机框架来提升探索效率和解决方案多样性。

Method: 提出SuS框架，包含两个互补组件：策略稳定性（SS）衡量跨时间步行为策略的一致性；策略惊喜（SuS）捕捉相对于智能体当前策略表示的意外结果。通过学习的权重系数将两个信号结合到奖励公式中。

Result: 在大型语言模型的数学推理任务上评估，相比基线方法，Pass@1提升17.4%，Pass@5提升26.4%。消融研究表明移除任一组件会导致至少10%的性能下降，验证了方法的协同效应。

Conclusion: SuS框架通过整合策略稳定性和策略惊喜信号，有效提升了强化学习中的探索效率和解决方案多样性，为内在动机方法提供了新思路。

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [70] [EvoMorph: Counterfactual Explanations for Continuous Time-Series Extrinsic Regression Applied to Photoplethysmography](https://arxiv.org/abs/2601.10356)
*Mesut Ceylan,Alexis Tabin,Patrick Langer,Elgar Fleisch,Filipe Barata*

Main category: cs.LG

TL;DR: EvoMorph：一种用于时间序列外生回归的多目标进化框架，可生成生理学上合理的反事实解释，支持临床时间序列应用的可信模型分析。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备实现了生理信号的连续监测，但现有反事实解释方法主要限于分类任务，忽视波形形态，且常产生生理学上不合理的信号，限制了其在连续生物医学时间序列中的应用。

Method: 提出EvoMorph多目标进化框架，优化基于可解释信号描述符的形态感知目标，应用变换以保持波形结构，生成生理学上合理且多样化的反事实解释。

Result: 在三个PPG数据集（心率、呼吸频率、血氧饱和度）上评估EvoMorph，相比最近非相似邻居基线表现更优；案例研究表明其可作为不确定性量化工具，将反事实敏感性与bootstrap集成不确定性和数据密度测量相关联。

Conclusion: EvoMorph能够为连续生物医学信号生成生理学感知的反事实解释，支持不确定性感知的可解释性，推进临床时间序列应用的可信模型分析。

Abstract: Wearable devices enable continuous, population-scale monitoring of physiological signals, such as photoplethysmography (PPG), creating new opportunities for data-driven clinical assessment. Time-series extrinsic regression (TSER) models increasingly leverage PPG signals to estimate clinically relevant outcomes, including heart rate, respiratory rate, and oxygen saturation. For clinical reasoning and trust, however, single point estimates alone are insufficient: clinicians must also understand whether predictions are stable under physiologically plausible variations and to what extent realistic, attainable changes in physiological signals would meaningfully alter a model's prediction. Counterfactual explanations (CFE) address these "what-if" questions, yet existing time series CFE generation methods are largely restricted to classification, overlook waveform morphology, and often produce physiologically implausible signals, limiting their applicability to continuous biomedical time series. To address these limitations, we introduce EvoMorph, a multi-objective evolutionary framework for generating physiologically plausible and diverse CFE for TSER applications. EvoMorph optimizes morphology-aware objectives defined on interpretable signal descriptors and applies transformations to preserve the waveform structure. We evaluated EvoMorph on three PPG datasets (heart rate, respiratory rate, and oxygen saturation) against a nearest-unlike-neighbor baseline. In addition, in a case study, we evaluated EvoMorph as a tool for uncertainty quantification by relating counterfactual sensitivity to bootstrap-ensemble uncertainty and data-density measures. Overall, EvoMorph enables the generation of physiologically-aware counterfactuals for continuous biomedical signals and supports uncertainty-aware interpretability, advancing trustworthy model analysis for clinical time-series applications.

</details>


### [71] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 研究具有多步前瞻信息的表格强化学习问题，提出自适应批处理策略（ABP）及其学习算法，获得最优遗憾界


<details>
  <summary>Details</summary>
Motivation: 现有处理前瞻信息的两种启发式方法（固定批处理策略和模型预测控制）存在问题，需要更优的策略利用前瞻信息

Method: 提出自适应批处理策略（ABP），推导其最优贝尔曼方程，设计乐观遗憾最小化算法学习未知环境中的最优ABP

Result: 获得阶最优的遗憾界（最多相差前瞻视野ℓ的因子），ℓ通常可视为小常数

Conclusion: 自适应批处理策略能有效利用前瞻信息，其学习算法在理论保证下优于现有启发式方法

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [72] [DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction](https://arxiv.org/abs/2601.10471)
*Zhancun Mu*

Main category: cs.LG

TL;DR: DeFlow提出解耦的离线强化学习框架，利用流匹配技术捕捉复杂行为流形，通过轻量级精炼模块避免ODE求解器反向传播的计算负担，在保持流迭代表达能力的同时实现稳定改进。


<details>
  <summary>Details</summary>
Motivation: 生成式策略优化通常需要通过ODE求解器进行反向传播，计算成本高昂。现有方法要么牺牲迭代生成能力进行单步蒸馏，要么面临求解器微分和损失项平衡的挑战。

Method: DeFlow采用解耦框架：1) 使用流匹配技术捕捉复杂行为流形；2) 在流形显式信任区域内学习轻量级精炼模块；3) 避免求解器微分，无需平衡损失项；4) 完全保持流的迭代表达能力。

Result: 在挑战性的OGBench基准测试中取得优越性能，并展示了高效的离线到在线适应能力。

Conclusion: DeFlow通过解耦设计和流匹配技术，在避免计算负担的同时保持了生成式策略的迭代表达能力，为离线强化学习提供了高效稳定的解决方案。

Abstract: We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.

</details>


### [73] [Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning](https://arxiv.org/abs/2601.10498)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: PROMA是一种用于大语言模型微调的近端策略更新方法，通过投影掉序列级梯度分量并在微批次间累积梯度，实现更稳定的策略学习


<details>
  <summary>Details</summary>
Motivation: 现有策略优化方法如PPO和GRPO存在熵崩溃、依赖参考策略或似然比裁剪等问题，需要一种更稳定、高效的近端策略更新方法

Method: PROMA在反向传播过程中逐层投影掉序列级梯度分量，然后在微批次间累积策略梯度，无需额外前向或反向传播

Result: 相比GRPO，PROMA能更严格地控制局部KL散度，实现更稳定的策略学习，且不会导致熵崩溃，也不依赖参考策略或似然比裁剪

Conclusion: PROMA是一种有效的近端策略更新方法，解决了现有方法在稳定性和效率方面的问题，为大语言模型微调提供了新方案

Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.

</details>


### [74] [Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models](https://arxiv.org/abs/2601.10519)
*Andrea Melis,Andrea Piroddi,Roberto Girau*

Main category: cs.LG

TL;DR: 使用GPT-2 Transformer模型生成新型调制方案，相比传统方法在SNR和PSD等性能指标上表现相当或更优，可提升认知无线电系统的频谱效率、鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 认知无线电系统需要动态适应变化的频谱环境，机器学习技术特别是Transformer模型可显著提升其频谱效率、鲁棒性和安全性。传统调制方案可能无法充分利用频谱资源，需要探索新的调制方案生成方法。

Method: 使用GPT-2架构的Transformer模型，在现有调制公式数据集上进行训练，生成新的调制方案。将生成的调制方案与传统方法在关键性能指标（信噪比SNR和功率谱密度PSD）上进行比较评估。

Result: Transformer生成的调制方案在性能上与传统方法相当，在某些情况下甚至优于传统方法。这表明Transformer模型能够生成有效的调制方案，为认知无线电系统提供性能改进。

Conclusion: Transformer模型在生成新型调制方案方面具有潜力，可显著提升认知无线电系统的效率、鲁棒性和安全性。先进的认知无线电系统可从Transformer模型的实施中获益，构建更高效的通信系统。

Abstract: Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.

</details>


### [75] [Mixtures of Transparent Local Models](https://arxiv.org/abs/2601.10541)
*Niffa Cheick Oumar Diaby,Thierry Duchesne,Mario Marchand*

Main category: cs.LG

TL;DR: 提出一种基于透明局部模型混合的可解释模型设计方法，通过多预测器损失函数学习透明标注函数及其适用区域，建立严格的PAC-Bayesian风险界


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在人类活动中日益普及，对模型透明度的需求不断增长。模型透明度有助于识别安全性和非歧视性等因素。需要为输入空间中不同区域可能具有不同但简单的透明函数的情况提供解决方案。

Method: 提出透明局部模型混合方法，同时学习透明标注函数和该函数在其分配区域内风险较小的输入空间区域。使用新的多预测器（多区域）损失函数，为二元线性分类和线性回归问题建立了严格的PAC-Bayesian风险界。

Result: 在合成数据集上展示了学习算法的工作原理。在真实数据集上的结果表明，该方法与其他现有方法以及某些不透明模型相比具有竞争力。

Conclusion: 透明局部模型混合方法为设计可解释模型提供了有效替代方案，特别适用于输入空间中不同区域需要不同简单透明函数的情况，在保持透明度的同时实现了良好的性能。

Abstract: The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.

</details>


### [76] [Process-Guided Concept Bottleneck Model](https://arxiv.org/abs/2601.10562)
*Reza M. Asiyabi,SEOSAW Partnership,Steven Hancock,Casey Ryan*

Main category: cs.LG

TL;DR: PG-CBM扩展概念瓶颈模型，通过领域定义的因果机制约束学习，使用生物物理意义的中介概念，提高科学应用中的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准概念瓶颈模型(CBMs)通常忽视领域特定关系和因果机制，且依赖完整概念标签，限制了在监督稀疏但过程定义明确的科学领域的应用。

Method: 提出过程引导概念瓶颈模型(PG-CBM)，通过领域定义的因果机制约束学习，使用生物物理意义的中介概念，利用多源异构训练数据。

Result: 以地球观测数据的地上生物量密度估计为案例研究，PG-CBM相比多个基准模型减少了误差和偏差，同时产生可解释的中间输出。

Conclusion: PG-CBM不仅提高准确性，还增强透明度，能够检测虚假学习，提供科学见解，是迈向科学应用中更可信AI系统的一步。

Abstract: Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.

</details>


### [77] [Combinatorial Optimization Augmented Machine Learning](https://arxiv.org/abs/2601.10583)
*Maximilian Schiffer,Heiko Hoppe,Yue Su,Louis Bouvier,Axel Parmentier*

Main category: cs.LG

TL;DR: COAML综述：将组合优化与机器学习结合的新范式，通过嵌入组合优化器构建数据驱动且保持可行性的策略，涵盖静态/动态问题、模仿学习、强化学习等应用。


<details>
  <summary>Details</summary>
Motivation: 组合优化增强机器学习（COAML）作为新兴范式，旨在整合预测模型与组合决策，构建既数据驱动又保持可行性的策略，弥合机器学习、运筹学和随机优化之间的传统鸿沟。

Method: 提出统一的COAML流程框架，描述方法构建模块，形式化其与经验成本最小化的联系；开发基于不确定性和决策结构的问题设置分类法；综述静态和动态问题的算法方法，涵盖调度、车辆路径、随机规划和强化学习等应用领域。

Result: 建立了COAML的全面分类体系，系统综述了该领域的最新进展，将方法论贡献归纳为经验成本最小化、模仿学习和强化学习三大类，为领域研究提供了结构化框架。

Conclusion: COAML是组合优化与机器学习交叉领域的重要研究方向，本综述既可作为该领域的入门教程，也可作为未来研究的路线图，并指出了关键研究前沿。

Abstract: Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.

</details>


### [78] [ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition](https://arxiv.org/abs/2601.10591)
*Arundeep Chinta,Lucas Vinh Tran,Jay Katukuri*

Main category: cs.LG

TL;DR: 提出ProbFM，一种基于深度证据回归的Transformer概率框架，首次为时间序列基础模型提供理论基础的认知-偶然不确定性分解，在加密货币预测中保持竞争力的同时实现不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型在金融预测中缺乏理论基础的、可分解的不确定性量化方法，现有方法要么依赖限制性分布假设，要么混淆不同不确定性来源，要么缺乏原则性校准机制，阻碍了其在金融应用中的采用。

Method: 提出ProbFM（概率基础模型），基于Transformer架构，利用深度证据回归实现原则性不确定性量化，通过高阶证据学习最优不确定性表示，保持单次计算效率。为独立评估DER方法，使用一致的LSTM架构对比五种概率方法：DER、高斯NLL、Student's-t NLL、分位数损失和共形预测。

Result: 在加密货币收益预测评估中，DER方法在保持竞争力的预测准确性的同时，提供了明确的认知-偶然不确定性分解，证明了其在金融应用中的有效性。

Conclusion: 该工作为时间序列基础模型中的原则性不确定性量化建立了可扩展框架，并提供了DER在金融应用中有效性的实证证据，解决了当前不确定性量化方法的核心限制。

Abstract: Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.

</details>


### [79] [Data-driven stochastic reduced-order modeling of parametrized dynamical systems](https://arxiv.org/abs/2601.10690)
*Andrew F. Ilersich,Kevin Course,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 提出基于摊销随机变分推断的数据驱动框架，用于学习连续时间随机降阶模型，能够在参数空间和强迫条件下泛化，无需昂贵前向求解器训练。


<details>
  <summary>Details</summary>
Motivation: 复杂动力系统在变化条件下的建模计算成本高昂，现有降阶模型方法难以处理随机动力学且无法量化预测不确定性，限制了在鲁棒决策中的实用性。

Method: 基于摊销随机变分推断框架，利用马尔可夫高斯过程的重参数化技巧，联合学习概率自编码器和控制潜在动力学的随机微分方程，训练成本与数据集大小和系统刚度无关。

Result: 在三个具有挑战性的测试问题上展示了优异性能，能够泛化到未见过的参数组合和强迫条件，相比现有方法获得显著效率提升。

Conclusion: 该框架为复杂随机动力系统提供了一种高效、可泛化的降阶建模方法，能够量化不确定性并支持鲁棒决策，同时保持计算效率。

Abstract: Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.

</details>


### [80] [Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis](https://arxiv.org/abs/2601.10701)
*Chun Hei Michael Shiu,Chih Wei Ling*

Main category: cs.LG

TL;DR: 本文对联邦学习中的通信高效隐私自适应机制（CEPAM）进行理论分析和实验评估，该机制通过拒绝采样通用量化器同时实现通信效率和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据治理约束下实现隐私保护协作，但仍面临通信效率和参与方间隐私保护等关键挑战。CEPAM作为一种新方法同时实现了这两个目标，需要对其隐私保证和收敛特性进行理论分析，并通过实验评估其性能。

Method: CEPAM采用拒绝采样通用量化器（RSUQ），这是一种随机向量量化器，其量化误差等价于预设噪声，可通过调节噪声水平来定制参与方间的隐私保护。本文对CEPAM进行理论分析，包括隐私保证和收敛特性，并通过实验评估其性能。

Result: 通过理论分析建立了CEPAM的隐私保证和收敛性质，实验评估包括收敛曲线与其他基线的比较，以及不同参与方间的准确率-隐私权衡分析。

Conclusion: CEPAM在联邦学习中同时实现了通信效率和隐私保护，通过理论分析和实验验证了其有效性，为联邦学习中的隐私保护协作提供了实用解决方案。

Abstract: Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.

</details>


### [81] [Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication](https://arxiv.org/abs/2601.10705)
*Keval Jain,Anant Raj,Saurav Prakash,Girish Varma*

Main category: cs.LG

TL;DR: 提出一种用于半异步客户端-服务器感知机的陈旧性分桶聚合方法，处理联邦学习中的延迟、部分参与和通信噪声问题，并证明有限轮次内的错误累积边界。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习和分布式部署中的三个系统效应：1) 由于延迟模型交付和延迟客户端计算应用导致的双向版本滞后（陈旧更新）；2) 部分参与（间歇性客户端可用性）；3) 上下行链路的不完美通信（建模为有界二阶矩的零均值加性噪声）。现有方法通常假设延迟或参与的随机模型，本文提出确定性方法。

Method: 提出服务器端聚合规则：陈旧性分桶聚合与填充。该方法确定性地强制执行预定的陈旧性配置文件，而不假设延迟或参与的随机模型。在边际可分性和有界数据半径条件下，通过迭代参数混合（IPM-style averaging）训练半异步客户端-服务器感知机。

Result: 证明了在给定服务器轮次数内，感知机错误累积的加权期望有限边界：延迟影响仅通过平均强制执行陈旧性体现，而通信噪声贡献一个与噪声能量平方根成比例增长的附加项。在无噪声情况下，展示了有限期望错误预算如何在温和的新鲜参与条件下产生显式的有限轮次稳定边界。

Conclusion: 提出的陈旧性分桶聚合方法能够有效处理联邦学习中的系统异质性，提供理论保证的收敛边界，延迟影响可控，通信噪声影响可量化，为实际部署提供了理论指导。

Abstract: We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.

</details>


### [82] [High-accuracy and dimension-free sampling with diffusions](https://arxiv.org/abs/2601.10708)
*Khashayar Gatmiry,Sitan Chen,Adil Salim*

Main category: cs.LG

TL;DR: 提出一种基于低阶近似和配点法的新型扩散模型求解器，其迭代复杂度在1/ε上呈多对数缩放，首次实现仅需访问数据分布分数近似的高精度采样保证


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型求解方法需要大量小步长迭代才能生成高质量样本，其迭代复杂度在环境维度和逆精度1/ε上呈多项式缩放，计算成本高昂。需要开发更高效的求解器来降低计算复杂度。

Method: 提出新型扩散模型求解器，结合低阶近似和配点法（Lee, Song, Vempala 2018）。该方法仅需近似访问数据分布的分数（score），通过巧妙设计实现高效采样。

Result: 证明新求解器的迭代复杂度在1/ε上呈多对数缩放（polylogarithmic），而非传统方法的多项式缩放。这是首个仅需近似访问数据分布分数就能实现高精度采样的扩散模型求解器理论保证。

Conclusion: 新方法显著降低了扩散模型采样的计算复杂度，实现了从多项式到多对数的改进。维度影响仅通过目标分布支撑集的有效半径体现，而非显式依赖环境维度，为高效扩散模型采样提供了理论框架。

Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.

</details>


### [83] [DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids](https://arxiv.org/abs/2601.10715)
*Navami Kairanda,Shanthika Naik,Marc Habermann,Avinash Sharma,Christian Theobalt,Vladislav Golyanik*

Main category: cs.LG

TL;DR: 提出DInf-Grid：一种基于可微分网格的表示方法，结合特征网格效率和径向基函数插值，用于快速求解微分方程，相比坐标MLP方法提速5-20倍。


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器存在计算效率问题：坐标MLP方法计算密集且训练慢；基于线性插值的网格方法（如Instant-NGP）虽然训练快，但无法计算高阶导数，不适用于求解微分方程。

Method: 提出DInf-Grid表示：结合特征网格效率与径向基函数插值（无限可微分）。引入多分辨率分解与共位网格，有效捕捉高频解并实现稳定快速的全局梯度计算。使用微分方程作为损失函数进行隐式训练。

Result: 在泊松方程（图像重建）、亥姆霍兹方程（波场）、基尔霍夫-洛夫边界值问题（布料模拟）等任务上验证，相比坐标MLP方法实现5-20倍加速，在秒或分钟级别求解微分方程，同时保持可比精度和紧凑性。

Conclusion: DInf-Grid通过结合网格效率与径向基函数可微性，为微分方程求解提供了高效、准确的表示方法，显著优于现有神经求解器架构。

Abstract: We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.

</details>
