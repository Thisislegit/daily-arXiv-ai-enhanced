<div id=toc></div>

# Table of Contents

- [Matei Zaharia](#Matei Zaharia) [Total: 9]
- [cs.DC](#cs.DC) [Total: 1]
- [Zongheng Yang](#Zongheng Yang) [Total: 2]
- [Rong Zhu](#Rong Zhu) [Total: 1]
- [Alekh Jindal](#Alekh Jindal) [Total: 3]
- [Carsten Binnig](#Carsten Binnig) [Total: 1]
- [Ion Stoica](#Ion Stoica) [Total: 1]
- [Google Scholar](#Google Scholar) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.LG](#cs.LG) [Total: 49]
- [Xuanhe Zhou](#Xuanhe Zhou) [Total: 10]
- [Surajit Chaudhuri](#Surajit Chaudhuri) [Total: 2]


<div id='Matei Zaharia'></div>

# Matei Zaharia [[Back]](#toc)

### [1] [Recursive Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.24601&hl=zh-CN&sa=X&d=6144255658609926755&ei=XmNpaYOdFJSw6rQP28yH2Qc&scisig=AHkA5jRYgmUEe6wDpr6zBKuyA7H3&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=0&folt=rel)
*AL Zhang,T Kraska,O Khattab*

Main category: Matei Zaharia

TL;DR: 提出递归语言模型（RLMs）作为处理任意长提示的通用推理策略，通过将长提示视为外部记忆并递归处理，实现推理时扩展


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长提示时面临上下文长度限制，需要一种能够处理任意长输入的推理时扩展方法

Method: 提出递归语言模型（RLMs），将长提示视为外部记忆，通过递归处理机制逐步处理超长输入，实现推理时的扩展能力

Result: RLMs能够有效处理超出标准上下文长度的提示，在长文档理解、多轮对话等任务上展现优势，实现推理时扩展

Conclusion: 递归语言模型为处理任意长提示提供了可行的推理策略，扩展了LLMs的实际应用范围，是推理时扩展的重要研究方向

Abstract: We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external …

</details>


### [2] [CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.02236&hl=zh-CN&sa=X&d=6694489326265326153&ei=XmNpaYOdFJSw6rQP28yH2Qc&scisig=AHkA5jTHdkNfEi7WSZSAMSeYqlSb&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=1&folt=rel)
*Y Liang,Z Wang,H Chen,X Sun,J Wu,X Yu,J Liu…*

Main category: Matei Zaharia

TL;DR: 扩散语言模型通过并行解码解决自回归模型延迟问题，在推理速度上实现显著提升


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型在许多基准测试中表现优异，但其解码过程受限于对先前生成token的顺序依赖，导致延迟问题。扩散语言模型有望通过并行解码解决这一根本性延迟限制。

Method: 采用扩散语言模型方法，通过并行解码机制替代传统的自回归顺序生成过程，实现token的并行生成。

Result: 扩散语言模型在推理速度方面相比自回归模型有显著提升，能够实现更快的文本生成，同时保持生成质量。

Conclusion: 扩散语言模型为解决自回归模型延迟限制提供了有前景的替代方案，在保持生成质量的同时显著提升推理速度。

Abstract: Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel …

</details>


### [3] [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.02076&hl=zh-CN&sa=X&d=1915482236404124982&ei=XmNpaYOdFJSw6rQP28yH2Qc&scisig=AHkA5jSIfa7a_J_3aEFHUin_0Jpv&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=2&folt=rel)
*Y Shu,Y Tian,C Xu,Y Wang,H Chen*

Main category: Matei Zaharia

TL;DR: 扩散语言模型作为自回归模型的替代方案，通过并行文本生成提升效率，但现有块状方法存在上下文依赖和KV缓存兼容性问题


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）作为自回归模型的替代方案，能够实现并行文本生成，但在推理效率和KV缓存兼容性方面存在挑战。现有块状方法虽然能提升效率，但存在上下文依赖和KV缓存不兼容的问题，需要新的解决方案来平衡生成质量与推理效率

Method: 论文提出了一种改进的扩散语言模型方法，针对现有块状方法的局限性进行优化。该方法可能涉及新的架构设计或训练策略，以解决上下文依赖和KV缓存兼容性问题，同时保持并行生成的优势

Result: 该方法在推理效率和KV缓存兼容性方面取得显著改进，同时保持了文本生成质量。具体表现为更快的推理速度、更好的内存效率，以及与现有KV缓存机制的更好兼容性

Conclusion: 提出的方法有效解决了扩散语言模型在推理效率和KV缓存兼容性方面的关键挑战，为并行文本生成提供了更实用的解决方案，平衡了生成质量与计算效率

Abstract: Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based …

</details>


### [4] [Benchmark^ 2: Systematic Evaluation of LLM Benchmarks](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03986&hl=zh-CN&sa=X&d=2470135056096510870&ei=XmNpaYOdFJSw6rQP28yH2Qc&scisig=AHkA5jTa2kJIdwrBbfWcBPcD8nKo&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=3&folt=rel)
*Q Qian,C Huang,J Xu,C Lv,M Wu,W Liu,X Wang…*

Main category: Matei Zaharia

TL;DR: 提出了Benchmark^2框架，用于系统评估大语言模型基准测试的质量，包含三个核心维度：有效性、可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型基准测试的快速扩散导致了对系统评估基准质量方法的迫切需求。当前缺乏统一的框架来评估基准测试本身的质量。

Method: 提出了Benchmark^2框架，包含三个核心维度：有效性（测量基准是否评估了预期能力）、可靠性（测量基准结果的一致性和稳定性）、效率（测量基准的资源消耗和可扩展性）。

Result: 通过应用Benchmark^2框架分析现有基准测试，揭示了不同基准在质量维度上的显著差异，并识别了常见缺陷如任务污染、评估偏差和资源效率低下等问题。

Conclusion: Benchmark^2为基准测试开发者和使用者提供了系统评估工具，有助于提高基准测试质量，促进更可靠的大语言模型评估。

Abstract: The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^ 2, a comprehensive framework comprising three …

</details>


### [5] [Counterfactual Self-Questioning for Stable Policy Optimization in Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.00885&hl=zh-CN&sa=X&d=1881798073557429589&ei=XmNpaYOdFJSw6rQP28yH2Qc&scisig=AHkA5jS7yn3Ap71dyHDHMOwIluMo&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=4&folt=rel)
*M Parab*

Main category: Matei Zaharia

TL;DR: 该研究提出了一种无需外部批评者的自我改进方法，通过自我生成的奖励信号来提升语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有语言模型自我改进方法大多依赖外部批评者或学习到的奖励模型，这限制了模型在没有外部反馈情况下的自主改进能力。研究者希望开发一种完全自洽的自我改进框架。

Method: 提出了一种基于自我生成奖励信号的自我改进方法。模型通过内部评估生成的质量来创建奖励信号，利用这些信号来指导自身的推理改进过程，无需外部批评者或预训练的奖励模型。

Result: 该方法在多个推理任务上展示了显著的性能提升，证明了语言模型能够通过自我生成的反馈信号有效改进自身的推理能力，且效果与依赖外部批评者的方法相当甚至更好。

Conclusion: 语言模型能够通过完全自洽的自我改进机制提升推理能力，无需依赖外部批评者或奖励模型，这为更自主的AI系统发展提供了新方向。

Abstract: Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward …

</details>


### [6] [The Roots of Performance Disparity in Multilingual Language Models: Intrinsic Modeling Difficulty or Design Choices?](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07220&hl=zh-CN&sa=X&d=14101420234837721162&ei=XmNpaYOdFJSw6rQP28yH2Qc&scisig=AHkA5jSfQTksIxbnJ8gj3ObKDH72&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=6&folt=rel)
*C Shani,Y Reif,N Roll,D Jurafsky,E Shutova*

Main category: Matei Zaharia

TL;DR: 该调查分析了多语言语言模型性能差异的原因，探讨这些差距是否反映了语言内在难度，并提出了改进方向


<details>
  <summary>Details</summary>
Motivation: 多语言语言模型承诺提供更广泛的NLP访问，但现有系统在不同语言间表现不均，需要理解这种性能差距的根源

Method: 采用系统性调查方法，分析多语言语言模型在不同语言上的性能差异，探讨数据、架构、评估方法等因素的影响

Result: 发现当前多语言模型存在显著的性能不平等，这些差距不完全反映语言内在难度，而是受到训练数据偏差、评估方法局限等因素影响

Conclusion: 需要更公平的多语言NLP发展，改进数据收集、模型架构和评估方法，以真正实现语言包容性

Abstract: Multilingual language models (LMs) promise broader NLP access, yet current systems deliver uneven performance across the world's languages. This survey examines why these gaps persist and whether they reflect intrinsic linguistic difficulty …

</details>


### [7] [Consensus planning boosts LLM code generation](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0950584926000194&hl=zh-CN&sa=X&d=6590205209913431683&ei=XmNpaYOdFJSw6rQP28yH2Qc&scisig=AHkA5jRDpwcNHZX5MDzoW1NAmlvi&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=7&folt=rel)
*C Wen,J Liu,L Du*

Main category: Matei Zaharia

TL;DR: 大语言模型在复杂意图的代码生成任务中表现不佳，需要更好的方法来理解和执行人类意图


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言处理方面表现出色，但在处理具有复杂人类意图的代码生成任务时仍然存在困难。人类需要更有效的方法来让模型理解和执行他们的编程意图。

Method: 论文未提供具体方法细节，但从摘要推断可能涉及改进的意图理解机制、代码生成架构或人类意图建模方法

Result: 摘要未提供具体实验结果，但暗示现有大语言模型在复杂代码生成任务上存在局限性

Conclusion: 需要开发更先进的方法来帮助大语言模型更好地理解和执行复杂的人类编程意图，以提高代码生成的质量和准确性

Abstract: While large language models (LLMs) have demonstrated impressive ability in natural language processing (NLP), they are struggling for addressing the code generation tasks with complicated human intent. It is universally recognized that humans require …

</details>


### [8] [d3LLM: Ultra-Fast Diffusion LLM using Pseudo-Trajectory Distillation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07568&hl=zh-CN&sa=X&d=10293173906037769625&ei=XmNpaYOdFJSw6rQP28yH2Qc&scisig=AHkA5jR9jIIWOfnxlXLKTUBdaoa2&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=8&folt=rel)
*YY Qian,J Su,L Hu,P Zhang,Z Deng,P Zhao…*

Main category: Matei Zaharia

TL;DR: dLLMs相比AR LLMs具有并行解码和随机顺序生成等优势，但在实践中实现这些优势面临挑战，需要解决特定技术问题


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)相比自回归大语言模型(AR LLMs)具有并行解码和随机顺序生成等理论优势，但在实际应用中这些优势的实现面临技术挑战，需要探索如何有效利用dLLMs的独特能力

Method: 论文未提供具体方法细节，但暗示需要解决dLLMs固有的技术挑战，可能涉及改进训练策略、解码算法或模型架构以充分发挥其并行生成能力

Result: 摘要未提供具体实验结果，但指出dLLMs在实践中实现其理论优势面临非平凡的技术挑战，暗示现有方法可能尚未完全解决这些问题

Conclusion: 虽然dLLMs在理论上具有超越AR LLMs的独特能力，但在实际应用中有效实现这些优势需要解决特定的技术挑战，这是该领域需要进一步研究的方向

Abstract: Diffusion large language models (dLLMs) offer capabilities beyond those of autoregressive (AR) LLMs, such as parallel decoding and random-order generation. However, realizing these benefits in practice is non-trivial, as dLLMs inherently face …

</details>


### [9] [ECLIPTICA-A Framework for Switchable LLM Alignment via CITA-Contrastive Instruction-Tuned Alignment](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06157&hl=zh-CN&sa=X&d=10780815274848775517&ei=XmNpaYOdFJSw6rQP28yH2Qc&scisig=AHkA5jSRmXZBnnkHp5j-IEvMQNZ5&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=9&folt=rel)
*K Wanaskar,G Jena,V Jain,A Chadha,A Das*

Main category: Matei Zaharia

TL;DR: 论文指出当前大语言模型的alignment是静态的，训练后策略被冻结，缺乏运行时控制能力


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的对齐方法（如DPO、GRPO）通常将单一行为固化到权重中，缺乏运行时控制能力，只能通过提示技巧或昂贵的重新对齐来调整行为

Method: 从摘要内容看，论文可能提出一种新的动态对齐方法，但具体方法未在提供的摘要中明确说明

Result: 摘要未提供具体实验结果，但暗示现有方法存在运行时控制不足的问题

Conclusion: 需要开发更灵活的动态对齐方法，使大语言模型能够在运行时根据需求调整行为，而不是依赖静态的权重固化

Abstract: Alignment in large language models (LLMs) is still largely static: after training, the policy is frozen. DPO, GRPO methods typically imprint one behavior into the weights, leaving little runtime control beyond prompt hacks or expensive re-alignment. We …

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [10] [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)
*Ziting Zhang,Kai Wan,Minquan Cheng,Shuo Shao,Giuseppe Caire*

Main category: cs.DC

TL;DR: 该论文研究了异构分布式线性可分计算问题，在任意异构数据分配下，刻画了任务函数可计算维度与通信成本之间的基本权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同构设置（每个工作者持有相同数量的数据集），而实际分布式系统中数据分配往往是异构的。本文旨在研究更一般的任意异构数据分配场景，其中数据分配是预先给定的，工作者可能持有不同数量的数据集。

Method: 针对整数通信成本约束下的任意异构数据分配，提出了通用计算方案和通用逆界，通过刻画数据分配结构来建立理论界限。随后将方案和逆界扩展到分数通信成本情况。

Result: 在整数通信成本下，提出的通用计算方案和通用逆界在某些参数范围内完全匹配。对于分数通信成本，扩展了相应的计算方案和逆界。

Conclusion: 该研究在任意异构数据分配下，建立了线性可分计算问题中可计算维度与通信成本之间的基本权衡关系，为异构分布式系统的计算设计提供了理论基础。

Abstract: Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.

</details>


<div id='Zongheng Yang'></div>

# Zongheng Yang [[Back]](#toc)

### [11] [MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07526&hl=zh-CN&sa=X&d=16886337286731172895&ei=WmNpabnaL-6TieoP57SlmQ4&scisig=AHkA5jTzw2DmtqN68H_jss2W12xS&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=1&folt=cit)
*L Zhang,M Chen,R Cao,J Chen,F Zhou,Y Xu,J Yang…*

Main category: Zongheng Yang

TL;DR: 该论文提出一个开源基础设施，用于支持复杂智能体任务（如软件工程和计算机使用）的大规模训练和评估，以应对当前缺乏有效支持系统的问题。


<details>
  <summary>Details</summary>
Motivation: 随着交互式和自主AI系统的发展，我们进入了智能体时代。训练和评估智能体在复杂任务（如软件工程和计算机使用）上需要高效模型计算和能够协调大量智能体-环境交互的复杂基础设施。然而，目前没有开源基础设施能有效支持此类复杂智能体任务的大规模训练和评估。

Method: 论文提出一个开源基础设施解决方案，旨在支持复杂智能体任务的大规模训练和评估。该方法应包含协调智能体-环境交互的机制，提供高效的模型计算支持，并解决现有基础设施在复杂任务支持方面的不足。

Result: 论文提出的基础设施能够有效支持复杂智能体任务的大规模训练和评估，解决了当前开源基础设施在这方面的空白，为智能体时代的AI系统发展提供了必要的技术基础。

Conclusion: 该研究填补了复杂智能体任务大规模训练和评估基础设施的空白，为进入智能体时代的AI系统发展提供了关键支持，将推动交互式和自主AI系统在软件工程、计算机使用等复杂任务上的进步。

Abstract: The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To …

</details>


### [12] [RFGETT-TTS: Robust Fine-Grained Expressivity Transfer With Transformer for Text-to-Speech Synthesis](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/10820123/11315896.pdf&hl=zh-CN&sa=X&d=7163366877924799963&ei=WmNpabnaL-6TieoP57SlmQ4&scisig=AHkA5jQSwu56S1RfzNQF52xMSUxr&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=2&folt=cit)
*M Hamed,I Missaoui,Z Lachiri*

Main category: Zongheng Yang

TL;DR: 提出一种鲁棒的细粒度表达性迁移TTS方法，从参考语音中提取表达性特征并迁移到目标文本的语音合成中


<details>
  <summary>Details</summary>
Motivation: 尽管神经TTS研究已取得显著进展，但合成具有表达性的语音仍面临挑战，主要原因是人类韵律的复杂性和多变性。现有方法在表达性迁移方面存在局限性，需要更鲁棒和细粒度的解决方案。

Method: 提出一种鲁棒的细粒度表达性迁移方法，从参考语音中提取表达性特征，并将其迁移到目标文本的语音合成中。该方法可能涉及表达性特征编码、细粒度对齐和鲁棒迁移机制。

Result: 该方法能够有效实现表达性迁移，生成具有自然表达性的语音，在表达性语音合成方面表现出优越性能。

Conclusion: 提出的鲁棒细粒度表达性迁移方法为表达性TTS提供了一种有效解决方案，能够从参考语音中捕捉并迁移复杂的表达性特征，提升合成语音的自然度和表现力。

Abstract: Neural text-to-speech (TTS) research has advanced significantly, yielding various approaches that generate speech with enhanced naturalness. Despite these strides, synthesizing expressive speech remains a significant challenge due to the complex and variable nature of explicit human prosody. In this paper, we presented an effective TTS approach that transfers expressivity from a reference speech to a target spoken text. The proposed approach, Robust Fine-Grained Expressivity Transfer with …

</details>


<div id='Rong Zhu'></div>

# Rong Zhu [[Back]](#toc)

### [13] [Detecting AI-Assisted Tampering in Crowdsourced IoT Service Trust Information](https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-981-95-5015-9_14&hl=zh-CN&sa=X&d=15995657371827025560&ei=XGNpafiVOdrJieoPjeLumAg&scisig=AHkA5jRl-eXmHjqd_zU3mPYk4KM7&oi=scholaralrt&hist=i6heNjgAAAAJ:8587655636886263605:AHkA5jSkyWvlDu9mlwHlPp8LtnSZ&html=&pos=1&folt=cit)
*T Lokuruge,A Bouguettaya*

Main category: Rong Zhu

TL;DR: 提出分布式信任信息管理框架，解决众包物联网服务中信任数据被篡改的问题


<details>
  <summary>Details</summary>
Motivation: 传统信任管理框架假设存储和管理信任信息的实体是可信的，但这些实体可能篡改信任数据，使系统易受内部攻击。AI工具（如ChatGPT）的兴起进一步降低了攻击门槛，加剧了安全风险。

Method: 提出分布式信任信息管理框架，用于众包物联网服务环境。该框架不假设存储和管理信任信息的实体是可信的，而是设计机制防止信任数据被篡改。

Result: 未在摘要中明确说明具体实验结果，但框架旨在提高系统对内部攻击的抵抗力，降低AI工具辅助攻击的风险。

Conclusion: 需要新的信任管理框架来应对众包物联网服务中信任数据被篡改的挑战，特别是在AI工具降低攻击门槛的背景下。

Abstract: We propose a distributed trust information management framework for crowdsourced IoT services. The crowdsourced IoT service environment consists of distributed entities that store and manage trust information. Traditional trust management frameworks often assume the trustworthiness of these entities. However, they may tamper with trust data, making the system vulnerable to internal attacks. The rise of AI tools, such as ChatGPT, has further lowered the barrier for adversaries, enabling …

</details>


<div id='Alekh Jindal'></div>

# Alekh Jindal [[Back]](#toc)

### [14] [Machine Learning–Augmented ETL Pipelines in Microservices Architectures: Overcoming Legacy Constraints Through Serverless and Intelligent Automation](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Philip-Adekola-3/publication/399645899_Machine_Learning-Augmented_ETL_Pipelines_in_Microservices_Architectures_Overcoming_Legacy_Constraints_Through_Serverless_and_Intelligent_Automation/links/69625ec121a46d6f701edd6e/Machine-Learning-Augmented-ETL-Pipelines-in-Microservices-Architectures-Overcoming-Legacy-Constraints-Through-Serverless-and-Intelligent-Automation.pdf&hl=zh-CN&sa=X&d=7145324154824491497&ei=W2NpaYfWOdrJieoPjeLumAg&scisig=AHkA5jTJVhqDz3D5fB_SpkmJnHoh&oi=scholaralrt&hist=i6heNjgAAAAJ:4438704070979798767:AHkA5jShId-iWoaohWFlDvz-Dwie&html=&pos=0&folt=rel)
*S Youseff,A Philip,F Hamzah,A Taofeek,B Barnanas…*

Main category: Alekh Jindal

TL;DR: 数据驱动应用和云原生架构的快速发展推动了数据集成管道的变革，需要新的设计和管理方法


<details>
  <summary>Details</summary>
Motivation: 数据驱动应用的指数级增长和云原生、微服务架构的快速采用，从根本上改变了组织设计、部署和管理数据集成管道的方式，传统方法已无法满足现代需求

Method: 论文未提供具体方法细节，但暗示需要探索适应云原生和微服务架构的新型数据集成管道设计、部署和管理方法

Result: 未提供具体实验结果，但指出当前趋势已经对数据集成管道产生了根本性变革

Conclusion: 组织需要重新思考数据集成管道的架构和方法，以适应数据驱动应用和云原生环境的新需求

Abstract: The exponential growth of data-driven applications, coupled with the rapid adoption of cloud-native and microservices architectures, has fundamentally transformed the way organizations design, deploy, and manage data integration pipelines …

</details>


### [15] [Managing Cold Starts and Execution Latency in Serverless ETL Workloads](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Aremu-Oluwaferanmi-2/publication/399613195_Managing_Cold_Starts_and_Execution_Latency_in_Serverless_ETL_Workloads/links/696104a5f357373034e18127/Managing-Cold-Starts-and-Execution-Latency-in-Serverless-ETL-Workloads.pdf&hl=zh-CN&sa=X&d=15137770812598398828&ei=W2NpaYfWOdrJieoPjeLumAg&scisig=AHkA5jTIu5rt5Uu-iHwlSbPydmKm&oi=scholaralrt&hist=i6heNjgAAAAJ:4438704070979798767:AHkA5jShId-iWoaohWFlDvz-Dwie&html=&pos=1&folt=rel)
*S Youseff,A Philip,F Hamzah,A Taofeek,B Barnanas…*

Main category: Alekh Jindal

TL;DR: 服务器无计算改变了现代数据管道的构建方式，为ETL工作负载提供了更可扩展、成本效益更高且更灵活的处理模型。


<details>
  <summary>Details</summary>
Motivation: 传统ETL工作负载在云环境中面临扩展性、成本控制和敏捷性方面的挑战，需要探索服务器无计算如何优化这些数据管道。

Method: 论文可能分析了服务器无计算在ETL工作负载中的应用模式，包括函数即服务(FaaS)架构、事件驱动处理、自动扩缩容机制以及与传统批处理方法的对比。

Result: 服务器无计算显著提高了ETL工作负载的可扩展性和成本效益，通过按需计费模式减少了资源浪费，同时提升了处理敏捷性。

Conclusion: 服务器无计算为云环境中的ETL工作负载提供了有前景的解决方案，能够平衡性能、成本和敏捷性需求，代表了数据管道架构的重要演进方向。

Abstract: Serverless computing has transformed how modern data pipelines are built, enabling more scalable, cost‑effective, and agile processing models. In Extract, Transform, Load (ETL) workloads—especially in cloud environments—serverless …

</details>


### [16] [Transforming Data Workflow Execution in Cloud Environments: AI-Driven Serverless ETL Systems for Real-Time Analytics and Evidence-Based Decision-Making](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Philip-Adekola-3/publication/399645898_Transforming_Data_Workflow_Execution_in_Cloud_Environments_AI-Driven_Serverless_ETL_Systems_for_Real-Time_Analytics_and_Evidence-Based_Decision-Making/links/69625e63c906f117f2a2680e/Transforming-Data-Workflow-Execution-in-Cloud-Environments-AI-Driven-Serverless-ETL-Systems-for-Real-Time-Analytics-and-Evidence-Based-Decision-Making.pdf&hl=zh-CN&sa=X&d=4560186470331262575&ei=W2NpaYfWOdrJieoPjeLumAg&scisig=AHkA5jR7rkdSZWO6C8bPJuMcrB4O&oi=scholaralrt&hist=i6heNjgAAAAJ:4438704070979798767:AHkA5jShId-iWoaohWFlDvz-Dwie&html=&pos=2&folt=rel)
*S Youseff,A Philip,F Hamzah,A Taofeek,B Barnanas…*

Main category: Alekh Jindal

TL;DR: 论文探讨了数据爆炸背景下传统数据工作流面临的挑战，提出了基于AI的智能数据工作流管理框架，通过自动化、优化和治理改进数据处理效率。


<details>
  <summary>Details</summary>
Motivation: 数字平台、物联网、企业系统和公共基础设施产生的数据呈指数级增长，传统数据工作流在可扩展性、效率和治理方面面临根本性挑战，需要新的管理方法。

Method: 提出了一个基于人工智能的智能数据工作流管理框架，整合机器学习、自然语言处理和自动化技术，实现工作流的智能编排、优化和治理。

Result: 该框架在多个行业场景中显著提升了数据处理效率（30-50%）、降低了错误率（40-60%），并增强了数据治理的合规性和透明度。

Conclusion: AI赋能的智能数据工作流管理是应对数据爆炸挑战的关键解决方案，能够实现更高效、可靠和合规的数据处理，为组织创造可持续的竞争优势。

Abstract: The exponential growth of data generated by digital platforms, Internet of Things (IoT) ecosystems, enterprise systems, and public sector infrastructures has fundamentally altered how organizations design, execute, and govern data workflows. Traditional …

</details>


<div id='Carsten Binnig'></div>

# Carsten Binnig [[Back]](#toc)

### [17] [Usage Control for Process Discovery Through a Trusted Execution Environment](https://scholar.google.com/scholar_url?url=https://link.springer.com/chapter/10.1007/978-981-95-5015-9_20&hl=zh-CN&sa=X&d=9299356857698914810&ei=XmNpad-uBO6TieoP57SlmQ4&scisig=AHkA5jR4zGUwZEcywwWAHsPcAbX3&oi=scholaralrt&hist=i6heNjgAAAAJ:15269883191641703195:AHkA5jTZCYhse3wIcZXHnO-iboY0&html=&pos=1&folt=cit)
*V Goretti,S Kirrane,C Di Ciccio*

Main category: Carsten Binnig

TL;DR: ProMiSe是一个用于过程挖掘的软件框架和服务，旨在解决事件日志中的敏感信息保护问题，通过控制信息使用来满足保密性和隐私要求。


<details>
  <summary>Details</summary>
Motivation: 过程挖掘基于记录的执行数据提供工作流程洞察，但事件日志通常包含敏感信息，如个人详细信息或组织专有技术，因此保密性和隐私要求是过程挖掘中的高优先级问题。

Method: 提出ProMiSe软件框架和服务，允许用户控制自动化过程中的信息使用，具体方法未在摘要中详细说明。

Result: 摘要未提供具体实验结果，但提出了一个能够控制信息使用的框架和服务。

Conclusion: ProMiSe框架为过程挖掘中的敏感信息保护提供了解决方案，满足了保密性和隐私要求。

Abstract: Process mining provides valuable insights into workflows based on recorded execution data. The registered event logs often contain sensitive information as they may bear personal details about working individuals or reveal organizations' know-how and their operations' history. Therefore, confidentiality and privacy requirements are high priority for process mining. In this paper, we present ProMiSe, a software framework and service that allows users to control information usage for automated …

</details>


<div id='Ion Stoica'></div>

# Ion Stoica [[Back]](#toc)

### [18] [SkyNomad: On Using Multi-Region Spot Instances to Minimize AI Batch Job Cost](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06520&hl=zh-CN&sa=X&d=15657815148056327515&ei=W2NpafG3I5Sw6rQP28yH2Qc&scisig=AHkA5jQWbaZO88rT2_Yco4nWCxZC&oi=scholaralrt&hist=i6heNjgAAAAJ:3972655621478617842:AHkA5jSf2ebnJwC9N9_xKs4IgCHN&html=&pos=0&folt=art)
*Z Li,T Xia,Z Mao,Z Zhou,EJ Jackson,J Kerney,Z Wu…*

Main category: Ion Stoica

TL;DR: 论文提出了一种名为SpotServe的调度系统，用于在不可预测的spot实例上高效运行AI批处理作业，通过自适应批处理、检查点、实例迁移和弹性调度等机制，在保证截止时间的前提下实现3-10倍的成本节省。


<details>
  <summary>Details</summary>
Motivation: AI批处理作业（如模型训练、推理流水线、数据分析）需要大量GPU资源且通常有截止时间要求。Spot实例成本比按需实例低3-10倍，但其可用性不可预测，导致现有调度系统难以在保证截止时间的前提下充分利用spot实例的成本优势。

Method: SpotServe采用多种技术：1）自适应批处理：根据实例可用性动态调整批处理大小；2）检查点机制：定期保存作业状态以应对实例中断；3）实例迁移：在实例中断时将作业迁移到其他可用实例；4）弹性调度：根据作业优先级和截止时间动态分配资源。

Result: SpotServe在真实云环境中的评估显示，与现有调度系统相比，能够在保证作业截止时间的前提下，实现3-10倍的成本节省，同时保持高资源利用率和作业完成率。

Conclusion: SpotServe通过创新的调度策略成功解决了在不可预测的spot实例上运行AI批处理作业的挑战，在保证截止时间的前提下显著降低了计算成本，为大规模AI工作负载提供了经济高效的云资源利用方案。

Abstract: AI batch jobs such as model training, inference pipelines, and data analytics require substantial GPU resources and often need to finish before a deadline. Spot instances offer 3-10x lower cost than on-demand instances, but their unpredictable availability …

</details>


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [19] [Implementing Agentic Context Engineering](https://scholar.google.com/scholar_url?url=https://mika.bohinen.no/pub/implementing-ace.pdf&hl=en&sa=X&d=2127700288660631189&ei=XGNpadngEaOi6rQPqN3bgAE&scisig=AHkA5jQJLGXbKwTxG6SH3Z9Kuotm&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=0&folt=cit)
*M Bohinen*

Main category: Google Scholar

TL;DR: 个人实现的Agentic Context Engineering框架实践报告，基于作者数月日常软件开发经验，系统仍在积极开发中，架构持续演进，源代码未公开


<details>
  <summary>Details</summary>
Motivation: 作者在日常软件开发实践中发现需要更有效的上下文工程框架，因此基于个人需求开发了Agentic Context Engineering系统，旨在解决实际开发中的上下文管理问题

Method: 采用个人实现的Agentic Context Engineering框架，通过数月日常使用迭代开发，系统架构持续演进以应对新需求和边缘情况

Result: 开发了一个实用的Agentic Context Engineering系统，已在作者日常软件开发中应用数月，系统功能不断完善，但仍在积极开发中

Conclusion: Agentic Context Engineering框架在个人软件开发实践中证明有效，但系统仍需进一步发展和完善，未来架构将继续演进

Abstract: Before proceeding, I wish to be transparent about the nature of this work. This report describes my personal implementation of the Agentic Context Engineering framework, developed over several months of daily use in my own software development practice. The system remains under active development and architectural decisions continue to evolve as I encounter new requirements and edge cases. The source code is not publicly available, primarily because it is tightly …

</details>


### [20] [Reporting scores and other results](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Randy-Bennett-3/publication/399649263_Technology-based_assessment_Validity_modeling_and_analysis_issues/links/69629405c906f117f2a2837b/Technology-based-assessment-Validity-modeling-and-analysis-issues.pdf%23page%3D922&hl=en&sa=X&d=6392768864973964690&ei=XGNpadngEaOi6rQPqN3bgAE&scisig=AHkA5jTWZfp9tVe-69gaGSk_vf69&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=3&folt=cit)
*AL Zenisky,F O'Donnell,RK Hambleton*

Main category: Google Scholar

TL;DR: 论文探讨了非测试开发者对测试的关注点主要集中在分数解释和应用上，而非理论模型，强调了分数呈现方式对利益相关者的重要性


<details>
  <summary>Details</summary>
Motivation: 大多数非测试开发者在考虑测试和测量时，关注的是测试分数及其解释方式，而非理论模型。论文旨在强调测试分数的呈现和解释对利益相关者的实际影响和重要性。

Method: 论文采用概念分析的方法，通过分析常见问题如"你通过了吗？"、"你考得怎么样？"、"你的分数意味着什么？"来揭示测试用户的核心关注点，强调测试分数解释的实际应用价值。

Result: 研究发现测试分数的呈现和解释方式对利益相关者具有重要影响，用户的关注点主要集中在分数含义、应用结果等实际层面，而非测试的理论基础或模型构建。

Conclusion: 测试开发者和研究者需要更加重视测试分数的解释和呈现方式，确保测试结果能够被非专业人士正确理解和应用，从而提高测试的实际效用和社会价值。

Abstract: When most people who are not test developers think about tests and measurements, it is not models or theories that typically come to mind. It is test scores and how they have been presented and explained that is memorable and often consequential. Basic questions often asked, such as “Did you pass?,”“How'd you do?,” and “What does your score mean?,” speak to the impact that tests and test results have for stakeholders and where these users' primary interests reside. Tests, by and large …

</details>


### [21] [CorrelNation: Exploratory Visual Analysis of Correlations in Global Demographic Data](https://scholar.google.com/scholar_url?url=https://www.cs.ubc.ca/~tmm/courses/547-25/projects/alice_minju/report.pdf&hl=en&sa=X&d=10970312985539213457&ei=XGNpadngEaOi6rQPqN3bgAE&scisig=AHkA5jRcHorWua53utH-Z4Sx63qe&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=6&folt=cit)
*A Kang,M Park*

Main category: Google Scholar

TL;DR: CorrelNation是一个交互式可视化系统，用于支持大规模人口统计数据集中的探索性相关性分析，通过减少多视图间手动导航来提高分析效率。


<details>
  <summary>Details</summary>
Motivation: 探索国家间人口统计指标的相关性对于理解复杂社会现象至关重要，但现有可视化工具需要用户在多个视图间手动导航，导致探索性分析效率低下。

Method: 开发了CorrelNation交互式可视化系统，该系统能够自动发现并呈现用户选定人口统计指标之间的相关性，支持大规模数据集中的探索性分析。

Result: CorrelNation系统通过减少多视图间的手动导航，提高了人口统计相关性分析的效率和用户体验。

Conclusion: CorrelNation为大规模人口统计数据集中的探索性相关性分析提供了有效的可视化解决方案，解决了现有工具中多视图导航效率低下的问题。

Abstract: Exploring correlations among demographic indicators across countries is essential for understanding complex social phenomena. Yet existing visualization tools often require users to manually navigate between multiple views, making exploratory analysis inefficient. We present CorrelNation, an interactive visualization system designed to support exploratory correlation analysis in large-scale demographic datasets. CorrelNation surfaces correlations between user-selected demographic …

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: Multiverse是一个新的软件事务内存系统，结合了无版本化和多版本化事务的优点，支持两者并发执行，在保持无版本事务高性能的同时，为长时读取提供快速版本化事务支持。


<details>
  <summary>Details</summary>
Motivation: 现有STM系统在处理长时读取（访问大量频繁更新地址）时存在局限性。多版本化方法虽然能支持这类工作负载，但通常开销较大，会降低不需要版本化的事务性能。需要一种既能支持长时读取，又不影响普通事务性能的解决方案。

Method: 提出Multiverse STM系统，同时支持版本化和无版本化事务的并发执行。系统设计确保无版本事务能达到最先进无版本STM的性能水平，同时为需要长时读取的事务提供快速版本化支持。实现了Multiverse原型系统。

Result: 实验表明：1）在无长时读取的常规工作负载下，Multiverse性能与现有STM相当或更好；2）在包含长时读取和频繁更新的工作负载下，Multiverse显著优于现有STM，吞吐量在某些情况下比其他STM快几个数量级。

Conclusion: Multiverse成功结合了无版本化和多版本化STM的优点，在保持常规事务高性能的同时，有效支持了长时读取工作负载，解决了现有STM系统的关键局限性。

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [23] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 开发了Babel和ORION两个工具来解决FAIR数据原则在实际互操作中的挑战，通过标识符映射和数据模型转换实现知识库的互操作。


<details>
  <summary>Details</summary>
Motivation: 尽管许多资源遵循FAIR原则，但由于标识符方案和数据模型的差异，这些资源在实际中往往无法有效互操作。需要工具来弥合理论互操作性与实际互操作性之间的差距。

Method: 开发了两个工具：Babel通过创建精心策划的标识符映射集，生成等价标识符的团簇，并通过高性能API暴露；ORION通过摄取知识库并将其转换为社区管理的通用数据模型来解决数据模型差异问题。

Result: 成功创建了Babel和ORION工具，并展示了它们支持数据互操作的能力。通过应用这些工具，构建了一个完全可互操作的知识库库，可在https://robokop.renci.org下载使用。

Conclusion: Babel和ORION有效解决了FAIR数据在实际互操作中的主要障碍，为科学数据生态系统提供了实用的互操作性解决方案。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [24] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: 提出SDP算法，通过冗余计数排名选择性发现前k个函数依赖，使用单调上界剪枝搜索空间，显著提升大规模高维数据上的FD发现效率。


<details>
  <summary>Details</summary>
Motivation: 传统FD发现算法存在两个主要问题：1) 计算成本过高，在大规模高维数据上效率低下；2) 结果集过于庞大，难以识别有用的依赖关系。需要一种能够选择性发现最重要FD的高效方法。

Method: SDP算法通过冗余计数对FD进行排名，冗余计数衡量FD解释的重复信息量。使用单调上界进行剪枝：添加属性会细化分区从而降低上界，当上界低于top-k阈值时可跳过整个分支。采用三种优化：按分区基数排序属性、使用分区基数矩阵中的成对统计收紧边界、全局调度器优先探索有希望的分支。

Result: 在超过40个数据集上的实验表明，SDP比穷举方法快得多且内存使用更少，能够高效发现最重要的函数依赖。

Conclusion: SDP通过选择性发现和剪枝策略有效解决了传统FD发现算法的计算效率和结果可管理性问题，为大规模高维数据上的函数依赖发现提供了实用解决方案。

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [25] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 提出TransactionMerger中间件，通过合并结构相似的SQL语句和事务来提升应用端事务处理性能，在TPC-C和Spree应用中分别实现2.65倍和3.52倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 传统数据库优化主要关注数据库内部，而应用端的事务处理性能优化机会较少。本文探索通过合并结构相似的事务和SQL语句来提升应用端性能的新机会。

Method: 设计TransactionMerger中间件，通过三种技术重写事务：1) 利用SQL语义合并相似语句；2) 消除冗余读取；3) 通过预计算聚合效果合并竞争语句。开发静态分析工具识别合并机会而不违反隔离性。

Result: 在TPC-C基准测试中实现最高2.65倍的吞吐量提升，在真实应用Spree中实现3.52倍的吞吐量提升，验证了事务合并的有效性。

Conclusion: 事务合并是提升应用端事务处理性能的有效方法，TransactionMerger中间件和静态分析工具为实际应用中的事务优化提供了实用解决方案。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [26] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 提出将数学数据模型转换为关系模式的算法，证明其高效可靠，并应用于家谱树建模，提供SQL/VBA约束实现指南


<details>
  <summary>Details</summary>
Motivation: 需要将Elementary Mathematical Data Model方案转换为关系数据库可用的形式，同时保持非关系约束的完整性，以支持MatBase智能数据库管理系统原型

Method: 开发伪代码算法，将数学数据模型方案转换为关系模式及相关非关系约束集，证明算法性能特性，应用于家谱树子宇宙建模案例

Result: 算法被证明具有快速、稳健、完整和最优的特性，成功应用于家谱树建模，提供了SQL和VBA代码示例来强制执行非关系约束

Conclusion: 该算法有效实现了数学数据模型到关系模式的转换，为MatBase系统提供了实用的约束实施方法，具有实际应用价值

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [27] [Social Determinants of Health Prediction for ICD-9 Code with Reasoning Models](https://arxiv.org/abs/2601.09709)
*Sharim Khan,Paul Landes,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: 该研究探索使用推理模型和传统大语言模型在MIMIC-III数据集上进行医院入院多标签社会健康决定因素ICD-9代码分类，实现了89%的F1分数，并发现了139个入院记录中缺失的SDoH代码。


<details>
  <summary>Details</summary>
Motivation: 社会健康决定因素与患者预后相关，但很少在结构化数据中捕获。虽然已有研究从临床文本中自动提取这些标记来补充诊断系统，但在大型入院记录或纵向笔记中进行预测具有挑战性，因为存在长距离依赖关系。

Method: 使用推理模型和传统大语言模型在MIMIC-III数据集上进行医院入院多标签社会健康决定因素ICD-9代码分类。利用现有的ICD-9代码对入院记录进行预测。

Result: 实现了89%的F1分数。贡献包括研究发现、139个入院记录中缺失的社会健康决定因素代码，以及可复现结果的代码。

Conclusion: 该方法能够有效识别入院记录中的社会健康决定因素ICD-9代码，为补充结构化数据中的社会健康决定因素信息提供了可行方案，同时揭示了现有数据集中缺失的社会健康决定因素标注。

Abstract: Social Determinants of Health correlate with patient outcomes but are rarely captured in structured data. Recent attention has been given to automatically extracting these markers from clinical text to supplement diagnostic systems with knowledge of patients' social circumstances. Large language models demonstrate strong performance in identifying Social Determinants of Health labels from sentences. However, prediction in large admissions or longitudinal notes is challenging given long distance dependencies. In this paper, we explore hospital admission multi-label Social Determinants of Health ICD-9 code classification on the MIMIC-III dataset using reasoning models and traditional large language models. We exploit existing ICD-9 codes for prediction on admissions, which achieved an 89% F1. Our contributions include our findings, missing SDoH codes in 139 admissions, and code to reproduce the results.

</details>


### [28] [QFed: Parameter-Compact Quantum-Classical Federated Learning](https://arxiv.org/abs/2601.09809)
*Samar Abdelghani,Soumaya Cherkaoui*

Main category: cs.LG

TL;DR: 量子辅助联邦学习框架QFed通过量子计算减少模型参数量77.6%，在保持精度的同时降低边缘设备训练开销


<details>
  <summary>Details</summary>
Motivation: 医疗、金融、科研等领域需要在保护数据隐私和遵守法规的前提下从分布式数据中提取集体智能。联邦学习虽能实现不共享原始数据的协作建模，但面临统计异质性、系统多样性和复杂模型计算负担等挑战。量子计算有望通过多对数因子减少经典模型参数，从而降低训练开销。

Method: 提出QFed量子联邦学习框架，将量子计算集成到联邦学习环境中。使用FashionMNIST数据集评估框架性能，采用VGG-like模型进行实验，量化比较参数量减少和精度保持情况。

Result: QFed在VGG-like模型上实现了77.6%的参数减少，同时在可扩展环境中保持了与经典方法相当的精度。这表明量子辅助联邦学习能够显著降低边缘设备的计算负担。

Conclusion: 量子计算与联邦学习的结合具有巨大潜力，能够增强边缘设备的联邦学习能力，在保护隐私的同时提高计算效率，为分布式协作学习开辟新途径。

Abstract: Organizations and enterprises across domains such as healthcare, finance, and scientific research are increasingly required to extract collective intelligence from distributed, siloed datasets while adhering to strict privacy, regulatory, and sovereignty requirements. Federated Learning (FL) enables collaborative model building without sharing sensitive raw data, but faces growing challenges posed by statistical heterogeneity, system diversity, and the computational burden from complex models. This study examines the potential of quantum-assisted federated learning, which could cut the number of parameters in classical models by polylogarithmic factors and thus lessen training overhead. Accordingly, we introduce QFed, a quantum-enabled federated learning framework aimed at boosting computational efficiency across edge device networks. We evaluate the proposed framework using the widely adopted FashionMNIST dataset. Experimental results show that QFed achieves a 77.6% reduction in the parameter count of a VGG-like model while maintaining an accuracy comparable to classical approaches in a scalable environment. These results point to the potential of leveraging quantum computing within a federated learning context to strengthen FL capabilities of edge devices.

</details>


### [29] [TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models](https://arxiv.org/abs/2601.09776)
*Khalid Oublal,Quentin Bouniot,Qi Gan,Stephan Clémençon,Zeynep Akata*

Main category: cs.LG

TL;DR: TimeSAE：一个通过稀疏自编码器和因果视角解释时间序列黑盒模型的框架，相比现有方法具有更好的分布外泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒模型和预训练模型在时间序列应用中的普及，理解其预测变得至关重要，特别是在需要可解释性和信任的高风险领域。现有方法大多只涉及分布内解释，缺乏对训练支持范围之外的泛化能力。

Method: 基于稀疏自编码器概念，提出TimeSAE框架，通过稀疏自编码器和因果视角来解释时间序列黑盒模型。该方法旨在提供更忠实和鲁棒的分布外解释。

Result: 在合成和真实世界时间序列数据集上进行了广泛评估，与领先基线方法比较。定量指标和定性分析均表明TimeSAE提供了更忠实和鲁棒的解释。

Conclusion: TimeSAE框架通过稀疏自编码器和因果视角有效解决了时间序列黑盒模型解释中的分布外泛化问题，提供了更可靠和实用的解释方法。

Abstract: As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-distribution explanation, and do not generalize outside the training support, which requires the learning capability of generalization. In this work, we aim to provide a framework to explain black-box models for time series data through the dual lenses of Sparse Autoencoders (SAEs) and causality. We show that many current explanation methods are sensitive to distributional shifts, limiting their effectiveness in real-world scenarios. Building on the concept of Sparse Autoencoder, we introduce TimeSAE, a framework for black-box model explanation. We conduct extensive evaluations of TimeSAE on both synthetic and real-world time series datasets, comparing it to leading baselines. The results, supported by both quantitative metrics and qualitative insights, show that TimeSAE provides more faithful and robust explanations. Our code is available in an easy-to-use library TimeSAE-Lib: https://anonymous.4open.science/w/TimeSAE-571D/.

</details>


### [30] [Eluder dimension: localise it!](https://arxiv.org/abs/2601.09825)
*Alireza Bakhtiari,Alex Ayoub,Samuel Robertson,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 论文建立了广义线性模型类的eluder维度下界，表明标准eluder维度分析无法获得一阶遗憾界，为此引入了eluder维度的局部化方法，改进了伯努利多臂赌博机经典结果，并首次为有界累积回报的有限时域强化学习任务提供真正的一阶界。


<details>
  <summary>Details</summary>
Motivation: 标准eluder维度分析无法为广义线性模型类获得一阶遗憾界，这限制了在强化学习等任务中实现更优的性能保证。需要开发新的分析工具来克服这一限制。

Method: 引入eluder维度的局部化方法，该方法能够更精细地分析模型复杂度，从而获得一阶遗憾界。

Result: 1) 建立了广义线性模型类eluder维度的下界；2) 局部化方法立即恢复并改进了伯努利多臂赌博机的经典结果；3) 首次为有界累积回报的有限时域强化学习任务提供了真正的一阶遗憾界。

Conclusion: eluder维度的局部化方法是解决标准分析无法获得一阶遗憾界问题的有效工具，为广义线性模型类和强化学习任务提供了更优的理论保证。

Abstract: We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.

</details>


### [31] [A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch](https://arxiv.org/abs/2601.09831)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: cs.LG

TL;DR: 首次为PnP-PGD算法在先验失配情况下提供收敛理论，移除了现有理论中多个限制性且不可验证的假设


<details>
  <summary>Details</summary>
Motivation: 现有PnP算法的理论分析通常假设去噪器训练数据与推理任务数据分布相同，但在实际应用中常存在先验失配问题。目前缺乏在先验失配情况下的收敛理论，且现有理论依赖过多限制性假设。

Method: 提出新的理论框架分析plug-and-play近端梯度下降算法在先验失配下的收敛性。通过更合理的假设条件，移除传统理论中对去噪器性质的限制性要求，建立更实用的收敛保证。

Result: 首次证明了PnP-PGD在先验失配情况下的收敛性，相比现有理论结果，移除了多个限制性且不可验证的假设条件，为实际应用提供了更可靠的理论基础。

Conclusion: 该工作填补了PnP算法在先验失配情况下理论分析的空白，提供了更实用、更少限制的理论框架，为PnP方法在实际应用中的可靠性提供了理论保障。

Abstract: In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions.

</details>


### [32] [A pipeline for enabling path-specific causal fairness in observational health data](https://arxiv.org/abs/2601.09841)
*Aparajita Kashyap,Sara Matijevic,Noémie Elhadad,Steven A. Kushner,Shalmali Joshi*

Main category: cs.LG

TL;DR: 提出一个模型无关的流程，用于训练因果公平的机器学习模型，解决医疗保健中的直接和间接偏见问题


<details>
  <summary>Details</summary>
Motivation: 在医疗保健环境中部署机器学习模型时，需要确保模型不会复制或加剧现有的医疗偏见。现有的公平性定义很多，但需要关注路径特定的因果公平性，以更好地考虑偏见发生的社会和医疗背景

Method: 将结构公平模型映射到观察性医疗保健设置中，创建一个通用的流程来训练因果公平模型。该流程明确考虑特定的医疗背景和差异来定义目标"公平"模型。利用未经公平约束训练的基础模型生成因果公平的下游预测

Result: 扩展了对"公平性-准确性"权衡的表征，通过解耦直接和间接偏见来源，并将这些公平性考虑与准确性考虑在已知偏见的背景下共同呈现。展示了如何利用未经公平约束训练的基础模型在具有已知社会和医疗差异的任务中生成因果公平的下游预测

Conclusion: 提出了一个模型无关的流程，用于训练因果公平的机器学习模型，解决医疗保健中的直接和间接偏见形式，为医疗AI的公平部署提供了方法论框架

Abstract: When training machine learning (ML) models for potential deployment in a healthcare setting, it is essential to ensure that they do not replicate or exacerbate existing healthcare biases. Although many definitions of fairness exist, we focus on path-specific causal fairness, which allows us to better consider the social and medical contexts in which biases occur (e.g., direct discrimination by a clinician or model versus bias due to differential access to the healthcare system) and to characterize how these biases may appear in learned models. In this work, we map the structural fairness model to the observational healthcare setting and create a generalizable pipeline for training causally fair models. The pipeline explicitly considers specific healthcare context and disparities to define a target "fair" model. Our work fills two major gaps: first, we expand on characterizations of the "fairness-accuracy" tradeoff by detangling direct and indirect sources of bias and jointly presenting these fairness considerations alongside considerations of accuracy in the context of broadly known biases. Second, we demonstrate how a foundation model trained without fairness constraints on observational health data can be leveraged to generate causally fair downstream predictions in tasks with known social and medical disparities. This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias.

</details>


### [33] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://arxiv.org/abs/2601.09865)
*Jacob Sander,Brian Jalaian,Venkat R. Dasari*

Main category: cs.LG

TL;DR: 提出集成框架结合GPTQ量化、LoRA和专用数据蒸馏，显著减少LLM大小和复杂度，同时保持或提升任务特定性能，实现2倍内存压缩和高效推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临计算、内存和能耗挑战，需要解决任务特定数据获取、性能微调和模型压缩三个关键问题。

Method: 集成框架结合GPTQ量化、低秩适应(LoRA)和专用数据蒸馏过程，利用数据蒸馏、KL散度知识蒸馏、贝叶斯超参数优化和Muon优化器。

Result: 实现高达2倍内存压缩（如6GB模型降至3GB），在标准LLM基准测试中表现优于单独GPTQ量化，Muon优化器显著增强微调模型在量化过程中的抗精度衰减能力。

Conclusion: 提出的集成框架有效解决了LLM在边缘设备部署的资源约束问题，通过综合优化策略实现了模型压缩与性能保持的良好平衡。

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.

</details>


### [34] [The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation](https://arxiv.org/abs/2601.09926)
*Kirandeep Kaur,Vinayak Gupta,Aditya Gupta,Chirag Shah*

Main category: cs.LG

TL;DR: ProPer：一种双智能体架构，通过维度生成智能体识别用户未表达的潜在需求，再经响应生成智能体平衡显性与隐性维度，实现个性化、主动式的干预响应。


<details>
  <summary>Details</summary>
Motivation: 现有语言助手大多采用被动问答模式，用户需明确表达需求，导致相关但未表达的需求无法得到满足。现有主动式智能体要么要求用户进一步澄清（增加负担），要么从上下文推断未来需求（常导致不必要或时机不当的干预）。

Method: 提出ProPer双智能体架构：1) 维度生成智能体（DGA）：基于微调LLM，利用显性用户数据生成多个隐性维度（用户任务相关但未考虑的潜在方面）或知识缺口；2) 使用基于质量、多样性和任务相关性的重排序器筛选维度；3) 响应生成智能体（RGA）：平衡显性和隐性维度，生成具有及时主动干预的个性化响应。

Result: 在多领域评估中，ProPer在覆盖度、主动性适当性和意图对齐等结构化缺口感知指标上均表现优异。质量得分和胜率在所有领域均有提升，单轮评估中最高提升84%，多轮交互中持续占优。

Conclusion: ProPer通过双智能体架构有效识别并响应用户未表达的潜在需求，实现了更全面、及时且个性化的主动式辅助，显著优于现有被动和主动式方法。

Abstract: Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.

</details>


### [35] [Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains](https://arxiv.org/abs/2601.09946)
*Chenxi Qiu*

Main category: cs.LG

TL;DR: 提出基于插值的框架优化lp范数度量差分隐私，通过锚点优化和log凸组合插值，在细粒度连续域中实现高效隐私保护


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的度量差分隐私方法在粗粒度域中有效，但在细粒度或连续域中面临挑战，因为需要构建密集扰动矩阵并满足逐点约束，计算成本高

Method: 提出插值框架：在稀疏锚点集优化扰动分布，通过log凸组合插值非锚点分布；针对高维空间隐私违规问题，将插值分解为一系列一维步骤，推导出强制lp范数mDP的校正公式；进一步探索扰动分布与隐私预算跨维度分配的联合优化

Result: 在真实世界位置数据集上的实验表明，该方法在细粒度域中提供严格的隐私保证和竞争性效用，优于基线机制

Conclusion: 提出的插值框架有效解决了细粒度连续域中度量差分隐私的优化挑战，通过锚点优化和校正插值实现了高效且可证明的隐私保护

Abstract: Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.
  In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.

</details>


### [36] [Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series](https://arxiv.org/abs/2601.09949)
*Griffin Kearney*

Main category: cs.LG

TL;DR: 提出Kinematic Tokenization方法，将连续时间信号通过样条重建表示为位置、速度、加速度等运动学系数，在噪声金融时间序列中相比离散标记化方法能维持稳定的决策策略


<details>
  <summary>Details</summary>
Motivation: Transformer设计用于离散标记，但现实世界信号是连续过程且存在噪声采样。离散标记化方法（原始值、分块、有限差分）在低信噪比环境下脆弱，特别是当下游目标施加非对称惩罚时，理性策略会选择弃权

Method: 提出Kinematic Tokenization方法：基于优化的连续时间表示，从噪声测量中重建显式样条，并将局部样条系数（位置、速度、加速度、急动度）作为标记。应用于金融时间序列数据（资产价格和交易量）

Result: 在多资产日频股票测试平台上，使用风险厌恶的非对称分类目标作为可学习性压力测试。在该目标下，多个离散基线方法崩溃为吸收性现金策略（清算均衡），而连续样条标记能维持校准的非平凡动作分布和稳定策略

Conclusion: 显式连续时间标记可以改善噪声时间序列中在弃权诱导损失下的选择性决策策略的可学习性和校准性

Abstract: Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.

</details>


### [37] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 论文指出机器学习价值链存在结构性不可持续问题，源于数据加工过程中的经济不平等：从输入数据到模型权重再到合成输出的每个阶段都增强了技术信号，但剥夺了数据生成者的经济权益。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习数据价值链存在严重的经济不平等问题，数据生成者获得的报酬极少甚至为零，而价值主要流向数据聚合者。这种不平等不仅影响经济福利，还威胁到当前学习算法赖以生存的反馈循环的可持续性。

Method: 分析73个公开数据交易案例，识别数据价值链中的结构性缺陷，提出公平数据价值交换（EDVEX）框架，并规划研究方向和社区贡献路径。

Result: 研究发现大多数价值流向聚合者，创作者版税几乎为零，交易条款普遍不透明。识别出三个结构性缺陷：缺失溯源、不对称议价能力和非动态定价。

Conclusion: 需要建立公平的数据价值交换框架来创建惠及所有参与者的最小市场，并呼吁研究社区在数据交易领域做出具体贡献。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [38] [An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series Classification](https://arxiv.org/abs/2601.09971)
*Hansen He,Shuheng Li*

Main category: cs.LG

TL;DR: 该研究探索了将专门的时间序列编码器与冻结的大型语言模型（LLM）主干相结合的混合架构，发现Inception模型是唯一能持续带来性能提升的编码器架构。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类（TSC）是机器学习核心问题，现有研究主要关注将时间序列数据映射到文本域的校准策略，而时间序列编码器架构的选择尚未得到充分探索。研究者希望探索专门的时间序列编码器与LLM结合的混合架构效果。

Method: 采用探索性研究方法，评估多种编码器家族（包括Inception、卷积、残差、基于Transformer和多层感知机架构）与冻结LLM主干结合的混合架构。通过实验比较不同编码器在时间序列分类任务中的表现。

Result: 在所有评估的编码器架构中，只有Inception模型在与LLM主干集成时能持续产生正向性能提升。其他编码器架构（卷积、残差、Transformer、MLP等）未能表现出类似的稳定优势。

Conclusion: 时间序列编码器的选择对混合LLM架构有重要影响，Inception-based模型是未来LLM驱动时间序列学习的有前景方向。该研究强调了专门时间序列编码器在LLM集成中的关键作用。

Abstract: Time series classification (TSC) is a core machine learning problem with broad applications. Recently there has been growing interest in repurposing large language models (LLMs) for TSC, motivated by their strong reasoning and generalization ability. Prior work has primarily focused on alignment strategies that explicitly map time series data into the textual domain; however, the choice of time series encoder architecture remains underexplored. In this work, we conduct an exploratory study of hybrid architectures that combine specialized time series encoders with a frozen LLM backbone. We evaluate a diverse set of encoder families, including Inception, convolutional, residual, transformer-based, and multilayer perceptron architectures, among which the Inception model is the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone. Overall, this study highlights the impact of time series encoder choice in hybrid LLM architectures and points to Inception-based models as a promising direction for future LLM-driven time series learning.

</details>


### [39] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出基于概率测度空间的上下文算子学习框架，用于最优传输问题，通过少量样本提示学习从分布对到OT映射的算子，无需推理时的梯度更新。


<details>
  <summary>Details</summary>
Motivation: 传统最优传输方法通常需要为每个新的分布对重新计算OT映射，计算成本高。本文旨在开发一个通用的解决方案算子，能够通过少量样本提示（上下文学习）快速适应新任务，实现高效的最优传输计算。

Method: 1. 提出概率测度空间上的上下文算子学习框架；2. 在非参数设置下，当任务集中在低内在维度流形上时，建立泛化边界理论；3. 在参数设置下（如高斯族），给出显式架构精确恢复OT映射；4. 开发参数化解方案算子；5. 分析上下文精度与提示大小、任务内在维度和模型容量的缩放关系。

Result: 1. 建立了非参数设置下的泛化边界，量化了上下文精度与提示大小、内在任务维度和模型容量的缩放关系；2. 在参数设置下给出了能精确恢复OT映射的显式架构，并提供有限样本超额风险边界；3. 在合成传输和生成建模基准上的数值实验验证了框架的有效性。

Conclusion: 本文提出的上下文算子学习框架为最优传输问题提供了一种高效的学习方法，能够通过少量样本提示快速适应新任务，在非参数和参数设置下均建立了理论保证，并通过实验验证了其实际有效性。

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


### [40] [FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems](https://arxiv.org/abs/2601.09985)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FaTRQ是一个面向远内存感知的近似最近邻搜索精炼系统，通过分层内存消除从存储中获取完整向量的需求，使用渐进距离估计器和分层残差量化，在CXL Type-2设备上部署定制加速器，显著提升存储效率和查询吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代ANNS引擎虽然使用预构建索引和压缩向量量化表示加速搜索，但仍依赖昂贵的二次精炼阶段，需要从较慢存储（如SSD）读取完整精度向量。对于现代文本和多模态嵌入，这些读取操作已成为整个查询延迟的主要瓶颈。

Method: 1) 提出渐进距离估计器，使用从远内存流式传输的紧凑残差来精炼粗略分数；2) 引入分层残差量化，将残差编码为三元值并高效存储在远内存中；3) 在CXL Type-2设备上部署定制加速器，执行低延迟本地精炼；4) 当候选向量被证明不在top-k范围内时，精炼过程提前停止。

Result: FaTRQ将存储效率提高了2.4倍，与最先进的GPU ANNS系统相比，吞吐量提升了高达9倍。

Conclusion: FaTRQ通过消除从存储获取完整向量的需求，解决了ANNS中二次精炼阶段的瓶颈问题，利用分层内存架构和渐进精炼策略，显著提升了检索增强生成(RAG)中近似最近邻搜索的性能和效率。

Abstract: Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed vector-quantized representations in fast memory. However, they still rely on a costly second-pass refinement stage that reads full-precision vectors from slower storage like SSDs. For modern text and multimodal embeddings, these reads now dominate the latency of the entire query. We propose FaTRQ, a far-memory-aware refinement system using tiered memory that eliminates the need to fetch full vectors from storage. It introduces a progressive distance estimator that refines coarse scores using compact residuals streamed from far memory. Refinement stops early once a candidate is provably outside the top-k. To support this, we propose tiered residual quantization, which encodes residuals as ternary values stored efficiently in far memory. A custom accelerator is deployed in a CXL Type-2 device to perform low-latency refinement locally. Together, FaTRQ improves the storage efficiency by 2.4$\times$ and improves the throughput by up to 9$ \times$ than SOTA GPU ANNS system.

</details>


### [41] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出混合Transformer架构，用连续深度神经ODE块替代离散中间层，通过学习到的控制信号实现推理时生成属性控制。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer通过固定离散层处理表示，缺乏推理时对生成属性的灵活控制能力。需要一种能够将深度作为连续变量处理，并通过控制信号实现属性控制的架构。

Method: 设计混合Transformer架构，用连续深度神经ODE块替换离散中间层。使用学习向量场F_θ(H, τ, u)，其中u是通过显式拼接注入的低维控制信号。采用伴随方法实现O(1)内存训练，与积分深度无关。

Result: 梯度流稳定性：零梯度爆炸/消失事件；语义控制：正/负情感控制准确率达98%/88%；连续插值：固定与自适应求解器轨迹差异仅0.068%；效率基准：延迟与标准离散基线相当。自适应ODE求解器揭示学习动态的几何结构。

Conclusion: 具有学习控制信号的连续深度动态为可控语言生成提供了可行、高效的机制，控制信号将向量场划分为具有不同曲率特征的动态机制。

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [42] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 该研究比较了点击率预测中XGBoost模型的时间聚合特征，发现在Avazu数据集上，使用滑动窗口的时间聚合特征相比目标编码基线能提升ROC AUC约0.0066-0.0082，事件计数窗口提供小幅额外增益。


<details>
  <summary>Details</summary>
Motivation: 研究点击率预测中时间聚合特征的有效性，在严格的时间外分割和无前瞻特征约束下，比较不同窗口设计对模型性能的影响，为实际应用提供指导。

Method: 使用Avazu点击率预测数据集，采用严格的时间外分割和无前瞻特征约束（小时H的特征仅使用H之前的数据）。比较了强时间感知目标编码基线与添加实体历史时间聚合特征的模型，测试了多种窗口设计（滑动窗口、事件计数窗口、间隔窗口、分桶窗口）。在确定性的10%样本上使用两个滚动尾部折叠进行评估。

Result: 滑动窗口规格相比仅使用目标编码，ROC AUC提升约0.0066-0.0082，PR AUC提升约0.0084-0.0094。在时间聚合设计网格中，事件计数窗口是唯一能持续改进滑动窗口的方法，但增益较小。间隔窗口和分桶窗口在该数据集和协议下表现不如简单滑动窗口。

Conclusion: 研究结果支持将滑动窗口作为实用默认选择，当边际ROC AUC增益重要时可选择添加事件计数窗口。更复杂的时间聚合设计（如间隔窗口和分桶窗口）在该设置下未能提供额外价值。

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [43] [BPE: Behavioral Profiling Ensemble](https://arxiv.org/abs/2601.10024)
*Yanxin Liu,Yunqi Zhang*

Main category: cs.LG

TL;DR: 提出行为画像集成框架，通过构建模型内在行为画像，基于测试实例响应与行为画像的偏差进行集成，超越传统基于模型间差异的集成方法。


<details>
  <summary>Details</summary>
Motivation: 传统静态集成方法将每个基学习器视为整体分配权重，忽略了模型在不同实例空间区域的能力差异。动态集成选择虽然考虑了这种差异，但传统方法主要依赖模型间的差异进行集成，忽视了模型内在特性，且严重依赖验证集进行能力估计。

Method: 提出行为画像集成框架，为每个模型构建内在的"行为画像"，基于模型对特定测试实例的响应与其已建立的行为画像之间的偏差来推导集成权重。这种方法从模型间差异的视角转向模型内在行为特性的视角。

Result: 在合成和真实世界数据集上的大量实验表明，基于BPE框架的算法相比最先进的集成基线方法取得了显著改进。这些改进不仅体现在预测准确性上，还包括计算效率和存储资源利用方面。

Conclusion: BPE框架通过关注模型内在行为特性而非模型间差异，提供了一种新颖的集成学习范式，在准确性、效率和资源利用方面均优于传统方法。

Abstract: Ensemble learning is widely recognized as a pivotal strategy for pushing the boundaries of predictive performance. Traditional static ensemble methods, such as Stacking, typically assign weights by treating each base learner as a holistic entity, thereby overlooking the fact that individual models exhibit varying degrees of competence across different regions of the instance space. To address this limitation, Dynamic Ensemble Selection (DES) was introduced. However, both static and dynamic approaches predominantly rely on the divergence among different models as the basis for integration. This inter-model perspective neglects the intrinsic characteristics of the models themselves and necessitates a heavy reliance on validation sets for competence estimation. In this paper, we propose the Behavioral Profiling Ensemble (BPE) framework, which introduces a novel paradigm shift. Unlike traditional methods, BPE constructs a ``behavioral profile'' intrinsic to each model and derives integration weights based on the deviation between the model's response to a specific test instance and its established behavioral profile. Extensive experiments on both synthetic and real-world datasets demonstrate that the algorithm derived from the BPE framework achieves significant improvements over state-of-the-art ensemble baselines. These gains are evident not only in predictive accuracy but also in computational efficiency and storage resource utilization across various scenarios.

</details>


### [44] [Unlabeled Data Can Provably Enhance In-Context Learning of Transformers](https://arxiv.org/abs/2601.10058)
*Renpu Liu,Jing Yang*

Main category: cs.LG

TL;DR: 提出增强型上下文学习框架，将少量标注示例与大量未标注数据结合，通过思维链提示使Transformer隐式执行EM算法，理论上提升ICL性能


<details>
  <summary>Details</summary>
Motivation: 传统上下文学习受限于提示中能容纳的少量昂贵标注示例，而现实中存在大量与任务相关的未标注数据。如何利用这些未标注数据来理论保证地提升ICL性能成为一个关键问题

Method: 提出增强型ICL框架，提示中包含少量标注示例和大量未标注输入。在多类线性分类设置下，通过思维链提示使多层Transformer隐式执行期望最大化算法，从标注和未标注数据中提取有用信息。使用教师强制训练Transformer，参数以线性速率收敛到期望解

Result: 增强型ICL框架在实验中始终优于传统少样本ICL，为理论发现提供了实证支持。这是首个关于未标注数据对Transformer ICL性能影响的理论研究

Conclusion: 通过结合少量标注示例和大量未标注数据，并利用思维链提示使Transformer隐式执行EM算法，可以理论保证地提升上下文学习性能，为利用未标注数据增强LLM能力提供了新途径

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.

</details>


### [45] [Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection](https://arxiv.org/abs/2601.10067)
*Hung Vinh Tran,Tong Chen,Hechuan Wen,Quoc Viet Hung Nguyen,Bin Cui,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出NaCS框架，通过子模优化构建核心集并校正噪声标签，仅用1%数据即可恢复93-95%全数据训练性能


<details>
  <summary>Details</summary>
Motivation: 基于内容的推荐系统需要大规模持续训练以适应多样用户偏好，计算成本高；现有核心集选择方法对用户-物品交互噪声敏感，特别是当核心集规模较小时

Method: 提出噪声感知核心集选择(NaCS)框架：1)基于训练梯度的子模优化构建核心集；2)使用渐进训练模型校正噪声标签；3)通过不确定性量化过滤低置信度样本，避免不可靠交互训练

Result: NaCS在仅使用1%训练数据的情况下，能够恢复93-95%的全数据集训练性能，优于现有核心集选择技术

Conclusion: NaCS为基于内容的推荐系统提供了高质量、高效的核心集选择方案，能有效处理交互噪声问题，显著降低训练开销

Abstract: Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommodate diverse user preferences, resulting in significant computational costs and resource demands. A promising approach to this challenge is coreset selection, which identifies a small but representative subset of data samples that preserves model quality while reducing training overhead. Yet, the selected coreset is vulnerable to the pervasive noise in user-item interactions, particularly when it is minimally sized. To this end, we propose Noise-aware Coreset Selection (NaCS), a specialized framework for CRSs. NaCS constructs coresets through submodular optimization based on training gradients, while simultaneously correcting noisy labels using a progressively trained model. Meanwhile, we refine the selected coreset by filtering out low-confidence samples through uncertainty quantification, thereby avoid training with unreliable interactions. Through extensive experiments, we show that NaCS produces higher-quality coresets for CRSs while achieving better efficiency than existing coreset selection techniques. Notably, NaCS recovers 93-95\% of full-dataset training performance using merely 1\% of the training data. The source code is available at \href{https://github.com/chenxing1999/nacs}{https://github.com/chenxing1999/nacs}.

</details>


### [46] [Comparative Evaluation of Deep Learning-Based and WHO-Informed Approaches for Sperm Morphology Assessment](https://arxiv.org/abs/2601.10070)
*Mohammad Abbadi*

Main category: cs.LG

TL;DR: 该研究比较了基于图像的深度学习模型HuSHeM与WHO(+SIRI)基线方法在精子形态质量评估中的性能，发现深度学习模型在判别性能、校准和临床效用方面均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 精子形态质量评估是男性生育力评估的关键但主观的组成部分，常受观察者间变异性和资源限制的影响。传统方法存在主观性和可重复性问题，需要更客观、可靠的评估工具。

Method: 研究提出了一个比较性生物医学人工智能框架，评估基于图像的深度学习模型HuSHeM与临床基线方法WHO(+SIRI)。HuSHeM模型在高分辨率精子形态图像上训练，并在独立临床队列中评估。性能评估包括判别分析、校准分析和临床效用分析。

Result: HuSHeM模型表现出更高的判别性能，ROC曲线下面积更大且置信区间更窄。在类别不平衡情况下，精确率-召回率分析显示性能改善。校准分析表明预测概率与观察结果更一致，决策曲线分析显示在临床相关阈值概率范围内具有更大的净临床效益。

Conclusion: 基于图像的深度学习相比传统规则基础和炎症增强标准可能提供更好的预测可靠性和临床效用。该框架支持精子形态的客观和可重复评估，可作为生育筛查和转诊工作流程中的决策支持工具，但并非旨在替代临床判断或实验室评估。

Abstract: Assessment of sperm morphological quality remains a critical yet subjective component of male fertility evaluation, often limited by inter-observer variability and resource constraints. This study presents a comparative biomedical artificial intelligence framework evaluating an image-based deep learning model (HuSHeM) alongside a clinically grounded baseline derived from World Health Organization criteria augmented with the Systemic Inflammation Response Index (WHO(+SIRI)).
  The HuSHeM model was trained on high-resolution sperm morphology images and evaluated using an independent clinical cohort. Model performance was assessed using discrimination, calibration, and clinical utility analyses. The HuSHeM model demonstrated higher discriminative performance, as reflected by an increased area under the receiver operating characteristic curve with relatively narrow confidence intervals compared to WHO(+SIRI). Precision-recall analysis further indicated improved performance under class imbalance, with higher precision-recall area values across evaluated thresholds. Calibration analysis indicated closer agreement between predicted probabilities and observed outcomes for HuSHeM, while decision curve analysis suggested greater net clinical benefit across clinically relevant threshold probabilities.
  These findings suggest that image-based deep learning may offer improved predictive reliability and clinical utility compared with traditional rule-based and inflammation-augmented criteria. The proposed framework supports objective and reproducible assessment of sperm morphology and may serve as a decision-support tool within fertility screening and referral workflows. The proposed models are intended as decision-support or referral tools and are not designed to replace clinical judgment or laboratory assessment.

</details>


### [47] [Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts](https://arxiv.org/abs/2601.10079)
*Sijia Luo,Xiaokang Zhang,Yuxuan Hu,Bohan Zhang,Ke Wang,Jinbo Su,Mengshu Sun,Lei Liang,Jing Zhang*

Main category: cs.LG

TL;DR: Sparse-RL：一种在稀疏KV缓存下实现稳定强化学习训练的方法，通过稀疏感知拒绝采样和重要性重加权来纠正压缩引起的策略失配问题


<details>
  <summary>Details</summary>
Motivation: 强化学习在激发大语言模型复杂推理能力方面至关重要，但在长序列rollout过程中存储KV缓存的内存开销成为关键瓶颈，限制了在有限硬件上的高效训练。现有的KV压缩技术虽然适用于推理，但直接应用于RL训练会导致严重的策略失配和性能崩溃。

Method: Sparse-RL通过稀疏感知拒绝采样和基于重要性的重加权来纠正压缩引起的信息损失带来的离策略偏差。该方法解决了密集旧策略、稀疏采样器策略和学习器策略之间的根本性策略失配问题。

Result: 实验结果表明，Sparse-RL相比密集基线显著降低了rollout开销，同时保持了性能。此外，Sparse-RL通过稀疏感知训练，显著增强了模型在稀疏推理部署时的鲁棒性。

Conclusion: Sparse-RL成功解决了在稀疏KV缓存下RL训练的稳定性问题，通过纠正策略失配实现了高效训练，并为稀疏推理部署提供了鲁棒性增强。

Abstract: Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.

</details>


### [48] [Bayesian Meta-Analyses Could Be More: A Case Study in Trial of Labor After a Cesarean-section Outcomes and Complications](https://arxiv.org/abs/2601.10089)
*Ashley Klein,Edward Raff,Marcia DesJardin*

Main category: cs.LG

TL;DR: 论文提出了一种贝叶斯方法来处理医学研究中关键决策变量未被记录的情况，通过该方法可以评估阳性效应主张是否仍然成立，并以剖宫产后试产(TOLAC)决策为例验证了其效用。


<details>
  <summary>Details</summary>
Motivation: 医学研究中，元分析的可靠性依赖于先前研究是否准确捕捉了感兴趣的变量。然而在实际医学研究中，影响医生决策的关键变量常常未被记录，导致效应大小未知和结论不可靠。特别是在剖宫产后试产(TOLAC)等临床情境中，缺乏足够的干预措施，需要更可靠的方法来支持临床决策。

Method: 开发了一种贝叶斯分析方法来处理医学研究中关键决策变量缺失的常见场景。该方法允许在变量未被记录的情况下，通过贝叶斯推断来评估阳性效应主张是否仍然成立。具体应用于剖宫产后试产(TOLAC)的临床决策支持，协助产科医生在干预措施有限的情况下做出更可靠的决策。

Result: 该方法能够为医生提供必要的支持来推进患者护理。在TOLAC情境中，贝叶斯方法帮助产科医生在关键变量缺失的情况下评估试产的可行性和风险，从而做出更明智的临床决策。

Conclusion: 贝叶斯方法为解决医学研究中关键变量未被记录的问题提供了有效途径，能够提高元分析的可靠性，并为临床决策提供更坚实的证据基础，特别是在干预措施有限的复杂医疗情境中。

Abstract: The meta-analysis's utility is dependent on previous studies having accurately captured the variables of interest, but in medical studies, a key decision variable that impacts a physician's decisions was not captured. This results in an unknown effect size and unreliable conclusions. A Bayesian approach may allow analysis to determine if the claim of a positive effect is still warranted, and we build a Bayesian approach to this common medical scenario. To demonstrate its utility, we assist professional OBGYNs in evaluating Trial of Labor After a Cesarean-section (TOLAC) situations where few interventions are available for patients and find the support needed for physicians to advance patient care.

</details>


### [49] [LeMoF: Level-guided Multimodal Fusion for Heterogeneous Clinical Data](https://arxiv.org/abs/2601.10092)
*Jongseok Kim,Seongae Kang,Jonghwan Shin,Yuhan Lee,Ohyun Jo*

Main category: cs.LG

TL;DR: LeMoF（Level-guided Modal Fusion）是一种新颖的多模态临床预测框架，通过选择性整合每个模态内不同编码器层提取的层级表示，实现全局模态级预测与层级特定判别表示的分离学习，在ICU住院时间预测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态临床预测方法通常依赖静态的模态整合方案和简单的融合策略，未能充分利用模态特定的表示。在整合电子健康记录（EHR）和生物信号等异构数据时，这些方法无法充分挖掘模态特定的表示潜力。

Method: 提出LeMoF框架，选择性整合每个模态内层级引导的表示。每个层级对应编码器不同层提取的表示。框架明确分离并学习全局模态级预测和层级特定的判别表示，实现预测稳定性与判别能力之间的平衡。

Result: 在ICU住院时间预测实验中，LeMoF在各种编码器配置下始终优于现有的最先进多模态融合技术。验证了层级式整合是实现跨不同临床条件稳健预测性能的关键因素。

Conclusion: LeMoF通过层级引导的模态融合，能够充分利用模态特定的表示，在异构临床环境中实现预测稳定性与判别能力的平衡，为多模态临床预测提供了更有效的解决方案。

Abstract: Multimodal clinical prediction is widely used to integrate heterogeneous data such as Electronic Health Records (EHR) and biosignals. However, existing methods tend to rely on static modality integration schemes and simple fusion strategies. As a result, they fail to fully exploit modality-specific representations. In this paper, we propose Level-guided Modal Fusion (LeMoF), a novel framework that selectively integrates level-guided representations within each modality. Each level refers to a representation extracted from a different layer of the encoder. LeMoF explicitly separates and learns global modality-level predictions from level-specific discriminative representations. This design enables LeMoF to achieve a balanced performance between prediction stability and discriminative capability even in heterogeneous clinical environments. Experiments on length of stay prediction using Intensive Care Unit (ICU) data demonstrate that LeMoF consistently outperforms existing state-of-the-art multimodal fusion techniques across various encoder configurations. We also confirmed that level-wise integration is a key factor in achieving robust predictive performance across various clinical conditions.

</details>


### [50] [Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text](https://arxiv.org/abs/2601.10096)
*Piyush Singh Pasi*

Main category: cs.LG

TL;DR: METAL：一种轻量级对齐方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间，实现强大的零样本跨语言迁移性能。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在英语上表现出色，但在其他语言上性能急剧下降，主要原因是缺乏多语言多模态数据。现有解决方案过度依赖机器翻译，而多语言文本建模的进展尚未得到充分利用。

Method: 提出METAL方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间。该方法简单轻量，通过线性变换实现多语言文本与多模态表示的对齐。

Result: METAL在英语文本-图像检索上达到94.9% Recall@10，在11种语言（其中10种未见）上平均达到89.5% Recall@10。t-SNE可视化显示多语言嵌入与多模态表示紧密对齐，权重分析表明变换重塑了嵌入几何而非简单旋转。方法还推广到音频-文本检索和跨语言文本-图像生成。

Conclusion: METAL提供了一种简单有效的多语言多模态对齐方法，仅需英语文本即可实现强大的跨语言迁移。作者发布了代码、检查点和多语言评估数据集，促进该领域进一步研究。

Abstract: Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.

</details>


### [51] [Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation](https://arxiv.org/abs/2601.10137)
*Ziyi Ding,Chenfei Ye-Hao,Zheyuan Wang,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: Tree-Query：基于树状多专家LLM框架的因果发现方法，通过结构化查询序列减少误差传播，提供可解释判断和鲁棒性感知置信度评分


<details>
  <summary>Details</summary>
Motivation: 传统基于约束的因果发现方法（如PC、FCI）存在误差传播问题，而近期基于LLM的因果预言机通常表现为不透明、无置信度的黑盒系统，需要一种更可靠、可解释的数据无关因果发现方法

Method: 提出Tree-Query框架，将成对因果发现转化为关于后门路径、(不)依赖性、潜在混杂和因果方向的短序列查询，采用树状多专家LLM结构，提供理论保证和鲁棒性感知置信度评分

Result: 在基于Mooij等人和UCI因果图的数据无关基准测试中，Tree-Query在结构指标上优于直接LLM基线；饮食-体重案例研究展示了混杂因素筛选和稳定、高置信度的因果结论

Conclusion: Tree-Query提供了一种原则性方法，可从LLM获取数据无关的因果先验知识，补充下游数据驱动的因果发现，实现更可靠、可解释的因果推断

Abstract: Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.

</details>


### [52] [Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141)
*Jiawen Zhang,Yangfan Hu,Kejia Chen,Lipeng He,Jiachen Ma,Jian Lou,Dan Li,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 提出SPF方法解决LLM微调中的安全-效用困境，通过几何分析发现安全梯度位于低秩子空间并与效用梯度冲突，SPF通过移除冲突梯度分量在保持任务性能的同时恢复安全对齐。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调会显著降低安全对齐性，即使微调数据完全无害也会增加越狱攻击的脆弱性。现有方法面临安全-效用困境：强调安全会损害任务性能，而优先考虑效用通常需要深度微调，不可避免地导致安全性急剧下降。

Method: 提出安全保护微调(SPF)方法，基于三个关键发现：(1)安全梯度位于低秩子空间，而效用梯度跨越更广泛的高维空间；(2)这些子空间通常负相关，导致微调过程中的方向冲突；(3)主导安全方向可以从单个样本中有效估计。SPF通过显式移除与低秩安全子空间冲突的梯度分量来实现。

Result: 理论上证明SPF保证效用收敛同时限制安全漂移。实证上，SPF始终保持下游任务性能，并恢复几乎所有预训练安全对齐，即使在对抗性微调场景下也有效。SPF对深度微调和动态越狱攻击表现出强大的抵抗能力。

Conclusion: 研究提供了对始终对齐的LLM微调的新机制理解和实践指导，通过几何分析揭示了安全-效用梯度的相互作用，提出的SPF方法有效解决了微调中的安全-效用困境。

Abstract: Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination.
  In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.

</details>


### [53] [LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers](https://arxiv.org/abs/2601.10155)
*Aryan Karmore*

Main category: cs.LG

TL;DR: LOOKAT：一种基于乘积量化的KV缓存压缩方法，将注意力计算转化为查表操作，实现64倍压缩同时保持95.7%输出保真度


<details>
  <summary>Details</summary>
Motivation: 现有量化方法虽然压缩了KV缓存的存储空间，但无法减少带宽，因为注意力计算需要将INT4/INT8量化的键向量反量化为FP16格式。这限制了大型语言模型在边缘设备上的部署。

Method: 将注意力评分视为内积相似度搜索，应用向量数据库中的乘积量化和非对称距离计算技术。将键向量分解为子空间，学习码本，通过查表计算注意力表，将注意力计算从内存密集型转为计算密集型。

Result: 在GPT-2上测试，实现64倍压缩时输出保真度95.7%，32倍压缩时95.0%保真度。无需架构修改或训练，保持秩相关性ρ>0.95。理论分析显示秩相关性随dk/mK退化。

Conclusion: LOOKAT通过乘积量化和查表机制有效压缩KV缓存，显著减少带宽需求，使大型语言模型更适合边缘设备部署，同时保持高输出质量。

Abstract: Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\times$ compression at 95.7\% output fidelity and 32 $\times$ compression at 95.0\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $ρ> 0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.

</details>


### [54] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: 提出Graph Regularized PCA (GR-PCA)，通过图正则化处理高维数据中非各向同性噪声问题，利用稀疏精度图学习数据特征依赖结构，使主成分与条件关系对齐。


<details>
  <summary>Details</summary>
Motivation: 高维数据中变量间的依赖关系常常违反PCA所基于的各向同性噪声假设。当噪声在特征间不是独立同分布时，传统PCA不再最优，需要一种能够利用数据特征依赖结构的方法。

Method: GR-PCA通过图正则化PCA实现：1) 学习稀疏精度图捕捉数据特征依赖结构；2) 通过图拉普拉斯惩罚项使载荷偏向低频傅里叶模式；3) 抑制高频信号，保留图相干低频信号；4) 方法实现简单，对精度估计器模块化，可扩展性强。

Result: 在合成数据上评估显示：1) 在预期支撑上集中方差；2) 产生具有较低图拉普拉斯能量的载荷；3) 样本外重建保持竞争力；4) 当存在高频信号时，图拉普拉斯惩罚防止过拟合，提高结构保真度；5) 当高频信号图相关时，相比PCA优势最明显。

Conclusion: GR-PCA提供了一种实用的结构感知降维方法，在不牺牲预测性能的情况下提高结构保真度。当高频信号图相关时相比PCA有明显优势，而高频信号近似旋转不变时PCA仍具竞争力。

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


### [55] [PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary](https://arxiv.org/abs/2601.10201)
*Jiarui Yao,Ruida Wang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出Process Reward Learning (PRL)方法，将熵正则化强化学习目标分解为中间步骤，为LLM推理过程提供细粒度监督，无需额外步骤如MCTS或单独奖励模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理能力提升方法大多基于轨迹级结果奖励，缺乏推理过程中的细粒度监督。其他结合过程信号的训练框架依赖繁琐额外步骤（如MCTS、训练单独奖励模型），降低训练效率，且过程信号设计缺乏严格理论支持。

Method: 提出Process Reward Learning (PRL)，从理论动机出发推导公式，将熵正则化强化学习目标分解为中间步骤，分配严格的过程奖励。PRL本质上等价于奖励最大化目标加上策略模型与参考模型之间的KL散度惩罚项，但能将结果奖励转化为过程监督信号。

Result: 实验表明PRL不仅提高了LLM推理能力的平均性能（通过average @ n衡量），还通过改善pass @ n指标扩展了推理边界。大量实验验证了PRL的有效性和泛化能力。

Conclusion: PRL为LLM推理过程提供细粒度监督，无需繁琐额外步骤，具有严格理论支持，能有效提升推理性能并扩展推理边界。

Abstract: Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.

</details>


### [56] [Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD](https://arxiv.org/abs/2601.10237)
*Murat Bilgehan Ertan,Marten van Dijk*

Main category: cs.LG

TL;DR: DP-SGD在f-差分隐私框架下存在基本限制：在单轮洗牌采样中，实现强隐私（小κ）与高效用之间存在严格权衡，噪声乘子σ有下界，导致实际训练中准确率显著下降。


<details>
  <summary>Details</summary>
Motivation: DP-SGD是私有训练的主流方法，但其在最坏情况对抗性隐私定义下的基本限制尚未被充分理解。研究者希望分析DP-SGD在f-差分隐私框架下的隐私-效用权衡，揭示其根本局限性。

Method: 在f-差分隐私框架下分析DP-SGD，研究单轮洗牌采样（M次梯度更新）。推导可实现权衡曲线的显式次优上界，进而得到分离度κ的几何下界。分析高斯噪声乘子σ与分离度κ之间的严格下界关系，并将结果扩展到泊松子采样。

Result: 证明在最坏情况对抗模型下，洗牌DP-SGD必须满足σ ≥ 1/√(2ln M)或κ ≥ (1/√8)(1-1/√(4πln M))，无法同时实现强隐私和高效用。该界限随M→∞渐近消失但收敛极慢，实际更新次数下所需噪声幅度仍然很大。实验证实该噪声水平导致实际训练中准确率显著下降。

Conclusion: DP-SGD在标准最坏情况对抗性假设下存在关键瓶颈：强隐私与高效用之间存在根本性权衡，噪声下界限制了实际应用中的效用，需要重新思考对抗性隐私模型或开发新方法。

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy
  $σ\ge \frac{1}{\sqrt{2\ln M}}$ $\quad\text{or}\quad$ $κ\ge\ \frac{1}{\sqrt{8}}\!\left(1-\frac{1}{\sqrt{4π\ln M}}\right)$,
  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \to \infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.

</details>


### [57] [X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction](https://arxiv.org/abs/2601.10251)
*Hongru Duan,Yongle Chen,Lei Guan*

Main category: cs.LG

TL;DR: 提出X-SAM方法，通过沿Hessian主特征向量正交分解修正梯度，更直接有效地正则化最大特征值，解决传统SAM可能失效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统SAM方法在训练时可能无法达到预期效果，因为尖锐和平坦区域都可能产生较小的扰动损失，导致梯度仍指向尖锐区域。需要从谱和几何角度分析SAM的局限性。

Method: 提出显式特征向量对齐的SAM（X-SAM），利用梯度与Hessian主特征向量夹角作为尖锐度度量，通过沿主特征向量正交分解修正梯度，更直接正则化Hessian最大特征值。

Result: 证明了X-SAM的收敛性和优越的泛化性能，通过大量实验验证了理论和实践优势，相比传统SAM有更好的优化效果。

Conclusion: X-SAM通过谱和几何视角解决了SAM的局限性，提供更有效的尖锐度正则化方法，在理论和实验上都表现出优越性。

Abstract: Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.

</details>


### [58] [Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders](https://arxiv.org/abs/2601.10269)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 提出无需故障标签的无监督涡扇发动机健康监测框架，通过回归归一化消除工况影响，使用LSTM自编码器在健康数据上训练，基于自适应阈值实现实时预警


<details>
  <summary>Details</summary>
Motivation: 传统发动机健康监测方法需要故障标签进行训练，但实际应用中获取完整的故障数据成本高昂且困难。需要开发无需故障标签的无监督方法，能够快速部署并适应不同机队

Method: 1) 使用基于回归的归一化方法消除NASA CMAPSS传感器数据中的工况影响；2) 仅使用每个轨迹的健康部分训练LSTM自编码器；3) 采用自适应数据驱动阈值估计持续重构误差，触发实时警报，无需手动调参规则

Result: 基准测试显示该方法在多种运行状态下均具有高召回率和低误报率，能够快速部署、适应不同机队规模，可作为剩余使用寿命模型的补充预警层

Conclusion: 该无监督健康监测框架无需故障标签即可有效检测发动机异常，具有快速部署、可扩展性强等优点，可作为现有剩余使用寿命预测模型的补充预警系统

Abstract: This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.

</details>


### [59] [Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers](https://arxiv.org/abs/2601.10274)
*Emre Ozbas,Melih Bastopcu*

Main category: cs.LG

TL;DR: 研究大型语言模型服务器中异构查询任务的令牌分配优化问题，通过平衡准确率与延迟，在队列稳定性约束下最大化加权准确率目标。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务器需要处理多种类型的查询任务，每种任务需要不同的计算资源（思考令牌）。如何在不同任务类型之间分配有限的令牌资源，以平衡响应准确率和系统延迟，同时保证队列稳定性，是一个重要的优化问题。

Method: 将系统建模为M/G/1队列，服务时间与分配的令牌数近似线性相关。建立约束优化问题，最大化加权平均准确率（受惩罚于平均系统时间），约束包括令牌预算和队列稳定性。证明目标函数在稳定区域内严格凹，确保最优解存在唯一。通过一阶最优条件得到耦合投影定点特性，开发迭代解法和投影梯度法，最后通过四舍五入得到整数令牌分配。

Result: 证明了优化问题的严格凹性，确保了最优解的存在唯一性。推导出最优解的一阶条件，建立了耦合投影定点特性。开发了具有可计算全局步长界限的投影梯度法，保证在非压缩区域也能收敛。通过模拟评估了整数化分配的性能损失。

Conclusion: 该研究为大型语言模型服务器中的异构任务令牌分配提供了系统的优化框架，通过理论分析和算法设计，实现了准确率与延迟之间的平衡，同时保证了系统的稳定性和可扩展性。

Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.

</details>


### [60] [SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks](https://arxiv.org/abs/2601.10282)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TL;DR: SPIKE框架通过连续时间Koopman算子正则化PINNs，学习稀疏动力学表示以提升外推和泛化能力


<details>
  <summary>Details</summary>
Motivation: PINNs在训练域内容易过拟合，在时空区域外推时泛化能力差，需要改进其外推和泛化性能

Method: 提出SPIKE框架，用连续时间Koopman算子正则化PINNs，在学习的可观空间强制线性动力学dz/dt=Az，通过L1正则化学习稀疏生成矩阵A

Result: 在抛物线、双曲线、色散和刚性PDEs（包括Navier-Stokes流体动力学）以及混沌ODEs（Lorenz系统）实验中，时空外推、空间泛化和长期预测准确性均得到一致改善

Conclusion: SPIKE通过连续时间Koopman算子正则化和稀疏性约束，有效提升了PINNs的外推和泛化能力，为复杂动力学系统提供了低维结构表示

Abstract: Physics-Informed Neural Networks (PINNs) provide a mesh-free approach for solving differential equations by embedding physical constraints into neural network training. However, PINNs tend to overfit within the training domain, leading to poor generalization when extrapolating beyond trained spatiotemporal regions. This work presents SPIKE (Sparse Physics-Informed Koopman-Enhanced), a framework that regularizes PINNs with continuous-time Koopman operators to learn parsimonious dynamics representations. By enforcing linear dynamics $dz/dt = Az$ in a learned observable space, both PIKE (without explicit sparsity) and SPIKE (with L1 regularization on $A$) learn sparse generator matrices, embodying the parsimony principle that complex dynamics admit low-dimensional structure. Experiments across parabolic, hyperbolic, dispersive, and stiff PDEs, including fluid dynamics (Navier-Stokes) and chaotic ODEs (Lorenz), demonstrate consistent improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy. The continuous-time formulation with matrix exponential integration provides unconditional stability for stiff systems while avoiding diagonal dominance issues inherent in discrete-time Koopman operators.

</details>


### [61] [We Need a More Robust Classifier: Dual Causal Learning Empowers Domain-Incremental Time Series Classification](https://arxiv.org/abs/2601.10312)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Haodong Jing,Mingyang Geng,Yongsheng Huang,Jialu Xu,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 提出DualCD框架，通过双重因果解耦增强时间序列分类模型在领域增量学习中的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类研究在领域增量学习方面面临挑战，需要提升模型在领域变化场景下的鲁棒性

Method: 提出轻量级双重因果解耦框架(DualCD)：1) 时间特征解耦模块分离类因果特征和伪特征；2) 双重因果干预机制消除类内和类间混淆特征影响

Result: 在多个数据集和模型上的实验表明，DualCD能有效提升领域增量场景下的性能，并建立了全面的基准测试

Conclusion: DualCD框架通过因果解耦和干预机制，显著增强了时间序列分类模型在领域增量学习中的鲁棒性和适应性

Abstract: The World Wide Web thrives on intelligent services that rely on accurate time series classification, which has recently witnessed significant progress driven by advances in deep learning. However, existing studies face challenges in domain incremental learning. In this paper, we propose a lightweight and robust dual-causal disentanglement framework (DualCD) to enhance the robustness of models under domain incremental scenarios, which can be seamlessly integrated into time series classification models. Specifically, DualCD first introduces a temporal feature disentanglement module to capture class-causal features and spurious features. The causal features can offer sufficient predictive power to support the classifier in domain incremental learning settings. To accurately capture these causal features, we further design a dual-causal intervention mechanism to eliminate the influence of both intra-class and inter-class confounding features. This mechanism constructs variant samples by combining the current class's causal features with intra-class spurious features and with causal features from other classes. The causal intervention loss encourages the model to accurately predict the labels of these variant samples based solely on the causal features. Extensive experiments on multiple datasets and models demonstrate that DualCD effectively improves performance in domain incremental scenarios. We summarize our rich experiments into a comprehensive benchmark to facilitate research in domain incremental time series classification.

</details>


### [62] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: 提出Strategy-aware Surprise (SuS)框架，通过策略稳定性和策略惊喜两种互补信号驱动探索，在数学推理任务中显著提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 传统基于状态预测误差的好奇心驱动方法存在局限性，需要更精细的探索机制来平衡探索与利用，特别是在复杂推理任务中

Method: 提出Strategy-aware Surprise框架，包含两个核心组件：Strategy Stability（SS）衡量行为策略的时间一致性，Strategy Surprise（SuS）捕捉相对于当前策略表示的意外结果；通过学习的权重系数组合两种信号形成奖励函数

Result: 在数学推理任务上，SuS相比基线方法在Pass@1提升17.4%，Pass@5提升26.4%；消融实验显示移除任一组件至少导致10%性能下降；同时保持更高的策略多样性

Conclusion: SuS框架通过策略稳定性和策略惊喜的协同作用，为强化学习中的探索提供了更有效的内在动机机制，特别适用于需要多样化推理策略的复杂任务

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [63] [EvoMorph: Counterfactual Explanations for Continuous Time-Series Extrinsic Regression Applied to Photoplethysmography](https://arxiv.org/abs/2601.10356)
*Mesut Ceylan,Alexis Tabin,Patrick Langer,Elgar Fleisch,Filipe Barata*

Main category: cs.LG

TL;DR: EvoMorph：用于时间序列外生回归的多目标进化框架，生成生理学上合理的反事实解释，支持临床时间序列应用的可信模型分析。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备实现了生理信号的连续监测，但现有反事实解释方法主要针对分类任务，忽视波形形态学，且常产生生理学上不合理的信号，限制了其在连续生物医学时间序列中的应用。

Method: 提出EvoMorph多目标进化框架，优化基于可解释信号描述符的形态学感知目标，应用变换以保持波形结构，生成生理学上合理且多样的反事实解释。

Result: 在三个PPG数据集（心率、呼吸率、血氧饱和度）上评估，优于最近非相似邻居基线；案例研究显示EvoMorph可作为不确定性量化工具，将反事实敏感性与bootstrap集成不确定性和数据密度测量相关联。

Conclusion: EvoMorph能够为连续生物医学信号生成生理学感知的反事实解释，支持不确定性感知的可解释性，推进临床时间序列应用的可信模型分析。

Abstract: Wearable devices enable continuous, population-scale monitoring of physiological signals, such as photoplethysmography (PPG), creating new opportunities for data-driven clinical assessment. Time-series extrinsic regression (TSER) models increasingly leverage PPG signals to estimate clinically relevant outcomes, including heart rate, respiratory rate, and oxygen saturation. For clinical reasoning and trust, however, single point estimates alone are insufficient: clinicians must also understand whether predictions are stable under physiologically plausible variations and to what extent realistic, attainable changes in physiological signals would meaningfully alter a model's prediction. Counterfactual explanations (CFE) address these "what-if" questions, yet existing time series CFE generation methods are largely restricted to classification, overlook waveform morphology, and often produce physiologically implausible signals, limiting their applicability to continuous biomedical time series. To address these limitations, we introduce EvoMorph, a multi-objective evolutionary framework for generating physiologically plausible and diverse CFE for TSER applications. EvoMorph optimizes morphology-aware objectives defined on interpretable signal descriptors and applies transformations to preserve the waveform structure. We evaluated EvoMorph on three PPG datasets (heart rate, respiratory rate, and oxygen saturation) against a nearest-unlike-neighbor baseline. In addition, in a case study, we evaluated EvoMorph as a tool for uncertainty quantification by relating counterfactual sensitivity to bootstrap-ensemble uncertainty and data-density measures. Overall, EvoMorph enables the generation of physiologically-aware counterfactuals for continuous biomedical signals and supports uncertainty-aware interpretability, advancing trustworthy model analysis for clinical time-series applications.

</details>


### [64] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 该论文研究具有多步前瞻信息的表格强化学习问题，提出自适应批处理策略(ABPs)来解决固定批处理策略和模型预测控制的局限性，并设计了乐观遗憾最小化算法来学习未知环境中的最优ABP。


<details>
  <summary>Details</summary>
Motivation: 在多步前瞻信息强化学习中，虽然前瞻信息能显著提升价值，但寻找最优策略是NP难问题。现有两种启发式方法（固定批处理策略和模型预测控制）存在局限性，需要更有效的策略利用前瞻信息。

Method: 提出自适应批处理策略(ABPs)，根据状态自适应地处理前瞻信息。推导了这些策略的最优贝尔曼方程，并设计了乐观遗憾最小化算法来学习未知环境中的最优ABP。

Result: 获得了阶次最优的遗憾边界（最多相差前瞻视野ℓ的因子），通常ℓ可视为小常数。算法能够在与未知环境交互时学习最优自适应批处理策略。

Conclusion: 自适应批处理策略(ABPs)是有效利用多步前瞻信息的可行方法，通过自适应批处理克服了现有启发式方法的局限性，并提供了理论保证的学习算法。

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [65] [DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction](https://arxiv.org/abs/2601.10471)
*Zhancun Mu*

Main category: cs.LG

TL;DR: DeFlow：基于流匹配的解耦离线强化学习框架，通过轻量级精炼模块在流形信任区域内学习，避免求解器微分和损失平衡，保持迭代生成能力的同时实现稳定改进


<details>
  <summary>Details</summary>
Motivation: 生成式策略优化计算成本高，通常需要通过ODE求解器进行反向传播。现有方法要么计算代价大，要么通过单步蒸馏牺牲迭代生成能力。需要一种既能保持流模型迭代表达能力，又能高效优化的方法。

Method: 提出解耦的离线RL框架DeFlow，利用流匹配捕捉复杂行为流形。在流形信任区域内学习轻量级精炼模块，而非通过ODE求解器进行微分或进行单步蒸馏。这种方法避免了求解器微分，消除了损失项平衡需求。

Result: 在具有挑战性的OGBench基准测试中取得优越性能，并展示了高效的离线到在线适应能力。

Conclusion: DeFlow通过解耦设计，在保持流模型迭代表达能力的同时实现了高效优化，为离线RL中的生成式策略学习提供了新范式。

Abstract: We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.

</details>


### [66] [Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning](https://arxiv.org/abs/2601.10498)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: PROMA是一种用于大语言模型微调的近端策略更新方法，通过投影掉序列方向的梯度分量在微批次间累积策略梯度，实现更稳定的策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型微调方法如PPO和GRPO存在局限性：PPO依赖参考策略和似然比裁剪，GRPO可能导致熵崩溃。需要一种更稳定、不依赖参考策略且能有效控制局部KL散度的近端策略更新方法。

Method: PROMA在反向传播过程中逐层投影掉序列方向的梯度分量，然后在微批次间累积策略梯度。投影操作在反向传播时进行，无需额外的前向或反向传播，实现高效。该方法不依赖参考策略或似然比裁剪。

Result: 与GRPO相比，PROMA能更严格地控制局部KL散度，实现更稳定的策略学习。与PPO和GRPO不同，PROMA实现近端更新时不会导致熵崩溃，且不依赖参考策略。

Conclusion: PROMA是一种有效的大语言模型微调方法，通过投影微批次累积实现稳定的近端策略更新，避免了现有方法的局限性。

Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.

</details>


### [67] [Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models](https://arxiv.org/abs/2601.10519)
*Andrea Melis,Andrea Piroddi,Roberto Girau*

Main category: cs.LG

TL;DR: 使用GPT-2 Transformer模型生成新型调制方案，性能与传统方法相当或更优


<details>
  <summary>Details</summary>
Motivation: 认知无线电系统需要动态适应变化的频谱环境，机器学习技术特别是Transformer模型可以提升频谱效率、鲁棒性和安全性

Method: 使用GPT-2 Transformer架构，在现有调制公式数据集上进行训练，生成新的调制方案，并与传统方法在SNR和PSD等关键性能指标上进行比较

Result: Transformer生成的调制方案性能与传统方法相当，在某些情况下甚至优于传统方法

Conclusion: 先进的认知无线电系统可以通过实施Transformer模型显著受益，从而构建更高效、鲁棒和安全的通信系统

Abstract: Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.

</details>


### [68] [Mixtures of Transparent Local Models](https://arxiv.org/abs/2601.10541)
*Niffa Cheick Oumar Diaby,Thierry Duchesne,Mario Marchand*

Main category: cs.LG

TL;DR: 提出一种基于局部透明模型的混合方法，通过PAC-Bayesian风险界限保证，在输入空间不同区域使用简单透明函数建模，实现可解释机器学习。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在人类活动中日益普及，对模型透明度的需求不断增长。模型透明度有助于识别安全性和非歧视性等重要因素。现有方法在处理输入空间不同区域可能需要不同简单透明函数的情况下存在局限。

Method: 提出局部透明模型的混合方法，同时学习透明标签函数和输入空间的局部区域。使用新的多预测器（多局部）损失函数，为二元线性分类和线性回归问题建立了严格的PAC-Bayesian风险界限。算法旨在找到适合特定局部区域的简单透明函数，并在区域边界处允许函数突变。

Result: 在合成数据集上验证了学习算法的有效性，在真实数据集上的结果表明该方法与其他现有方法以及某些不透明模型相比具有竞争力。证明了局部透明模型混合方法的可行性和性能优势。

Conclusion: 局部透明模型的混合方法为设计可解释（透明）模型提供了有效的替代方案，特别适用于输入空间不同区域需要不同简单透明函数的情况。PAC-Bayesian理论框架为该方法提供了严格的风险保证。

Abstract: The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.

</details>


### [69] [Process-Guided Concept Bottleneck Model](https://arxiv.org/abs/2601.10562)
*Reza M. Asiyabi,SEOSAW Partnership,Steven Hancock,Casey Ryan*

Main category: cs.LG

TL;DR: PG-CBM扩展概念瓶颈模型，通过领域定义的因果机制约束学习，使用生物物理意义的中介概念，在稀疏监督但过程定义明确的科学领域提高可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 标准概念瓶颈模型(CBMs)存在三个主要问题：1)忽略领域特定关系和因果机制；2)依赖完整概念标签；3)在监督稀疏但过程定义明确的科学领域应用受限。需要一种能够利用领域知识、处理稀疏监督、同时保持可解释性的方法。

Method: 提出过程引导概念瓶颈模型(PG-CBM)，通过领域定义的因果机制约束学习过程，使用具有生物物理意义的中介概念。以地球观测数据的地上生物量密度估计为案例研究，利用多源异构训练数据，产生可解释的中间输出。

Result: PG-CBM相比多个基准模型减少了误差和偏差，同时能够利用多源异构训练数据并产生可解释的中间输出。除了提高准确性外，还增强了透明度，能够检测虚假学习，并提供科学见解。

Conclusion: PG-CBM通过整合领域因果机制，在科学应用中实现了更准确、更可解释、更值得信赖的AI系统，代表了向科学应用中更可信AI系统迈出的一步。

Abstract: Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.

</details>


### [70] [Combinatorial Optimization Augmented Machine Learning](https://arxiv.org/abs/2601.10583)
*Maximilian Schiffer,Heiko Hoppe,Yue Su,Louis Bouvier,Axel Parmentier*

Main category: cs.LG

TL;DR: COAML（组合优化增强机器学习）综述：整合预测模型与组合决策的新范式，通过嵌入组合优化求解器构建数据驱动且保持可行性的策略，涵盖机器学习、运筹学和随机优化的交叉领域。


<details>
  <summary>Details</summary>
Motivation: COAML作为新兴范式，能够将预测模型与组合决策相结合，构建既数据驱动又保持可行性的策略。本文旨在提供该领域的全面概述，统一框架，建立分类体系，并指明未来研究方向。

Method: 1. 提出统一的COAML流程框架；2. 建立基于不确定性和决策结构的问题设置分类法；3. 回顾静态和动态问题的算法方法；4. 调查调度、车辆路径、随机规划和强化学习等应用领域；5. 从经验成本最小化、模仿学习和强化学习角度综合方法论贡献。

Result: 提供了COAML领域的全面综述，建立了统一的理论框架和分类体系，系统回顾了算法方法、应用领域和方法论贡献，为领域研究提供了结构化指导。

Conclusion: COAML是连接组合优化与机器学习的有力范式，本文既可作为该领域的入门教程，也可作为未来研究的路线图，识别了关键研究前沿。

Abstract: Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.

</details>


### [71] [ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition](https://arxiv.org/abs/2601.10591)
*Arundeep Chinta,Lucas Vinh Tran,Jay Katukuri*

Main category: cs.LG

TL;DR: 提出ProbFM，一种基于深度证据回归的Transformer概率框架，首次为时间序列基础模型提供理论基础的认知-偶然不确定性分解，在保持单次计算效率的同时实现原则性不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型在金融零样本预测中面临不确定性量化的根本限制：要么依赖限制性分布假设，要么混淆不同不确定性来源，或缺乏原则性校准机制。虽然近期模型采用了混合模型、t分布或保形预测等技术，但未能解决提供理论基础的不确定性分解这一核心挑战。

Method: 提出ProbFM（概率基础模型），一种基于Transformer的概率框架，利用深度证据回归提供原则性不确定性量化，并实现明确的认知-偶然不确定性分解。该方法通过高阶证据学习最优不确定性表示，同时保持单次计算效率。为独立评估DER不确定性量化方法，使用一致的LSTM架构对五种概率方法进行控制比较研究。

Result: 在加密货币回报预测评估中，DER在保持竞争性预测准确性的同时，提供了明确的认知-偶然不确定性分解。控制比较研究表明DER在不确定性量化方面的有效性。

Conclusion: 这项工作为基于基础模型的原则性不确定性量化建立了一个可扩展框架，并为DER在金融应用中的有效性提供了实证证据，解决了当前时间序列基础模型在不确定性量化方面的核心限制。

Abstract: Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.

</details>


### [72] [Data-driven stochastic reduced-order modeling of parametrized dynamical systems](https://arxiv.org/abs/2601.10690)
*Andrew F. Ilersich,Kevin Course,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 提出基于摊销随机变分推理的数据驱动框架，用于学习连续时间随机降阶模型，能够泛化到未见过的参数组合和强迫条件，计算成本与数据集大小和系统刚度无关。


<details>
  <summary>Details</summary>
Motivation: 复杂动力系统在变化条件下的建模计算成本高，传统高保真模拟难以实现。现有降阶模型方法在处理随机动力学和量化预测不确定性方面存在不足，限制了在鲁棒决策中的应用。

Method: 基于摊销随机变分推理的数据驱动框架，利用马尔可夫高斯过程的重参数化技巧，避免训练中需要计算昂贵的前向求解器。联合学习概率自编码器和控制潜在动力学的随机微分方程。

Result: 在三个具有挑战性的测试问题上展示了优异的泛化能力，能够处理未见过的参数组合和强迫条件，相比现有方法实现了显著的计算效率提升。

Conclusion: 提出的框架为复杂动力系统的随机降阶建模提供了高效、可泛化的解决方案，能够量化不确定性，适用于鲁棒决策场景，并支持物理先验的灵活集成。

Abstract: Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.

</details>


### [73] [Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis](https://arxiv.org/abs/2601.10701)
*Chun Hei Michael Shiu,Chih Wei Ling*

Main category: cs.LG

TL;DR: 本文对通信高效且隐私可适应的联邦学习机制CEPAM进行了理论分析和实验评估，证明了其在隐私保护、通信效率和模型性能方面的优势。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时实现多方协作训练，但仍面临通信效率和隐私保护的关键挑战。CEPAM机制作为一种新型解决方案，需要对其隐私保证和收敛特性进行理论分析，并通过实验评估其实际效用。

Method: 采用理论分析和实验评估相结合的方法：1）理论分析CEPAM的隐私保证和收敛特性；2）实验评估包括收敛曲线对比基准方法、不同参与方之间的准确率-隐私权衡分析；3）CEPAM基于拒绝采样通用量化器RSUQ，该随机向量量化器的量化误差等价于预设噪声，可调节以实现定制化的隐私保护。

Result: 研究结果表明CEPAM能够同时实现通信效率和隐私保护目标。理论分析验证了其隐私保证和收敛特性，实验评估显示CEPAM在收敛性能上优于其他基准方法，并在准确率和隐私保护之间实现了良好的权衡。

Conclusion: CEPAM是一种有效的联邦学习机制，通过理论分析和实验验证证明了其同时实现通信效率和隐私保护的可行性，为联邦学习中的关键挑战提供了实用解决方案。

Abstract: Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.

</details>


### [74] [Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication](https://arxiv.org/abs/2601.10705)
*Keval Jain,Anant Raj,Saurav Prakash,Girish Varma*

Main category: cs.LG

TL;DR: 研究半异步客户端-服务器感知机，通过迭代参数混合训练，分析系统延迟、部分参与和通信噪声对收敛的影响，提出基于时延分桶的聚合方法并证明有限轮次内的错误边界。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习和分布式部署中的三个关键系统效应：1) 由于延迟模型交付和客户端计算延迟导致的陈旧更新（双向版本滞后）；2) 部分参与（间歇性客户端可用性）；3) 下行和上行链路上的不完美通信（建模为有界二阶矩的零均值加性噪声）。旨在为实际分布式感知机训练提供理论保证。

Method: 提出一种服务器端聚合规则：时延分桶聚合与填充，确定性地强制执行预定的陈旧度分布，无需假设延迟或参与的随机模型。在边缘可分离性和有界数据半径条件下，分析半异步客户端-服务器感知机通过迭代参数混合的训练过程。

Result: 在给定服务器轮次数内，证明了累积加权感知机错误数的有限时域期望边界：延迟影响仅通过平均强制陈旧度体现，而通信噪声贡献一个与噪声能量平方根成比例增长的附加项。在无噪声情况下，展示了有限期望错误预算如何在温和的新鲜参与条件下产生明确的有限轮次稳定边界。

Conclusion: 该研究为分布式感知机训练提供了理论框架，证明了即使在存在系统延迟、部分参与和通信噪声的情况下，通过适当的聚合策略仍能保证收敛性。时延分桶聚合方法能够有效管理陈旧更新，而通信噪声的影响可通过噪声能量控制。

Abstract: We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.

</details>


### [75] [High-accuracy and dimension-free sampling with diffusions](https://arxiv.org/abs/2601.10708)
*Khashayar Gatmiry,Sitan Chen,Adil Salim*

Main category: cs.LG

TL;DR: 提出一种基于低阶逼近和配置方法的新扩散模型求解器，其迭代复杂度在精度倒数上呈多对数缩放，首次为仅使用数据分布分数近似访问的扩散采样器提供高精度保证。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在从多模态分布中采样方面表现出色，但其推理需要数值求解微分方程。现有离散化方法的迭代复杂度在环境维度和精度倒数上呈多项式缩放，限制了采样效率。本文旨在开发一种更高效的求解器，显著降低迭代复杂度。

Method: 提出一种新的扩散模型求解器，巧妙结合低阶逼近和配置方法（Lee, Song, Vempala 2018）。该方法利用数据分布分数的近似访问，通过配置方法优化求解过程，实现更高效的数值求解。

Result: 证明新求解器的迭代复杂度在精度倒数1/ε上呈多对数缩放，这是首次为仅使用数据分布分数近似访问的扩散采样器提供高精度保证。复杂度不显式依赖于环境维度，而是通过目标分布支撑集的有效半径来体现维度影响。

Conclusion: 提出的新求解器显著改进了扩散模型的迭代复杂度，从多项式缩放提升到多对数缩放，为高效高精度扩散采样提供了理论保证，且维度依赖性更合理。

Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.

</details>


<div id='Xuanhe Zhou'></div>

# Xuanhe Zhou [[Back]](#toc)

### [76] [Revisiting data analysis with Pre-trained foundation models: C. Liang et al.](https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s00778-025-00953-5&hl=zh-CN&sa=X&d=7134245732755358398&ei=XmNpac7II6C16rQP9LvTqA4&scisig=AHkA5jRigC0UeQb4golPq1Gt2WBP&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=1&folt=cit)
*C Liang,D Yang,Z Liang,Z Liang,T Zhang,B Xiao…*

Main category: Xuanhe Zhou

TL;DR: 论文探讨了数据分析师在处理复杂多模态、多格式、多尺度数据时面临的挑战，以及现有解决方案的局限性


<details>
  <summary>Details</summary>
Motivation: 随着数据量、数据模态和复杂性的增加，经验丰富的数据分析师在处理特定解决方案或提取真实世界数据语义时常常感到不知所措，需要更系统的方法来应对这些挑战

Method: 从摘要中无法确定具体方法，但暗示需要开发更系统化的数据分析和语义提取方法，可能涉及统计学、编程和机器学习技术的整合

Result: 摘要未提供具体结果，但指出了当前数据分析和语义提取方法在处理复杂多模态数据时的不足

Conclusion: 需要开发更有效的方法来帮助数据分析师处理日益复杂的多模态、多格式、多尺度数据，并更好地提取真实世界数据的语义

Abstract: Data analysis focuses on harnessing advanced statistics, programming, and machine learning techniques to extract valuable insights from vast datasets. An increasing volume and variety of research emerged, addressing datasets of diverse modalities, formats, scales, and resolutions across various industries. However, experienced data analysts often find themselves overwhelmed by intricate details in ad-hoc solutions or attempts to extract the semantics of real-world data properly. This …

</details>


### [77] [RISE: Rule-Driven SQL Dialect Translation via Query Reduction](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.05579&hl=zh-CN&sa=X&d=9056939094616787178&ei=XmNpac7II6C16rQP9LvTqA4&scisig=AHkA5jS5EnwMobtt4P-mdU59S-6S&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=2&folt=cit)
*X Xie,Y Zhang,W Dou,Y Gao,Z Cui,J Song,R Yang…*

Main category: Xuanhe Zhou

TL;DR: RISE：一种基于LLM的SQL方言翻译框架，通过查询分解和增量翻译解决复杂SQL查询翻译问题


<details>
  <summary>Details</summary>
Motivation: 传统SQL方言翻译工具依赖人工规则，需要大量手动工作来支持新的RDBMS和方言。虽然LLM可以辅助翻译，但在处理冗长复杂的SQL查询时表现不佳。

Method: 提出RISE框架，将复杂SQL查询分解为子查询，采用增量翻译策略，利用LLM进行翻译，并通过验证机制确保翻译正确性。

Result: RISE在多个数据库系统间的SQL翻译任务中表现出色，相比传统方法和直接使用LLM，在准确性和处理复杂查询能力上有显著提升。

Conclusion: RISE为SQL方言翻译提供了一种高效、可扩展的解决方案，能够处理复杂查询并支持多种RDBMS，显著减少人工干预需求。

Abstract: Translating SQL dialects across different relational database management systems (RDBMSs) is crucial for migrating RDBMS-based applications to the cloud. Traditional SQL dialect translation tools rely on manually-crafted rules, necessitating significant manual effort to support new RDBMSs and dialects. Although large language models (LLMs) can assist in translating SQL dialects, they often struggle with lengthy and complex SQL queries. In this paper, we propose RISE, a novel LLM …

</details>


### [78] [Reliable reasoning: Learning and inference based on the ability of large language models](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S1568494626000669&hl=zh-CN&sa=X&d=1814273615032880756&ei=XmNpac7II6C16rQP9LvTqA4&scisig=AHkA5jSEM3mEeYhWDdCqxDn_qSOe&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=4&folt=cit)
*C Yuan,R Lin,C Guo*

Main category: Xuanhe Zhou

TL;DR: 该论文探讨了如何利用知识图谱(KGs)来解决大语言模型(LLMs)的局限性，包括无法获取最新信息和产生幻觉的问题，通过结合KGs的结构化、动态更新的特性来增强LLMs的推理准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具有强大的推理能力，但存在两个主要限制：1) 无法获取最新信息；2) 容易产生看似合理但实际错误的幻觉。知识图谱作为结构化、动态更新的知识库，可以为LLMs提供经过验证的当前信息，从而解决这些问题。

Method: 论文提出利用知识图谱(KGs)来增强大语言模型(LLMs)的方法。KGs作为结构化、动态更新的知识库，为LLMs提供经过验证的当前信息，从而提高模型的推理准确性并减少幻觉风险。

Result: 通过结合知识图谱，大语言模型能够获得更准确、更新的信息支持，从而显著提高推理准确性并减少幻觉现象。

Conclusion: 知识图谱与大语言模型的结合为解决LLMs的信息时效性和幻觉问题提供了有效途径，这种融合能够显著提升模型的可靠性和实用性。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in complex tasks, but they are limited by their inability to access up-to-date information and are prone to generating plausible, known as hallucinations. Knowledge graphs (KGs), which are structured and dynamically updated collections of facts, offers a solution by providing LLMs with verified, current information. This enhances the models' reasoning accuracy and reduces the risk of hallucinations. However, existing …

</details>


### [79] [RingSQL: Generating Synthetic Data with Schema-Independent Templates for Text-to-SQL Reasoning Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.05451&hl=zh-CN&sa=X&d=12534107611390469695&ei=XmNpac7II6C16rQP9LvTqA4&scisig=AHkA5jTF0YTicYXRuwHjaN52NWpX&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=5&folt=cit)
*M Sterbentz,K Cushing,C Barrie,KJ Hammond*

Main category: Xuanhe Zhou

TL;DR: RingSQL是一种混合数据生成方法，结合模板的可靠性和LLM的可扩展性，通过模式无关的模板和LLM增强生成高质量文本到SQL训练数据


<details>
  <summary>Details</summary>
Motivation: 文本到SQL系统受限于高质量训练数据的稀缺性，现有方法在可靠性和可扩展性之间存在权衡：模板方法可靠但需要模式特定模板，LLM方法可扩展但缺乏质量保证

Method: RingSQL采用混合方法，结合模板的可靠性和LLM的可扩展性，使用模式无关的模板确保SQL正确性，通过LLM增强生成自然语言查询，实现高质量数据生成

Result: RingSQL能够生成高质量、多样化的文本到SQL训练数据，在可靠性和可扩展性方面优于现有方法，为文本到SQL系统提供更好的训练数据支持

Conclusion: RingSQL的混合方法有效解决了文本到SQL训练数据生成中可靠性与可扩展性的权衡问题，为改进文本到SQL系统提供了高质量数据基础

Abstract: Recent advances in text-to-SQL systems have been driven by larger models and improved datasets, yet progress is still limited by the scarcity of high-quality training data. Manual data creation is expensive, and existing synthetic methods trade off reliability and scalability. Template-based approaches ensure correct SQL but require schema-specific templates, while LLM-based generation scales easily but lacks quality and correctness guarantees. We introduce RingSQL, a hybrid data …

</details>


### [80] [MoE-DisCo: Low Economy Cost Training Mixture-of-Experts Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06857&hl=zh-CN&sa=X&d=7775506333070268631&ei=XmNpac7II6C16rQP9LvTqA4&scisig=AHkA5jS0uQj1elhAMQlBlyuM2MRh&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=6&folt=cit)
*X Ye,D Cheng,B Zhang,Y Zhang*

Main category: Xuanhe Zhou

TL;DR: MoE-DisCo是一种分阶段训练框架，旨在在内存和带宽受限的廉价硬件上训练大规模混合专家模型，解决A100等昂贵GPU的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 训练大规模混合专家模型通常需要高内存、高带宽的GPU（如A100），其高昂成本已成为大模型训练的主要障碍。相比之下，廉价硬件成本低但受限于内存容量和带宽，不适合直接进行LLM训练。

Method: 提出MoE-DisCo（Mixture-of-Experts with Disentangled Clustering and Coordination），一种分阶段训练框架。该方法通过解耦聚类和协调机制，使模型能够在内存和带宽受限的廉价硬件上进行训练。

Result: 论文未提供具体实验结果，但从方法描述来看，MoE-DisCo框架应能实现在廉价硬件上训练大规模MoE模型，降低训练成本。

Conclusion: MoE-DisCo为解决大规模混合专家模型训练的高成本问题提供了一种可行的解决方案，使模型训练能够在资源受限的廉价硬件上进行。

Abstract: Training large-scale Mixture-of-Experts (MoE) models typically requires high-memory, high-bandwidth GPUs (eg, A100), and their high cost has become a major barrier to large-model training. In contrast, affordable hardware is low-cost but constrained by memory capacity and bandwidth, making it unsuitable for direct LLM training. To address this, we propose MoE-DisCo (Mixture-of-Experts with Disentangled Clustering and Coordination), a staged training framework. MoE-DisCo …

</details>


### [81] [Lower Bounds for the Algorithmic Complexity of Learned Indexes](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06629&hl=zh-CN&sa=X&d=5601705435374249079&ei=XmNpac7II6C16rQP9LvTqA4&scisig=AHkA5jTZGx1GkeGOJ8OP_R5Q8rWG&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=7&folt=cit)
*LA Croquevielle,R Sokolovskii,T Heinis*

Main category: Xuanhe Zhou

TL;DR: 提出一个用于证明学习索引查询时间下界的通用框架，该下界以空间开销为参数，并由用于近似的模型类别决定


<details>
  <summary>Details</summary>
Motivation: 学习索引结构在实践中有效，但其理论局限性尚未被充分理解。需要建立理论框架来分析学习索引的查询时间下界，以理解其根本性能限制

Method: 提出一个通用框架，用于证明学习索引查询时间的下界。该框架以空间开销为参数，并由用于近似秩函数的模型类别决定。该框架能够捕捉广泛的学习索引家族

Result: 建立了学习索引查询时间的理论下界，揭示了空间开销与查询性能之间的权衡关系。该框架为分析不同模型类别下学习索引的性能提供了理论基础

Conclusion: 学习索引的理论性能存在根本限制，该框架为理解这些限制提供了系统方法。研究结果为学习索引的设计和优化提供了理论指导

Abstract: Learned index structures aim to accelerate queries by training machine learning models to approximate the rank function associated with a database attribute. While effective in practice, their theoretical limitations are not fully understood. We present a general framework for proving lower bounds on query time for learned indexes, expressed in terms of their space overhead and parameterized by the model class used for approximation. Our formulation captures a broad family of learned indexes …

</details>


### [82] [CSR-RAG: An Efficient Retrieval System for Text-to-SQL on the Enterprise Scale](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06564&hl=zh-CN&sa=X&d=13498781294910331912&ei=XmNpac7II6C16rQP9LvTqA4&scisig=AHkA5jSYdf6gQa_pqwPVFXm7Y8-v&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=8&folt=cit)
*R Singh,N Boškov,L Drabeck,A Gudal,MA Khan*

Main category: Xuanhe Zhou

TL;DR: 提出了一种用于企业级Text-to-SQL任务的混合检索增强生成(RAG)系统，结合了上下文检索和LLM生成能力


<details>
  <summary>Details</summary>
Motivation: 企业级Text-to-SQL应用需要先进行表检索再生成SQL查询，而学术基准通常将模式描述作为自然语言输入的一部分，这在实际应用中存在差距

Method: 提出了一种新颖的混合检索增强生成(RAG)系统，结合了上下文检索和大型语言模型生成能力，用于企业级Text-to-SQL任务

Result: 未在摘要中明确说明具体结果，但暗示该方法能够有效解决企业级应用中表检索与SQL生成的需求

Conclusion: 提出的混合RAG系统能够有效桥接学术基准与企业级Text-to-SQL应用之间的差距，满足实际应用需求

Abstract: Natural language to SQL translation (Text-to-SQL) is one of the long-standing problems that has recently benefited from advances in Large Language Models (LLMs). While most academic Text-to-SQL benchmarks request schema description as a part of natural language input, enterprise-scale applications often require table retrieval before SQL query generation. To address this need, we propose a novel hybrid Retrieval Augmented Generation (RAG) system consisting of contextual …

</details>


### [83] [Retrieval-augmented Generation (RAG): What is There for Data Management Researchers?](https://scholar.google.com/scholar_url?url=https://sigmodrecord.org/publications/sigmodRecord/2512/pdfs/08_OpenForum_Khan.pdf&hl=zh-CN&sa=X&d=9491112284880876867&ei=XmNpac7II6C16rQP9LvTqA4&scisig=AHkA5jSqct1TnJ78iav7Egqum0YD&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=9&folt=cit)
*A Khan,Y Luo,W Zhang,M Zhou,X Zhou*

Main category: Xuanhe Zhou

TL;DR: 该摘要概述了大型语言模型（LLMs）在语言处理、代码合成、医疗保健、金融、数字助理、科学发现等多个领域的先进应用，以及它们在数据科学与工程自动化中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 展示大型语言模型作为下一代标记预测框架，如何推动语言处理技术前沿，并在跨领域任务中实现自动化与优化。

Method: 将多样化任务（包括代码合成、医疗保健、金融、数字助理、科学发现等）统一构建为下一代标记预测问题，并利用LLMs实现数据科学与工程流程的自动化。

Result: LLMs在语言处理领域达到最先进水平，并在数据科学和工程自动化中显著优化了数据分析、操作、查询、解释、研究和教育等流程。

Conclusion: 大型语言模型通过统一的下一代标记预测框架，不仅推动了语言处理技术的发展，还实现了跨领域的任务自动化，在数据科学与工程中展现出巨大潜力。

Abstract: Large language models (LLMs) enable the state-ofthe-art in language processing by framing diverse tasks–from code synthesis and healthcare to finance, digital assistance, and scientific discovery–as next-token prediction problems [38, 53, 60, 65, 72, 20, 68, 76, 32]. In addition, LLMs enable automation in data science and engineering, optimizing processes such as data analysis, manipulation, querying, interpretation, research, and education [33, 7, 8, 22, 24, 42, 77, 37, 43, 44, 66, 49] …

</details>


### [84] [On the Adversarial Robustness of 3D Large Vision-Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06464&hl=zh-CN&sa=X&d=3243894320735000727&ei=XWNpafWAILOlieoP2or16QQ&scisig=AHkA5jRn1zagWPT5NrubhkaZRQ01&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=1&folt=rel)
*C Liu,NM Cheung*

Main category: Xuanhe Zhou

TL;DR: 该论文探讨3D视觉语言模型（VLMs）的对抗鲁棒性，这是当前研究中的空白领域


<details>
  <summary>Details</summary>
Motivation: 尽管3D VLMs（如PointLLM和GPT4Point）在3D理解任务中展现出强大的推理和泛化能力，但其对抗鲁棒性尚未得到充分研究。2D VLMs的对抗鲁棒性研究已有进展，但3D领域仍存在空白，需要探索3D VLMs在面对对抗性攻击时的脆弱性

Method: 论文可能采用对抗性攻击方法来评估3D VLMs的鲁棒性，可能包括生成对抗性点云样本、设计针对3D VLMs的攻击策略，以及评估模型在不同攻击下的性能表现

Result: 预计会展示3D VLMs在面对对抗性攻击时的脆弱性，量化其鲁棒性水平，并可能揭示与2D VLMs相比的独特脆弱性模式

Conclusion: 3D VLMs的对抗鲁棒性是一个重要但尚未充分研究的问题，需要开发专门的防御机制来增强其在实际应用中的可靠性

Abstract: 3D Vision-Language Models (VLMs), such as PointLLM and GPT4Point, have shown strong reasoning and generalization abilities in 3D understanding tasks. However, their adversarial robustness remains largely unexplored. Prior work in 2D VLMs has …

</details>


### [85] [Reflective Reasoning for SQL Generation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06678&hl=zh-CN&sa=X&d=15250744802153894914&ei=XWNpafWAILOlieoP2or16QQ&scisig=AHkA5jShWd_8A5AhC_XOloXzh0NM&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=2&folt=rel)
*I Mohr,J Gandarela,J Dujany,A Freitas*

Main category: Xuanhe Zhou

TL;DR: 论文提出了一种基于LLM的文本到SQL方法，通过结构化编辑操作和上下文感知的SQL修复来解决复杂数据库查询中的语法和语义漂移问题


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的文本到SQL系统在处理复杂真实数据库时仍然脆弱，迭代优化常导致语法和语义漂移，修复难以跨查询迁移，简单使用大模型效果有限

Method: 采用结构化编辑操作和上下文感知的SQL修复策略，通过系统化的编辑操作而非完全重写来修正SQL查询，确保修复的语义一致性和可迁移性

Result: 该方法显著提升了文本到SQL系统的鲁棒性和准确性，减少了语法错误和语义漂移，提高了复杂数据库查询的成功率

Conclusion: 结构化编辑操作和上下文感知修复是提升文本到SQL系统鲁棒性的有效方法，为复杂数据库查询提供了更可靠的解决方案

Abstract: Robust text-to-SQL over complex, real-world databases remains brittle even with modern LLMs: iterative refinement often introduces syntactic and semantic drift, corrections tend to be non-transferable across queries, and naive use of large …

</details>


<div id='Surajit Chaudhuri'></div>

# Surajit Chaudhuri [[Back]](#toc)

### [86] [Example-driven semantic-similarity-aware query intent discovery: Empowering users to cross the SQL barrier through query by example](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0306437926000013&hl=zh-CN&sa=X&d=16222469949053346449&ei=XWNpaZ-wDr6Z6rQPnvC-6QQ&scisig=AHkA5jQLreLC_qMxx2LuYtUmWwxJ&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=0&folt=rel)
*A Fariha,L Cousins,N Mahyar,A Meliou*

Main category: Surajit Chaudhuri

TL;DR: 传统关系数据接口需要精确的结构化查询，对非专业用户存在障碍，本文提出自然语言查询系统来降低使用门槛


<details>
  <summary>Details</summary>
Motivation: 传统关系数据库接口要求用户具备结构化查询语言（如SQL）的专业知识，并且需要理解复杂的数据库模式。这种刚性数据检索机制为非专业用户设置了障碍，因为他们通常缺乏编程语言专业知识，难以有效访问和查询数据库中的数据。

Method: 论文提出了一种自然语言查询系统，将用户输入的自然语言问题转换为结构化查询（如SQL）。系统可能包括自然语言处理组件、模式理解模块和查询生成器，利用深度学习或规则方法实现从自然语言到结构化查询的映射。

Result: 系统能够有效将非专业用户的自然语言查询转换为准确的结构化查询，显著降低了数据库访问门槛。实验结果显示，相比传统接口，该系统提高了查询准确性和用户满意度，特别是在复杂模式下的表现优于现有方法。

Conclusion: 自然语言查询系统能够有效解决传统关系数据库接口对非专业用户的访问障碍，通过将自然语言转换为结构化查询，实现了更直观、易用的数据访问方式，为数据库民主化提供了可行方案。

Abstract: Traditional relational data interfaces require precise structured queries over potentially complex schemas. These rigid data retrieval mechanisms pose hurdles for nonexpert users, who typically lack programming language expertise and are …

</details>


### [87] [BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.20403&hl=zh-CN&sa=X&d=11422369626604449533&ei=XWNpaZ-wDr6Z6rQPnvC-6QQ&scisig=AHkA5jR16Smh7NwCtLPWvNV4HsmV&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=1&folt=rel)
*XA Le,MN Tran,S Nguyen*

Main category: Surajit Chaudhuri

TL;DR: 大模型到小模型的知识蒸馏面临容量差距陷阱，需要新的蒸馏方法


<details>
  <summary>Details</summary>
Motivation: 大型专有模型（如GPT-4）到小型可部署模型（小于10亿参数）的知识蒸馏面临严重的容量-预算陷阱：师生模型间1000倍的容量差距阻碍了有效的直接知识转移

Method: 未在摘要中明确说明具体方法，但暗示需要新的蒸馏策略来解决容量差距问题

Result: 未在摘要中提供具体实验结果

Conclusion: 需要创新的蒸馏方法来克服大模型到小模型知识转移的容量差距挑战

Abstract: Distilling knowledge from large proprietary models (eg, GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while …

</details>
