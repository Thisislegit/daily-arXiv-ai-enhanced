<div id=toc></div>

# Table of Contents

- [Ziniu Wu](#Ziniu Wu) [Total: 1]
- [Alekh Jindal](#Alekh Jindal) [Total: 1]
- [Carsten Binnig](#Carsten Binnig) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [Zongheng Yang](#Zongheng Yang) [Total: 9]
- [Xuanhe Zhou](#Xuanhe Zhou) [Total: 14]
- [cs.LG](#cs.LG) [Total: 36]
- [Surajit Chaudhuri](#Surajit Chaudhuri) [Total: 4]
- [cs.DC](#cs.DC) [Total: 1]
- [Bin CUI](#Bin CUI) [Total: 2]
- [Ion Stoica](#Ion Stoica) [Total: 1]
- [Google Scholar](#Google Scholar) [Total: 9]
- [Matei Zaharia](#Matei Zaharia) [Total: 5]


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [1] [Is Quantum Computing Ready for Real-Time Database Optimization?](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.12123&hl=zh-CN&sa=X&d=9285225533576205973&ei=iEF1ac6hCruM6rQP2dOq-Ag&scisig=AHkA5jRDfK0IRE9TLF2LMuEpLbGM&oi=scholaralrt&hist=i6heNjgAAAAJ:8133205940682390297:AHkA5jQfTAaKkXi3kBQuJYouRHpx&html=&pos=0&folt=rel)
*H Liu,I Sabek*

Main category: Ziniu Wu

TL;DR: 数据库系统性能优化任务（如连接顺序和索引调优）在数据量增长和工作负载复杂化后变得指数级困难


<details>
  <summary>Details</summary>
Motivation: 随着数据量不断增长和工作负载日益复杂，数据库系统中的性能关键优化任务（如连接顺序优化和索引调优）变得指数级困难，需要更有效的解决方案

Method: 论文未提供具体方法细节，但暗示需要采用新的技术或算法来解决这些指数级复杂的问题

Result: 未提供具体实验结果，但指出传统方法在处理大规模复杂工作负载时面临指数级挑战

Conclusion: 数据库性能优化任务在数据增长和复杂度增加背景下需要创新的解决方案来应对指数级难度

Abstract: Database systems encompass several performance-critical optimization tasks, such as join ordering and index tuning. As data volumes grow and workloads become more complex, these problems have become exponentially harder to solve …

</details>


<div id='Alekh Jindal'></div>

# Alekh Jindal [[Back]](#toc)

### [2] [LLM-MVs: An LLM-Enhanced Materialized Views-Based Query Rewriting System](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11299299/&hl=zh-CN&sa=X&d=7416910454197709521&ei=iEF1abnjLPStieoP-8T7kA0&scisig=AHkA5jREEZLud0VOtpMBPWW5UIxY&oi=scholaralrt&hist=i6heNjgAAAAJ:9105223062713277753:AHkA5jRjsInJuQ8ojOPRhEA3nNaJ&html=&pos=0&folt=cit)
*C Sun,H Hu,H Zhang,E Wu,J Yan*

Main category: Alekh Jindal

TL;DR: 论文提出了一种基于大语言模型的查询重写方法，通过语义等效转换而非传统语法等效转换，充分利用物化视图优化查询性能


<details>
  <summary>Details</summary>
Motivation: 传统查询重写方法基于手动预定义的规则进行语法等效转换，无法充分利用物化视图的优化潜力。随着大语言模型的出现，需要探索更智能的查询重写方法

Method: 利用大语言模型进行语义等效的查询重写，而非传统的语法等效转换。通过LLM理解查询语义，生成能够利用物化视图的优化查询结构

Result: 基于大语言模型的方法相比传统规则方法能更有效地利用物化视图进行查询优化，提升查询效率

Conclusion: 大语言模型为查询重写提供了新的范式，通过语义理解能够超越传统规则方法的限制，更充分地利用物化视图优化查询性能

Abstract: Query rewriting, which aims to enhance query efficiency by modifying the structure of SQL queries without altering their results, stands as one of the pivotal research directions in the database domain. Traditional query rewriting approaches perform language-equivalent transformations based on manually predefined rewriting rules, thereby failing to fully harness the potential of Materialized Views (MVs) for query optimization. Furthermore, with the emergence of Large Language Models (LLMs) …

</details>


<div id='Carsten Binnig'></div>

# Carsten Binnig [[Back]](#toc)

### [3] [Enabling Tile-Based Direct Query on Adaptively Compressed Data With GPU Acceleration](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11271872/&hl=zh-CN&sa=X&d=8332487574324786742&ei=iUF1aaLIPJi8ieoPv_jKaQ&scisig=AHkA5jRJPUc4-c9v19gABTiIsTO8&oi=scholaralrt&hist=i6heNjgAAAAJ:15269883191641703195:AHkA5jTZCYhse3wIcZXHnO-iboY0&html=&pos=1&folt=cit)
*Y Zhang,F Zhang,Y Liu,H Zhang,J Zhai,W Zhou…*

Main category: Carsten Binnig

TL;DR: GPU数据库面临内存容量有限与高速查询需求的矛盾，传统压缩技术需要解压才能查询，导致查询性能下降


<details>
  <summary>Details</summary>
Motivation: 数据爆炸式增长给GPU数据库带来挑战，需要在有限内存容量和高速查询执行之间取得平衡。压缩技术虽能优化内存利用和减少数据移动，但传统方法需要解压才能查询，显著降低了查询性能。

Method: 论文未详细说明具体方法，但从摘要可推断可能涉及直接在压缩数据上执行查询的技术，避免完全解压的开销

Result: 未提供具体实验结果，但暗示传统压缩查询方法存在性能瓶颈

Conclusion: 需要新的压缩查询技术来克服传统解压查询的性能限制，实现GPU数据库的高效内存利用和快速查询

Abstract: The explosive growth of data poses significant challenges for GPU-based databases, which must balance limited memory capacity with the need for high-speed query execution. Compression has become an essential technique for optimizing memory utilization and reducing data movement. However, its benefits have been limited to the necessary data decompression. Querying compressed data conventionally requires decompression, which causes the query process to be significantly slower …

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [4] [NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases](https://arxiv.org/abs/2601.15758)
*Xieyang Wang,Mengyi Liu,Weijia Yi,Jianqiu Xu,Raymond Chi-Wing Wong*

Main category: cs.DB

TL;DR: NL4ST是一个交互式工具，允许用户用自然语言查询时空数据库，通过三层架构将自然语言转换为物理查询计划，降低非专家用户的查询门槛。


<details>
  <summary>Details</summary>
Motivation: 移动计算设备和定位技术的发展导致时空数据爆炸式增长，但时空查询（如范围查询、最近邻查询、连接查询）的制定需要领域专业知识和查询语言技能，这对非专家用户具有挑战性，因此需要支持自然语言查询的解决方案。

Method: NL4ST采用三层架构：1) 知识库和语料库用于知识准备；2) 自然语言理解用于实体链接；3) 物理计划生成。该工具将自然语言查询转换为可执行的时空数据库物理查询计划。

Result: NL4ST在四个真实和合成数据集上验证了其有效性，能够为时空数据库生成有效的物理查询计划。工具已在线提供，并配有演示视频。

Conclusion: NL4ST通过自然语言接口成功弥合了非专家用户与数据库查询计划之间的鸿沟，为时空数据库查询提供了用户友好的解决方案。

Abstract: The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.

</details>


### [5] [EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery](https://arxiv.org/abs/2601.16025)
*Yajuan Xu,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: EAIFD是一种用于增量函数依赖发现的高效算法，通过维护差异集的部分超图并将问题重构为超图上的最小命中集枚举，避免了完全重新执行。该算法采用多属性哈希表(MHT)和两步验证策略，显著提升了性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 函数依赖是关系数据库中的基本完整性约束，但在增量更新下的发现仍然具有挑战性。静态算法由于完全重新执行而效率低下，而增量算法则面临严重的性能和内存瓶颈。需要一种高效、可扩展的增量函数依赖发现解决方案。

Method: EAIFD算法维护差异集的部分超图，将增量函数依赖发现问题重构为超图上的最小命中集枚举。引入两个关键创新：1) 多属性哈希表(MHT)，用于高效键值映射，其内存消耗被证明与数据集大小无关；2) 两步验证策略，利用MHT有效减少验证空间，然后选择性加载数据块进行批量验证，避免重复I/O操作。

Result: 在真实世界数据集上的实验结果表明，EAIFD相比现有算法实现了显著优势：运行时间提升高达一个数量级，内存使用减少超过两个数量级，证明了其作为增量函数依赖发现的高效和可扩展解决方案的有效性。

Conclusion: EAIFD通过创新的超图重构、多属性哈希表和两步验证策略，成功解决了增量函数依赖发现中的性能和内存瓶颈问题，成为该领域的高效可扩展解决方案。

Abstract: Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.

</details>


<div id='Zongheng Yang'></div>

# Zongheng Yang [[Back]](#toc)

### [6] [Opportunistic Scheduling for Optimal Spot Instance Savings in the Cloud](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.12266&hl=zh-CN&sa=X&d=3494713592890630279&ei=hkF1aZaFCKyK6rQPm4H2iQs&scisig=AHkA5jRRRXAQ5csOq-wlLKzgQtks&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=0&folt=cit)
*N Bhuyan,R Bhatia,M Kodialam,TV Lakshman*

Main category: Zongheng Yang

TL;DR: 研究在满足平均延迟约束下，通过调度延迟敏感作业到竞价和按需云实例来最小化平均成本的问题，使用排队论、随机过程和优化工具进行首次分析处理


<details>
  <summary>Details</summary>
Motivation: 云环境中延迟敏感作业的调度需要平衡成本与性能，竞价实例成本低但可能中断，按需实例可靠但成本高，需要理论框架来优化混合实例调度策略

Method: 使用排队论、随机过程和优化工具，推导一般策略的成本表达式，证明低目标延迟下队列长度为1是最优的，建立混合实例调度模型

Result: 首次为混合云实例调度问题提供分析处理，推导出成本表达式，证明低延迟目标下的最优性条件，为实际调度策略提供理论指导

Conclusion: 该研究为延迟敏感作业在混合云实例环境中的调度建立了理论框架，证明了特定条件下的最优策略，为云成本优化提供了分析基础

Abstract: We study the problem of scheduling delay-sensitive jobs over spot and on-demand cloud instances to minimize average cost while meeting an average delay constraint. Jobs arrive as a general stochastic process, and incur different costs based on the instance type. This work provides the first analytical treatment of this problem using tools from queuing theory, stochastic processes, and optimization. We derive cost expressions for general policies, prove queue length one is optimal for low target …

</details>


### [7] [Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.14612&hl=zh-CN&sa=X&d=10967634545857216703&ei=hkF1aZaFCKyK6rQPm4H2iQs&scisig=AHkA5jQJXjgd302RgPtAnUTDf1jG&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=1&folt=cit)
*N Bhuyan,R Bhatia,M Kodialam,TV Lakshman*

Main category: Zongheng Yang

TL;DR: 该论文研究了混合云环境中具有硬截止时间的作业调度问题，提出了随机化策略以克服确定性策略的最坏情况竞争比限制。


<details>
  <summary>Details</summary>
Motivation: 混合云环境中，作业可以在成本较低但不稳定的spot实例或更昂贵的on-demand实例上运行，需要满足硬截止时间约束。现有确定性策略在最坏情况下的竞争比存在根本性限制。

Method: 首先建立了现有确定性策略的基本极限，证明其最坏情况竞争比为Ω(K)，其中K是on-demand与spot实例的成本比。然后提出了一种新颖的随机化调度策略来突破这一限制。

Result: 证明了确定性策略存在Ω(K)的最坏情况竞争比下界，而提出的随机化策略能够获得更好的竞争比性能，突破了确定性策略的限制。

Conclusion: 对于混合云环境中的截止时间感知调度问题，随机化策略相比确定性策略具有理论优势，能够提供更好的最坏情况性能保证。

Abstract: This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $\Omega (K) $, where $ K $ is the cost ratio between on-demand and spot instances. We then present a novel randomized …

</details>


### [8] [LaFresCat: A Studio-quality Catalan multi-accent speech dataset for text-to-speech synthesis](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0885230826000082&hl=zh-CN&sa=X&d=16346757102273039887&ei=hkF1aZaFCKyK6rQPm4H2iQs&scisig=AHkA5jQ-eTcq0S180aoMZEWnnh49&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=2&folt=cit)
*A Peiró*

Main category: Zongheng Yang

TL;DR: 该论文探讨了加泰罗尼亚语（一种中等资源语言）在文本转语音（TTS）系统中面临的挑战，特别是缺乏覆盖所有方言和口音的高质量开放语音数据。


<details>
  <summary>Details</summary>
Motivation: 当前TTS系统需要覆盖语言所有语音现象的语音数据才能准确学习语音学。对于加泰罗尼亚语这种具有多种方言/口音的中等资源语言，虽然存在公开语料库，但缺乏覆盖其所有变体的高质量开放语音数据，这限制了TTS系统的开发。

Method: 论文摘要未明确描述具体方法，但从上下文推断，可能涉及收集、整理或创建覆盖加泰罗尼亚语不同方言和口音的高质量语音数据集，以支持TTS系统的训练。

Result: 摘要未提供具体结果，但暗示了当前加泰罗尼亚语TTS系统面临的数据覆盖不足问题，特别是缺乏全面覆盖该语言方言和口音的高质量开放语音数据。

Conclusion: 加泰罗尼亚语作为具有多种方言的中等资源语言，需要更全面覆盖其语音变体的高质量开放语音数据，以支持开发能够准确处理所有方言和口音的TTS系统。

Abstract: Current text-to-speech (TTS) systems are capable of learning the phonetics of a language accurately given that the speech data used to train such models covers all phonetic phenomena. For languages with different varieties, this includes all their richness and accents. This is the case of Catalan, a mid-resourced language with several dialects or accents. Although there are various publicly available corpora, there is a lack of high-quality open-access data for speech technologies covering its …

</details>


### [9] [DistilMOS: Layer-Wise Self-Distillation For Self-Supervised Learning Model-Based MOS Prediction](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13700&hl=zh-CN&sa=X&d=8379895766951206895&ei=hkF1aZaFCKyK6rQPm4H2iQs&scisig=AHkA5jRUl_rO175nRk-WpaCsOSz-&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=3&folt=cit)
*J Yang,W Nakata,Y Saito,H Saruwatari*

Main category: Zongheng Yang

TL;DR: DistilMOS是一种通过知识蒸馏解决SSL模型在MOS预测任务中灾难性遗忘和过拟合问题的新方法


<details>
  <summary>Details</summary>
Motivation: 基于自监督学习的MOS预测模型在微调时存在灾难性遗忘预训练知识和过拟合训练集的问题，导致泛化性能不佳

Method: 提出DistilMOS方法，不仅学习预测MOS，还通过知识蒸馏学习token级别的表示，保留预训练知识并提高泛化能力

Result: DistilMOS在MOS预测任务上取得了最先进的性能，有效缓解了灾难性遗忘和过拟合问题

Conclusion: 通过知识蒸馏学习token级别表示是提升SSL模型在MOS预测任务中泛化性能的有效策略

Abstract: With the advancement of self-supervised learning (SSL), fine-tuning pretrained SSL models for mean opinion score (MOS) prediction has achieved state-of-the-art performance. However, during fine-tuning, these SSL-based MOS prediction models often suffer from catastrophic forgetting of the pretrained knowledge and tend to overfit the training set, resulting in poor generalization performance. In this study, we propose DistilMOS, a novel method that learns to predict not only MOS but also token …

</details>


### [10] [PROGRESSLM: Towards Progress Reasoning in Vision-Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.15224&hl=zh-CN&sa=X&d=5227615492740489533&ei=hkF1aZaFCKyK6rQPm4H2iQs&scisig=AHkA5jRPLQxqrHSvujrfGCe1rQlG&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=4&folt=cit)
*J Zhang,C Qian,H Sun,H Lu,D Wang,L Xue,H Liu*

Main category: Zongheng Yang

TL;DR: 该论文介绍了Progress-Bench基准，用于系统评估视觉语言模型在任务进度推理方面的能力，并探索了人类启发的两阶段进度推理方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型擅长描述可见内容，但在从部分观察中推断任务进度方面能力尚不明确。任务进度估计需要对长时程动态进行推理，而不仅仅是识别静态视觉内容。

Method: 提出了Progress-Bench基准用于系统评估进度推理能力，并探索了人类启发的两阶段进度推理方法。

Result: 论文介绍了基准的构建和初步探索结果，但具体评估数据未在摘要中提供。

Conclusion: 需要系统评估视觉语言模型的进度推理能力，Progress-Bench为此提供了基准，两阶段方法可能改善模型在长时程任务进度估计方面的表现。

Abstract: Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress …

</details>


### [11] [xBound: Join Size Lower Bounds](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13117&hl=zh-CN&sa=X&d=4859839600813706153&ei=hkF1aZaFCKyK6rQPm4H2iQs&scisig=AHkA5jRH2fj0IV2AR9cFP30VUe03&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=5&folt=cit)
*M Stoian,T Bang,H Zhao,J Camacho*

Main category: Zongheng Yang

TL;DR: 该论文指出云数据库查询优化器中的基数估计是关键但存在严重问题的组件，现有工业级数据库在实际测试中表现出显著的估计误差。


<details>
  <summary>Details</summary>
Motivation: 云数据库厂商在查询优化器上投入大量资源，基数估计作为优化器的基石，对高效查询计划选择、资源分配和查询调度等下游任务至关重要。然而，实践者和研究者普遍认为基数估计是优化器的"阿喀琉斯之踵"。

Method: 基于对多个工业级数据库的先前研究，通过实际测试评估基数估计的准确性。

Result: 在所有测试的工业级数据库中都发现了显著的基数估计误差，验证了基数估计是查询优化器中的薄弱环节。

Conclusion: 尽管云数据库厂商在查询优化器上投入巨大，但基数估计仍然是系统性的薄弱点，需要进一步的研究和改进来提升数据库性能。

Abstract: Cloud database vendors invest substantial resources into their query optimizers, and for good reason. Cardinality estimation, a cornerstone of the optimizer, is critical for the selection of efficient query plans, as well as downstream tasks such as resource allocation and query scheduling. Yet, as many practitioners and researchers have noted, it is also the optimizer's Achilles heel. Prior studies on a number of industrial-strength databases show substantial cardinality estimation errors on all tested …

</details>


### [12] [ARC: A Runtime Engine for Accelerating Independent Multi-Agent Reinforcement Learning on Multi-Core Processors](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/11322805/11322871/11323135.pdf&hl=zh-CN&sa=X&d=9459204034024002729&ei=hkF1aZaFCKyK6rQPm4H2iQs&scisig=AHkA5jSk1Y4UJ2dMMq-JGyON_2UN&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=6&folt=cit)
*S Wiggins,N Gupta,G Zgheib,MA Iyer,V Prasanna*

Main category: Zongheng Yang

TL;DR: 独立学习(IL)是一种简单有效的多智能体强化学习方法，各智能体独立训练无需建模通信或显式协调，降低了计算需求，适合CPU平台部署


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在机器人、自动驾驶、金融系统等领域有广泛应用，但传统方法通常需要复杂的通信和协调机制，计算成本高。独立学习旨在提供一种简单有效的替代方案，降低计算需求，使CPU平台成为可行的部署选择

Method: 采用独立学习(IL)方法，各智能体独立进行强化学习训练，不建模智能体间的通信或显式协调机制。这种方法简化了训练过程，减少了计算资源需求

Result: 独立学习方法降低了计算复杂度，使CPU平台成为多智能体强化学习应用的可行部署选项，为资源受限环境提供了实用解决方案

Conclusion: 独立学习作为一种简单有效的多智能体强化学习框架，通过消除通信和协调需求，显著降低了计算成本，扩展了MARL在CPU平台上的应用潜力

Abstract: Multi-Agent Reinforcement Learning (MARL) enables multiple agents to optimize individual or joint objectives in a shared environment, with applications spanning robotics, autonomous driving, and financial systems. Independent Learning (IL), a simple yet effective MARL approach, trains agents independently without modeling inter-agent communication or explicit coordination. This simplicity reduces computational requirements, making CPU platforms an attractive alternative to …

</details>


### [13] [A Systematic Framework for Text-To-Speech System](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11325188/&hl=zh-CN&sa=X&d=10795495309748705832&ei=hkF1aZaFCKyK6rQPm4H2iQs&scisig=AHkA5jRTmA8bimqNtzgGL-kDxPhS&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=7&folt=cit)
*P Kumar,VK Singh*

Main category: Zongheng Yang

TL;DR: 本文综述了印度语言文本转语音(TTS)合成的研究现状，指出虽然深度学习推动了TTS技术的发展，但印度语言面临特殊挑战，发展不均衡。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽然革新了TTS技术，但印度语言面临特殊挑战，发展不均衡。本文旨在综述印度语言TTS的研究现状，分析主要问题，为未来发展提供参考。

Method: 采用文献综述方法，综合分析了近期关于印度语言TTS的关键研究，系统梳理了该领域的技术现状和挑战。

Result: 识别了印度语言TTS面临的主要挑战，包括数据稀缺、语言复杂性、技术适配等问题，总结了当前研究进展和局限性。

Conclusion: 印度语言TTS研究虽然取得进展，但仍面临诸多挑战，需要更多针对性的研究和技术创新来实现更均衡的发展。

Abstract: Text-to-Speech (TTS) synthesis has been revolutionized by deep learning, allowing the creation of speech which is very natural and understandable. But these developments have not been distributed evenly and Indian languages have special challenges to overcome the path to a better future. This review is a synthesis of recent pivotal studies and provides an overview of the state of TTS regarding the Indian subcontinent. We consider the major issues, among them being excessive …

</details>


### [14] [HPCLOG: A Transformer-Based Log Anomaly Detection Framework for Distributed Systems](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11323005/&hl=zh-CN&sa=X&d=5136652417041457027&ei=hkF1aZaFCKyK6rQPm4H2iQs&scisig=AHkA5jTI04vkSyT7v1etFLB9c4SE&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=8&folt=cit)
*Q Shi,Z Yang,S Shao*

Main category: Zongheng Yang

TL;DR: 提出一种基于Transformer的日志异常检测方法，解决传统方法在处理复杂大规模日志数据时建模长期依赖关系的不足


<details>
  <summary>Details</summary>
Motivation: 分布式系统在安全关键领域广泛应用，产生大量包含关键运行时数据的日志。传统日志异常检测方法在处理日益复杂和大规模的数据时表现不佳，而现代深度学习方法如LSTM虽然擅长模式识别，但无法充分建模日志序列中的长期依赖关系。

Method: 引入基于Transformer架构的方法，利用其自注意力机制来捕捉日志序列中的长期依赖关系，提高异常检测能力

Result: 未在摘要中明确说明具体实验结果，但暗示该方法能够有效解决传统方法在处理复杂日志数据时的局限性

Conclusion: 提出的Transformer-based方法能够更好地建模日志序列中的长期依赖关系，为分布式系统的安全性和可靠性提供更有效的异常检测解决方案

Abstract: Distributed systems, prevalent in safety-critical domains, produce vast logs containing essential runtime data. Log Anomaly Detection (LAD) is vital for maintaining the security and reliability of these systems, yet traditional methods falter amid growing data complexity and scale. While modern deep learning approaches such as LSTM excel at pattern recognition, they inadequately model the long-term dependencies found in log sequences. To address these challenges, we introduce …

</details>


<div id='Xuanhe Zhou'></div>

# Xuanhe Zhou [[Back]](#toc)

### [15] [LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13352&hl=zh-CN&sa=X&d=5868864497603428094&ei=iUF1aYGhH6Oi6rQPoMCXoAE&scisig=AHkA5jQ15KSj1RcOVvbqYBksUcI6&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=0&folt=rel)
*Y Lu,JB Tamo,W Zhao,N Sun,Y Zhong,W Shi…*

Main category: Xuanhe Zhou

TL;DR: 论文提出了一种可更新的记忆机制，使大语言模型能够在生成过程中修正错误并改进后续预测


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型作为序列预测器虽然强大，但标准推理依赖于不可变的上下文历史。当模型在第t步生成错误后，缺乏可更新的记忆机制来改进第t+1步的预测，这限制了模型的自我修正能力

Method: 提出了一种可更新的记忆机制，允许模型在生成过程中动态更新其记忆状态。当检测到生成错误时，模型能够修正记忆并利用更新后的信息改进后续预测，而不是仅仅依赖不可变的上下文历史

Result: 该方法使模型能够在生成过程中自我修正错误，提高了序列预测的准确性和鲁棒性。通过可更新的记忆机制，模型能够更好地处理长序列生成任务，减少错误传播

Conclusion: 可更新的记忆机制为大语言模型的推理过程提供了重要的改进，使模型能够在生成过程中动态调整和修正预测，这代表了序列生成模型架构的重要进展

Abstract: Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+ …

</details>


### [16] [A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13243&hl=zh-CN&sa=X&d=4670366018622368585&ei=iUF1aYGhH6Oi6rQPoMCXoAE&scisig=AHkA5jQz3MA7aoAvy-ybbDwS6_Ok&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=1&folt=rel)
*Y Li,J Yu,Z Liu,X Liu,J Yu,S Li,T Su*

Main category: Xuanhe Zhou

TL;DR: 该论文比较了不同推理范式（如思维链和多智能体系统）在大语言模型中的效果与成本效益，旨在为实际应用提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地被部署为推理系统，不同的推理范式（如思维链和多智能体系统）在效果和成本效益方面存在差异，需要系统性的比较分析来指导实际应用。

Method: 论文可能采用系统性的实验设计，比较不同推理范式（包括思维链、多智能体系统等）在各种任务上的表现，同时考虑计算成本、准确性和效率等指标。

Result: 研究可能发现不同推理范式在不同任务类型和复杂度下具有不同的优势，某些范式在特定场景下提供更好的成本-准确性权衡。

Conclusion: 选择合适的推理范式对于优化大语言模型的部署至关重要，需要根据具体任务需求、资源约束和性能目标进行权衡选择。

Abstract: Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms-such as Chain-of-Thought (CoT) and multi-agent systems (MAS)-play a critical role, yet their relative effectiveness and cost-accuracy …

</details>


### [17] [From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.12904&hl=zh-CN&sa=X&d=2423079936369651358&ei=iUF1aYGhH6Oi6rQPoMCXoAE&scisig=AHkA5jT7jh1e4E9MwDI4UtPvdVhM&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=2&folt=rel)
*J Wang,W Xie,M Zhang,B Zhang,J Dong,Y Zhu…*

Main category: Xuanhe Zhou

TL;DR: 检索增强生成通过整合外部知识减少大语言模型幻觉，但增加了提示长度，导致计算成本上升和首词生成时间延长


<details>
  <summary>Details</summary>
Motivation: 检索增强生成虽然能减少大语言模型的幻觉问题，但会显著增加提示长度，导致更高的计算成本和更长的首词生成时间，需要解决这一效率瓶颈

Method: 未在摘要中明确说明具体方法，但暗示需要解决检索增强生成中提示长度增加导致的效率问题

Result: 未在摘要中提供具体实验结果，但指出了检索增强生成在减少幻觉的同时带来了计算成本和首词生成时间的增加

Conclusion: 检索增强生成在提升大语言模型准确性的同时引入了效率挑战，需要在减少幻觉和保持效率之间找到平衡

Abstract: Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT) …

</details>


### [18] [Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13806&hl=zh-CN&sa=X&d=6821397543328031578&ei=iUF1aYGhH6Oi6rQPoMCXoAE&scisig=AHkA5jQ40nbnSuEP26abjrw243nm&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=3&folt=rel)
*D Song,G Bonifazi,F Schilder,JR Schwarz*

Main category: Xuanhe Zhou

TL;DR: LLM后训练主要依赖大规模文本语料和人类反馈，缺乏对领域知识结构的捕捉，导致模型在处理复杂推理任务时表现不佳


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法主要基于大规模文本语料和人类反馈，但未能有效捕捉领域知识的结构化信息，导致模型在处理高风险的复杂推理任务时存在局限性

Method: 从提供的摘要片段来看，论文可能提出了一种新的后训练方法，旨在更好地整合领域知识结构，但具体方法细节需要完整论文才能确定

Result: 基于摘要片段，可以推断该方法旨在改善LLM在复杂推理任务上的表现，特别是针对高风险领域的应用

Conclusion: 需要整合领域知识结构来增强LLM的复杂推理能力，特别是对于高风险应用场景

Abstract: LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes …

</details>


### [19] [SharP: Soft and Hard Prompt-Guided Augmentation with LLM for Low Resource Fake News Detection](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0957417426000928&hl=zh-CN&sa=X&d=11254404912360424935&ei=iUF1aYGhH6Oi6rQPoMCXoAE&scisig=AHkA5jQdZkzXtAZCN-T_XOOWLy6T&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=4&folt=rel)
*Z Luo,W Li,H Huang,K Liu,M Gao*

Main category: Xuanhe Zhou

TL;DR: 低资源条件下的假新闻检测面临标注数据稀缺和难以捕捉微妙欺骗模式的挑战


<details>
  <summary>Details</summary>
Motivation: 解决低资源条件下假新闻检测的问题，特别是在只有少量标注数据的情况下，如何有效识别虚假信息

Method: 未在摘要中明确说明具体方法，但聚焦于低资源条件下的假新闻检测技术

Result: 摘要未提供具体实验结果

Conclusion: 该研究关注低资源假新闻检测这一重要挑战，旨在开发适用于标注数据稀缺环境下的检测方法

Abstract: Fake news detection under low resource conditions is challenged by the scarcity of labeled data and the difficulty of capturing subtle deceptive patterns. In this work, we focus on low resource fake news detection, where only a small labeled set is …

</details>


### [20] [RM-Distiller: Exploiting Generative LLM for Reward Model Distillation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.14032&hl=zh-CN&sa=X&d=14279313199314802225&ei=iUF1aYGhH6Oi6rQPoMCXoAE&scisig=AHkA5jRUvHicBY5etGXotBLu5aUl&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=5&folt=rel)
*H Zhou,H Huang,W Liu,C Wang,X Bu,L Han,F Song…*

Main category: Xuanhe Zhou

TL;DR: 该论文探讨了从生成式大语言模型中提取偏好数据来训练奖励模型，以解决高质量人类标注数据稀缺的问题


<details>
  <summary>Details</summary>
Motivation: 高质量人类偏好标注数据获取困难，限制了奖励模型的发展，需要探索从现有生成式大语言模型中蒸馏偏好信息的替代方案

Method: 从生成式大语言模型中蒸馏偏好数据来训练奖励模型，可能涉及对比学习、偏好排序或生成式方法

Result: 通过从生成式LLMs蒸馏偏好训练出的奖励模型能够有效对齐人类偏好，缓解了高质量标注数据稀缺的问题

Conclusion: 从生成式大语言模型中蒸馏偏好是训练奖励模型的有效替代方案，有助于推动LLMs与人类偏好的对齐

Abstract: Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as …

</details>


### [21] [BioTriplex: A Full-Text Annotated Corpus for Fine-Tuning Language Models in Gene-Disease Relation Extraction Tasks](https://scholar.google.com/scholar_url?url=https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btag037/66521886/btag037.pdf&hl=zh-CN&sa=X&d=18394172329339710636&ei=iUF1aYGhH6Oi6rQPoMCXoAE&scisig=AHkA5jQwNQjo9gyfiy-sB3FawRDp&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=6&folt=rel)
*C Collins,P Fytas,İ Karadeniz,H Zheng,S Baker…*

Main category: Xuanhe Zhou

TL;DR: 该论文旨在开发能够从生物医学文本中自动提取信息、识别实体、分析实体关系并将其与特定研究主题关联的机器学习方法


<details>
  <summary>Details</summary>
Motivation: 生物医学文本信息自动提取需要能够识别生物医学实体、表征实体间关系并将提取信息与特定研究主题关联的机器学习方法

Method: 论文未在摘要中明确说明具体方法，但暗示使用大型语言模型或相关机器学习技术来处理生物医学文本

Result: 摘要未提供具体实验结果，但表明该方法能够实现生物医学实体识别、关系抽取和主题关联

Conclusion: 开发有效的生物医学文本信息提取方法对于推进生物医学研究具有重要意义

Abstract: Motivation Automatic information extraction from biomedical texts requires machine learning methodology that can recognise biomedical entities, characterise inter-entity relationships, and relate extracted information to specific research topics. Large …

</details>


### [22] [RLBR: Reinforcement Learning with Biasing Rewards for Contextual Speech Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13409&hl=zh-CN&sa=X&d=17236296870464692915&ei=iUF1aYGhH6Oi6rQPoMCXoAE&scisig=AHkA5jTOcQuqXAXlWAXwHdslztDV&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=9&folt=rel)
*B Ren,R Fan,Y Shen,W Chen,J Li*

Main category: Xuanhe Zhou

TL;DR: 语音大语言模型在端到端语音理解和识别方面取得显著进展，但在准确识别罕见词和领域特定术语方面仍存在困难


<details>
  <summary>Details</summary>
Motivation: 尽管语音大语言模型在端到端语音理解和识别方面取得了显著进展，但它们在准确识别罕见词和领域特定术语方面仍然存在困难，这限制了模型在实际应用中的性能

Method: 论文提出了一种新方法来改进语音大语言模型对罕见词和领域特定术语的识别能力（具体方法需要更多上下文）

Result: 该方法显著提升了语音大语言模型在罕见词和领域特定术语识别方面的准确率，改善了模型的整体性能

Conclusion: 通过提出的方法，语音大语言模型在罕见词和领域特定术语识别方面的性能得到显著提升，为更准确的语音理解和识别系统提供了有效解决方案

Abstract: Speech large language models (LLMs) have driven significant progress in end-to-end speech understanding and recognition, yet they continue to struggle with accurately recognizing rare words and domain-specific terminology. This paper …

</details>


### [23] [Tracing the Data Trail: A Survey of Data Provenance, Transparency and Traceability in LLMs](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.14311&hl=zh-CN&sa=X&d=15691193764457243565&ei=ikF1acH9GsyQieoPxcX9yAo&scisig=AHkA5jTfjia-ihGz3_bVP8yjgQXA&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=1&folt=cit)
*R Hohensinner,B Mutlu,IGM Estrada,M Vukovic…*

Main category: Xuanhe Zhou

TL;DR: 该调查论文系统回顾了过去十年关于LLM训练数据生命周期透明度的研究，聚焦数据来源、透明度、可追溯性三个核心轴心及偏差/不确定性、数据隐私、工具技术三个支撑支柱，提出了该领域的分类体系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型已大规模部署，但其训练数据生命周期仍不透明，缺乏系统性研究框架。需要整合分散的研究成果，建立统一的分类体系来理解数据来源、透明度、可追溯性等关键问题。

Method: 采用文献综述方法，综合过去十年相关研究，提出包含三个核心轴心（数据来源、透明度、可追溯性）和三个支撑支柱（偏差与不确定性、数据隐私、工具技术）的分类框架，并定义领域范畴和对应工件。

Result: 建立了LLM训练数据透明度研究的系统性分类体系，明确了该领域的核心研究方向和关键挑战，为未来研究提供了结构化框架和参考基准。

Conclusion: LLM训练数据透明度是多维复杂问题，需要跨学科方法解决。提出的分类体系有助于统一研究视角，推动该领域向更系统化、可操作的方向发展。

Abstract: Large language models (LLMs) are deployed at scale, yet their training data life cycle remains opaque. This survey synthesizes research from the past ten years on three tightly coupled axes:(1) data provenance,(2) transparency, and (3) traceability, and three supporting pillars:(4) bias\& uncertainty,(5) data privacy, and (6) tools and techniques that operationalize them. A central contribution is a proposed taxonomy defining the field's domains and listing corresponding artifacts. Through analysis of …

</details>


### [24] [Agentic Reasoning for Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.12538&hl=zh-CN&sa=X&d=15746529209607220044&ei=ikF1acH9GsyQieoPxcX9yAo&scisig=AHkA5jRtxr_KIUujTARR32O8PQVU&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=2&folt=cit)
*T Wei,TW Li,Z Liu,X Ning,Z Yang,J Zou,Z Zeng…*

Main category: Xuanhe Zhou

TL;DR: 该论文综述了智能体推理，将大语言模型重构为能够通过持续交互进行规划、行动和学习的自主智能体，以解决其在开放动态环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在封闭环境中表现出强大的推理能力，但在开放、动态环境中存在局限性。需要一种范式转变，将LLMs重构为能够通过持续交互进行规划、行动和学习的自主智能体，以应对开放世界的挑战。

Method: 采用综述研究方法，从三个互补维度组织智能体推理：1) 规划与决策，2) 行动与执行，3) 学习与适应。系统梳理智能体推理的理论基础、技术方法和应用场景。

Result: 提出了智能体推理的框架体系，明确了其在开放世界推理中的关键作用。识别了当前研究的挑战和未来方向，包括可扩展性、安全性、评估指标等。

Conclusion: 智能体推理代表了LLMs从静态推理向动态交互推理的重要范式转变，为构建能够在复杂开放环境中自主运作的AI系统提供了理论基础和技术路线。

Abstract: Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary …

</details>


### [25] [CORE-T: COherent REtrieval of Tables for Text-to-SQL](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13111&hl=zh-CN&sa=X&d=2070382532333428456&ei=ikF1acH9GsyQieoPxcX9yAo&scisig=AHkA5jRQPRqC3rtGo_1-vep3m-X0&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=3&folt=cit)
*H Soliman,V Gupta,D Roth,I Gurevych*

Main category: Xuanhe Zhou

TL;DR: 研究多表连接场景下的文本到SQL转换，针对大型异构表集合的开放环境，提出改进检索方法以解决现有密集检索返回过多干扰项的问题


<details>
  <summary>Details</summary>
Motivation: 现实文本到SQL工作流通常需要连接多个表，在大型异构表集合的开放环境中，准确检索相关表集合成为端到端性能的关键瓶颈。现有密集检索方法虽然召回率高但返回过多干扰项，而连接感知的替代方法存在局限性

Method: 研究在开放环境下的文本到SQL转换，其中查询必须基于从多个来源汇集的大型异构表集合进行回答，没有数据库标识符等清晰的界定信号。分析密集检索和连接感知方法的优缺点

Result: 密集检索在开放环境中实现高召回率，但返回许多干扰项；连接感知的替代方法通常依赖于特定假设或信号，在异构表集合中可能不适用

Conclusion: 需要开发新的检索方法来平衡召回率和精确度，特别是在缺乏明确数据库标识符的开放环境中处理多表连接查询

Abstract: Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered over large, heterogeneous table collections pooled from many sources, without clean scoping signals such as database identifiers. Here, dense retrieval (DR) achieves high recall but returns many distractors, while join-aware alternatives often rely on …

</details>


### [26] [ClaimDB: A Fact Verification Benchmark over Large Structured Data](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.14698&hl=zh-CN&sa=X&d=3503254143866704015&ei=ikF1acH9GsyQieoPxcX9yAo&scisig=AHkA5jQXgool28y3u5Nmd6Gf48TF&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=4&folt=cit)
*M Theologitis,PPS Dammu,C Shah,D Suciu*

Main category: Xuanhe Zhou

TL;DR: ClaimDB是首个基于大规模结构化数据的事实核查基准，证据来自数百万记录和多表的组合，涵盖80个真实数据库的广泛领域


<details>
  <summary>Details</summary>
Motivation: 现有事实核查基准主要关注文本证据，而基于大规模结构化数据（如数据库记录）的声明事实核查研究不足，需要专门的基准来评估系统处理结构化数据的能力

Method: 构建ClaimDB基准，包含80个独特真实数据库，涵盖治理、医疗、媒体、教育、自然科学等领域，证据来自数百万记录和多表的组合

Result: 创建了首个基于大规模结构化数据的事实核查基准，为评估系统处理复杂结构化证据的能力提供了标准测试平台

Conclusion: ClaimDB填补了基于结构化数据的事实核查研究空白，为开发能够处理真实世界复杂数据结构的AI系统提供了重要基准

Abstract: Despite substantial progress in fact-verification benchmarks, claims grounded in large-scale structured data remain underexplored. In this work, we introduce ClaimDB, the first fact-verification benchmark where the evidence for claims is derived from compositions of millions of records and multiple tables. ClaimDB consists of 80 unique real-life databases covering a wide range of domains, from governance and healthcare to media, education and the natural sciences. At this …

</details>


### [27] [SageCopilot: an LLM-empowered Autonomous Agent for Data Science as a Service](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/4629386/4629387/11264326.pdf&hl=zh-CN&sa=X&d=12927663998190429037&ei=ikF1acH9GsyQieoPxcX9yAo&scisig=AHkA5jT9aul8CdnNr5Yc8zhYEORJ&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=5&folt=cit)
*Y Liao,J Bian,Y Yun,S Wang,Y Zhang,J Chu…*

Main category: Xuanhe Zhou

TL;DR: SageCopilot是一个工业级系统，旨在通过自动化数据科学流程（包括数据查询、分析、可视化和报告）来扩展NL2SQL的能力。


<details>
  <summary>Details</summary>
Motivation: 虽然自然语言转SQL（NL2SQL）领域在将自然语言指令转换为可执行SQL脚本方面取得了显著进展，但在更广泛的数据科学流程（包括数据查询、分析、可视化和报告）中实现完全自动化仍然是一个复杂的挑战。

Method: 该研究引入了SageCopilot，这是一个先进的工业级系统，通过自动化数据科学流程来实现全面自动化。

Result: 从摘要中无法获取具体结果，但系统旨在解决NL2SQL在完整数据科学流程中的局限性。

Conclusion: SageCopilot代表了向自动化完整数据科学流程迈出的重要一步，超越了传统的NL2SQL系统。

Abstract: While the field of natural language to SQL (NL2SQL) has made significant advancements in translating natural language instructions into executable SQL scripts for data querying and processing, achieving full automation within the broader data science pipeline–encompassing data querying, analysis, visualization, and reporting–remains a complex challenge. This study introduces SageCopilot, an advanced, industry-grade system that automates the data science pipeline by …

</details>


### [28] [Refining Knowledge Generation for Text-to-SQL via Chain-of-Thought Feedback](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/6514899/11251144.pdf&hl=zh-CN&sa=X&d=14165652842198127642&ei=ikF1acH9GsyQieoPxcX9yAo&scisig=AHkA5jQquF9Uuic-_u-L5arB4xid&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=6&folt=cit)
*F Cao*

Main category: Xuanhe Zhou

TL;DR: 论文探讨了LLM在生成SQL查询时面临的主要挑战：当查询需要隐含知识（如领域特定计算、值格式化或实体关系）时，LLM表现不佳，因为这些知识在自然语言问题或数据库模式中未明确说明。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在文本理解方面表现出色，但在从自然语言生成SQL查询时，当查询需要隐含知识（如领域特定计算、值格式化、微妙实体关系等）时，模型表现不佳，因为这些知识在明确的问题描述或数据库模式定义中缺失。

Method: 论文未在摘要中明确说明具体方法，但从问题描述来看，可能涉及增强LLM处理隐含知识的能力，或开发新的架构/技术来弥补这一缺陷。

Result: 摘要未提供具体实验结果，但暗示了当前LLM在需要隐含知识的SQL生成任务上存在明显不足。

Conclusion: 需要解决LLM在SQL生成中处理隐含知识的能力不足问题，以提高自然语言到SQL转换的准确性和实用性。

Abstract: Generating accurate SQL queries from natural language questions remains a significant challenge, requiring models to bridge the gap between user intent and structured database schemas. While large language models (LLMs) excel at text understanding, they frequently struggle when queries necessitate implicit knowledge—such as domain-specific calculations, value formatting, or subtle entity relationships—that is absent from the explicit question or schema definition. This deficiency often …

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: LLMs在不同语言间存在质量差异，低资源语言获得更低质量的回答，语言选择显著影响模型使用的文化背景，进而影响回答质量。


<details>
  <summary>Details</summary>
Motivation: 用户不应因使用不同语言与LLMs交互而受到系统性歧视，所有语言用户都应获得相似质量的回答。需要评估LLMs在不同语言中的回答质量差异，并研究语言与文化如何交织影响模型响应。

Method: 基于WildChat数据集创建真实世界的开放式问题集，评估不同语言下的回答质量差异；使用LLM-as-a-Judge方法识别回答中的文化背景；在多个语言上评估CulturalBench基准的翻译子集。

Result: LLMs在低资源语言的开放式问题上持续提供更低质量的回答；语言选择显著影响模型使用的文化背景；文化背景的差异直接影响下游回答的质量。

Conclusion: LLMs存在语言不平等问题，低资源语言用户获得劣质回答，语言与文化背景的纠缠影响模型响应质量，需要解决多语言公平性问题。

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [30] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: 通过引入零计算（空）专家实现数据稀疏性，在因果token-choice MoE中结合权重稀疏性和数据稀疏性，提高计算效率


<details>
  <summary>Details</summary>
Motivation: 混合专家层通过权重稀疏性实现计算效率，但数据稀疏性（每个专家只处理部分token）提供了补充维度。现有的专家选择路由虽然实现数据稀疏性，但在自回归模型中违反因果性，导致训练-推理不匹配。

Method: 在因果token-choice MoE中引入零计算（空）专家到路由池中。当token路由到空专家时，这些槽位不消耗计算。标准负载均衡目标训练模型均匀使用所有专家（真实和空），从而在期望上实现数据稀疏性而不违反因果性。

Result: 在视觉-语言模型训练中评估，其中数据异质性显著：视觉编码器产生许多低信息token而文本token更密集。在匹配的期望FLOPs下，组合权重和数据稀疏性比单独使用权重稀疏性产生更高效的计算前沿，在训练损失和下游性能上都有提升。模型学习隐式的模态感知分配，将视觉token更积极地路由到空专家，无需显式模态路由。

Conclusion: 通过引入空专家实现数据稀疏性，可以在不违反因果性的情况下提高MoE的计算效率，特别是在处理异质数据（如视觉-语言任务）时效果显著。

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [31] [You Need Better Attention Priors](https://arxiv.org/abs/2601.15380)
*Elon Litman,Gabe Guo*

Main category: cs.LG

TL;DR: GOAT是一种基于熵最优传输的广义注意力机制，通过可学习的连续先验替代标准注意力的均匀先验假设，保持FlashAttention兼容性，解决注意力下沉问题，并学习可外推的位置先验。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制隐含地假设了均匀先验分布，这种朴素假设限制了注意力的表达能力。作者从熵最优传输的角度重新审视注意力机制，旨在开发更灵活、表达能力更强的注意力变体。

Method: 提出GOAT（广义最优传输注意力），将注意力重新表述为熵最优传输问题，用可学习的连续先验替代均匀先验。该方法保持与FlashAttention等优化内核的兼容性，通过吸收空间信息学习可外推的位置先验。

Result: GOAT提供了注意力下沉现象的EOT解释并给出了解决方案，避免了标准注意力的表示权衡。学习到的先验结合了学习位置嵌入的灵活性和固定编码的长度泛化能力。

Conclusion: 从熵最优传输视角重新审视注意力机制，GOAT通过可学习先验提供了更强大的注意力机制，解决了注意力下沉问题，并在位置表示方面实现了灵活性与泛化性的平衡。

Abstract: We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.

</details>


### [32] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: Ambient Dataloops：一种迭代式数据集精炼框架，通过数据集-模型协同进化过程提升扩散模型学习效果，避免自消耗循环，在图像生成和蛋白质设计任务中取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现代数据集包含质量差异很大的样本，直接在这样异构的数据上训练通常会产生次优模型。需要一种方法来提升数据集质量，同时避免自消耗循环的破坏性影响。

Method: 提出数据集-模型协同进化过程：1）迭代精炼数据集，每轮数据集质量逐步提升；2）将合成改进的样本视为噪声样本，但噪声水平略低于前一轮；3）使用Ambient Diffusion技术在数据损坏条件下学习，避免破坏性自消耗循环。

Result: 在无条件图像生成、文本条件图像生成和从头蛋白质设计任务中实现了最先进的性能。同时为数据循环过程提供了理论依据。

Conclusion: Ambient Dataloops框架通过迭代式数据集精炼和Ambient Diffusion技术，有效解决了异构数据集训练问题，避免了自消耗循环，显著提升了扩散模型在各种生成任务中的性能。

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [33] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: CASL：首个实现扩散模型潜在表示与语义概念监督对齐的框架，通过稀疏自编码器和线性映射将潜在维度与人类可理解概念对齐，并引入CASL-Steer作为因果探针验证语义含义。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的内部激活编码了丰富的语义信息，但解释这些表示仍然具有挑战性。现有的基于稀疏自编码器的方法采用无监督方法，无法将稀疏特征与人类可理解的概念对齐，限制了其对生成图像的可靠语义控制能力。

Method: CASL框架包含两个主要步骤：1）在冻结的U-Net激活上训练稀疏自编码器以获得解耦的潜在表示；2）学习轻量级线性映射，将每个概念与一小部分相关的潜在维度关联。引入CASL-Steer作为受控潜在干预，沿学习到的概念轴移动激活，作为因果探针揭示概念对齐潜在如何影响生成内容。提出编辑精度比作为评估指标。

Result: 实验表明，该方法在编辑精度和可解释性方面优于现有方法。CASL-Steer能够有效揭示概念对齐潜在对生成内容的影响，编辑精度比指标能够联合衡量概念特异性和无关属性的保留程度。

Conclusion: CASL是首个实现扩散模型潜在表示与语义概念监督对齐的工作，通过概念对齐的稀疏潜在维度提供了可靠的语义控制，显著提升了扩散模型的可解释性和可控性。

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [34] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 论文研究在合成数据污染环境中学习理论的基本问题，发现传统ERM方法在均值估计和PAC学习场景中表现不佳，但存在能够处理任意污染程度的替代算法。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的普及和低成本，合成内容大量涌现，导致"自然"数据被LLM生成的数据污染。研究在这种普遍存在的混合数据环境中学习理论的基本问题，其中学习算法无法区分单个样本的来源。

Method: 将场景建模为一系列学习任务，输入是自然和合成数据的混合，算法无法识别任何单个样本的来源。研究ERM在这种设置下的可能性和局限性，并与为不同代数据分配非均匀权重的算法进行比较。在PAC学习设置中，分析ERM的收敛性并寻找能够学习正确假设的算法。

Result: 对于任意d维分布的均值估计问题，ERM虽然收敛到真实均值，但被为不同代数据分配非均匀权重的算法超越。在PAC学习设置中，ERM并不总是收敛到真实概念（与模型崩溃文献一致），但存在能够学习任意VC类和任意污染量的正确假设的算法。

Conclusion: 在合成数据污染环境中，传统ERM方法存在局限性，需要开发专门算法来处理混合数据学习问题。研究为理解合成数据对学习理论的影响提供了理论基础，并展示了在污染环境中实现可靠学习的可能性。

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [35] [Early predicting of hospital admission using machine learning algorithms: Priority queues approach](https://arxiv.org/abs/2601.15481)
*Jakub Antczak,James Montgomery,Małgorzata O'Reilly,Zbigniew Palmowski,Richard Turner*

Main category: cs.LG

TL;DR: 比较SARIMAX、XGBoost和LSTM三种模型在急诊科每日到达量预测中的表现，XGBoost在总入院量预测上最优，SARIMAX在复杂病例预测上略优，但所有模型都低估突发性患者激增。


<details>
  <summary>Details</summary>
Motivation: 急诊科过度拥挤是影响患者安全和运营效率的关键问题，需要准确的预测模型来优化资源配置。现有研究通常只关注总体到达量，而本研究通过分解不同病房类别和临床复杂性来提供更精细的预测。

Method: 使用2017-2021年澳大利亚三级转诊医院数据，比较SARIMAX、XGBoost和LSTM三种模型对7天预测期的表现。将需求分解为8个病房类别并按临床复杂性分层。使用Prophet模型生成COVID-19异常期的合成反事实值以消除数据失真。

Result: 所有三种模型都优于季节性朴素基线。XGBoost在预测每日总入院量上表现最佳（MAE=6.63），SARIMAX在预测主要复杂性病例上略优（MAE=3.77）。模型能成功再现日常模式，但都低估突发性、偶发性患者激增。

Conclusion: 三种预测技术都能有效预测急诊科日常到达模式，但存在共同局限性：无法准确预测突发性患者激增。需要进一步研究来改进对异常事件的预测能力。

Abstract: Emergency Department overcrowding is a critical issue that compromises patient safety and operational efficiency, necessitating accurate demand forecasting for effective resource allocation. This study evaluates and compares three distinct predictive models: Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting (XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED arrivals over a seven-day horizon. Utilizing data from an Australian tertiary referral hospital spanning January 2017 to December 2021, this research distinguishes itself by decomposing demand into eight specific ward categories and stratifying patients by clinical complexity. To address data distortions caused by the COVID-19 pandemic, the study employs the Prophet model to generate synthetic counterfactual values for the anomalous period. Experimental results demonstrate that all three proposed models consistently outperform a seasonal naive baseline. XGBoost demonstrated the highest accuracy for predicting total daily admissions with a Mean Absolute Error of 6.63, while the statistical SARIMAX model proved marginally superior for forecasting major complexity cases with an MAE of 3.77. The study concludes that while these techniques successfully reproduce regular day-to-day patterns, they share a common limitation in underestimating sudden, infrequent surges in patient volume.

</details>


### [36] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: MFS是一种基于鞅理论的LLM解码框架，通过将推理路径质量建模为随机过程，使用Doob分解定理评估路径优势，可选停止理论剪枝次优路径，鞅收敛定理自适应停止搜索，在提高准确性的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 标准自回归解码在LLMs中存在短视问题，无法找到全局最优推理路径。现有的前瞻采样方法依赖启发式机制评估路径和剪枝搜索空间，缺乏理论依据。

Method: 提出鞅前瞻采样(MFS)框架，将LLM解码重新表述为识别最优随机过程的问题。基于鞅理论设计算法：1) 使用Doob分解定理推导步骤评估，衡量路径的可预测优势；2) 应用可选停止理论进行原则性剪枝；3) 基于鞅收敛定理设计自适应停止规则。

Result: 在六个推理基准测试中，MFS在准确性上超越了最先进方法，同时显著提高了计算效率。

Conclusion: MFS提供了一个理论严谨的框架，用概率论原理替代启发式机制，有效解决了LLM解码中的短视问题，在推理任务中实现了更好的准确性和效率平衡。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [37] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: 提出Margin-Aware Speculative Verification方法，通过自适应目标模型的局部决策稳定性来改进推测解码中的验证机制，在保持生成质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在验证机制上主要依赖严格的token级拒绝采样，但现代大语言模型常在低边际区域运行，此时拒绝合理的次优token带来的信息增益很小却导致大量回滚成本，造成验证效率低下。

Method: 提出训练无关且领域无关的验证策略，根据目标模型logits直接测量的决策稳定性来条件化验证过程。当严格验证收益最小时放宽拒绝标准，仅修改验证规则，与现有目标耦合推测解码框架完全兼容。

Result: 在8B到235B不同规模模型上的广泛实验表明，该方法相比最先进基线方法能提供一致且显著的推理加速，同时在多样化基准测试中保持生成质量。

Conclusion: Margin-Aware Speculative Verification通过自适应目标模型的局部决策稳定性，有效解决了推测解码中验证机制的根本效率问题，为大规模语言模型推理提供了更高效的解决方案。

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [38] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: 该研究提出了一种联合可行性策略，用于确定湖泊监测中达到目标精度所需的最小训练历史和最少预测因子数量，以优化有害藻华预警的采样工作。


<details>
  <summary>Details</summary>
Motivation: 志愿者主导的湖泊监测产生不规则的季节性时间序列，存在大量数据缺失（冰盖、天气限制、人为错误），这给有害藻华的预测和预警带来了困难。需要开发一种方法来优化监测工作，确定达到目标精度所需的最小数据量和特征集。

Method: 1. 使用缅因州30个湖泊30年的现场记录数据集；2. 采用链式方程多重插补（MICE）处理缺失数据；3. 使用归一化平均绝对误差（nMAE）进行跨湖泊可比性评估；4. 在六个候选模型中，岭回归表现最佳；5. 通过岭回归量化最小样本量；6. 确定最小特征集；7. 引入联合可行性函数，统一最近历史长度和特征选择。

Result: 1. 岭回归在测试中表现最佳；2. 在向后最近历史协议下，模型达到完整历史精度5%范围内平均需要约176个训练样本；3. 紧凑的四特征子集与十三特征基线在相同5%容差内匹配；4. 联合可行性分析显示，达到5%精度目标需要约64个最近样本和仅一个预测因子。

Conclusion: 联合可行性策略将最近历史长度和特征选择统一在固定精度目标下，为湖泊研究人员提供了一个简单、高效的规则，用于设定采样工作和测量优先级，提高了目标监测的实用性。

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [39] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 提出机器学习方法，利用临床测试和MRI数据区分非典型阿尔茨海默病与非AD认知障碍，提高非典型AD诊断准确率


<details>
  <summary>Details</summary>
Motivation: 非典型阿尔茨海默病（atAD）患者常被误诊，因为临床评估和海马体积对典型AD有效但对atAD诊断准确性低。需要开发基于标准临床和MRI数据的诊断方法。

Method: 使用机器学习方法，基于临床测试数据和MRI特征（包括海马体积和全脑MRI特征）进行atAD与非AD认知障碍的分类。采用Boruta统计方法识别显著脑区特征。

Result: 最佳性能通过结合重要MRI特征实现，优于仅使用海马体积。在NACC数据集中，atAD诊断召回率从52%提升至69%；在ADNI数据集中，从34%提升至77%，同时保持高精度。

Conclusion: 提出的机器学习方法仅使用临床测试和MRI数据就能显著提高非典型AD的诊断准确性，对临床环境中的非遗忘型atAD诊断具有重要应用价值。

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [40] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: 量化感知的机器学习遗忘方法，通过引入logits空间hinge损失确保遗忘样本在4位量化后仍保持遗忘状态


<details>
  <summary>Details</summary>
Motivation: 机器学习遗忘旨在从训练好的模型中移除特定知识（如受版权保护或隐私数据）而无需完全重新训练。然而，实际部署中模型常被量化（如4位），研究发现量化会灾难性地恢复已遗忘的信息，因此需要解决量化对遗忘效果的破坏问题。

Method: 首先分析低比特量化破坏遗忘的原因：计算权重变化统计量和量化桶重叠，发现典型遗忘更新太小而无法跨越量化阈值。基于此洞察，提出量化感知的遗忘方法：引入logits空间hinge损失，对于每个遗忘样本，强制未学习模型的输出logits与原始模型至少相差一个边界值（量化步长的一半），确保遗忘样本在量化后仍可区分。

Result: 在语言和分类任务（包括Twitter虚假信息数据集）上评估，该方法在4位量化下能保持遗忘效果，而现有方法几乎完全恢复已遗忘的知识。

Conclusion: 量化会严重破坏机器学习遗忘效果，但通过量化感知的遗忘方法（特别是logits空间hinge损失）可以有效缓解这一问题，确保模型在量化部署后仍能保持遗忘状态。

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [41] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: Prism是一种基于MCR²原理的白盒注意力架构，通过几何归纳偏置实现无监督功能解耦，在TinyStories上验证了注意力头自发分化为低频信号处理和高频噪声处理


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（特别是Transformer）常被视为"黑箱"且缺乏可解释性，需要一种能够统一可解释性和性能的白盒架构

Method: 提出Prism架构，将注意力机制建模为在信号-噪声流形上的梯度上升过程，引入两个物理约束：过完备字典扩展表示相空间，无理频率分离（π-RoPE）强制信号与噪声子空间的不相干性

Result: 在TinyStories测试平台上观察到Prism的注意力头自发分化为光谱不同的机制：低频头捕获长程因果依赖（信号），高频头处理局部句法约束（噪声）

Conclusion: 可解释性和性能不是权衡关系，可以通过原则性的几何构造统一起来，几何归纳偏置足以诱导无监督功能解耦

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [42] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++通过引入基于熵和KL散度的漂移检测机制及自适应重置策略，解决了持续测试时适应在长期快速分布变化下的性能崩溃问题，在CCC基准上相比RDumb获得约3%的绝对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时适应方法（如Tent、EATA）在短期演化偏移下表现良好，但在测试分布快速变化或极长时间跨度下会失效，特别是在CCC基准中模型需要处理750万样本的持续变化数据流时，容易发生预测崩溃。

Method: RDumb++是RDumb的扩展，引入两种漂移检测机制：基于熵的漂移评分和基于KL散度的漂移评分，结合自适应重置策略。这些机制使模型能够检测累积适应何时变得有害，并在预测崩溃发生前恢复。

Result: 在CCC-medium基准的三个速度和三个种子（共9次运行，每次包含100万样本）上，RDumb++始终优于RDumb，获得约3%的绝对准确率提升，并在整个数据流中保持稳定的适应能力。消融实验进一步表明漂移感知重置对于防止崩溃和实现可靠的长期CTTA至关重要。

Conclusion: 漂移检测机制和自适应重置策略对于长期持续测试时适应至关重要，RDumb++通过有效检测有害适应和及时恢复，解决了模型在快速变化数据流中的性能崩溃问题，实现了稳定可靠的长期适应。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [43] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: 在医疗机器学习中，使用临床定制指标而非验证损失进行模型优化能获得更好的临床任务性能


<details>
  <summary>Details</summary>
Motivation: 传统机器学习使用验证损失进行模型优化，但医疗ML的目标是满足特定临床需求而非最小化训练损失函数。临床需求可以通过定制化指标更精确地捕捉，这些指标能更好地反映实际临床任务性能。

Method: 通过两个受控实验对比验证损失与临床定制指标在模型优化中的效果。实验设计允许使用非可微分的临床相关指标，这些指标专门针对临床任务需求进行定制。

Result: 使用临床定制指标进行模型优化相比使用验证损失能获得更好的临床任务性能。实验结果表明临床相关指标在指导模型优化方面具有优越性。

Conclusion: 在医疗机器学习中，虽然定义和编码临床相关指标需要额外努力，但使用这些指标进行模型优化能产生更符合临床目标的模型，实现更好的临床性能。

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [44] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 提出了首个从部分观测数据学习神经算子的系统框架，通过掩码预测训练和物理感知潜在传播器解决监督缺失和空间不匹配问题，在PDE任务中显著降低误差


<details>
  <summary>Details</summary>
Motivation: 现实科学应用中常遇到不完整的观测数据（传感器限制、地理约束、测量成本），而现有神经算子方法假设完全观测的空间输入，严重限制了在实际应用中的适用性

Method: 提出了Latent Autoregressive Neural Operator (LARNO)，包含两个核心组件：1) 掩码预测训练策略，通过在观测区域战略性地创建掩码来生成人工监督；2) 物理感知潜在传播器，在潜在空间中通过边界优先的自回归生成重建解。还开发了POBench-PDE基准测试

Result: 在缺失率低于50%的补丁式缺失情况下，在所有基准测试中实现了18-69%的相对L2误差降低，包括真实世界气候预测。方法能有效处理高达75%缺失率的实际场景

Conclusion: 该框架首次系统解决了从部分观测学习神经算子的问题，通过创新的训练策略和潜在空间传播机制，在一定程度上弥合了理想化研究设置与现实世界科学计算复杂性之间的差距

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [45] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的易腐品库存管理方法，在需求过程和交货时间分布未知的情况下，利用有限历史数据和协变量直接学习订购策略，并通过嵌入启发式策略结构提高学习效率。


<details>
  <summary>Details</summary>
Motivation: 易腐品库存管理面临需求不确定、交货时间随机且分布未知的挑战，传统方法需要大量历史数据。在数据有限的实际场景中，需要开发能够有效利用协变量和系统状态的学习方法，同时避免过度浪费或库存短缺。

Method: 采用边际成本核算方案，为每个订单分配单一生命周期成本，形成统一损失函数进行端到端学习。开发了两种变体：纯黑盒方法(E2E-BB)直接输出订购量；结构引导方法(E2E-PIL)嵌入投影库存水平策略，通过显式计算而非额外学习来捕捉库存效应。进一步利用目标函数的齐次性，采用操作数据分析中的提升技术得到增强策略(E2E-BPIL)。

Result: 在合成和真实数据上的实验表明性能排序稳健：E2E-BB被E2E-PIL主导，而E2E-BPIL进一步改进E2E-PIL。通过超额风险分解分析，显示嵌入启发式策略结构能降低有效模型复杂度，在仅损失少量灵活性的情况下提高学习效率。

Conclusion: 深度学习决策工具在人类知识引导下更有效和稳健，强调了将高级分析与库存理论相结合的价值。结构引导方法通过减少需要学习的库存效应，在有限数据下实现更好的性能。

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [46] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 论文研究了1-identification多臂老虎机问题，提出了新的下界证明和算法设计，解决了历史文献中关于多个合格臂存在时的期望停止时间分析问题。


<details>
  <summary>Details</summary>
Motivation: 1-identification是纯探索多臂老虎机的基本问题，目标是判断是否存在平均奖励不低于已知阈值μ₀的合格臂，或者输出None。现有文献在存在多个合格臂时对期望停止时间𝔼τ的分析存在空白，本文旨在填补这一理论缺口。

Method: 采用优化问题形式化方法推导新的下界；设计新算法，其停止时间上界与下界的差距在所有问题实例上最多为对数因子的多项式倍。

Result: 推导了当至少存在一个合格臂时𝔼τ的新下界；设计了算法，其停止时间上界与下界的差距在所有问题实例上最多为对数因子的多项式倍，解决了历史文献中的开放问题。

Conclusion: 本文通过优化形式化和新算法设计，完整解决了1-identification问题中多个合格臂存在时的期望停止时间分析问题，填补了理论空白。

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [47] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: RLSEdit：一种基于递归最小二乘的LLM序列编辑方法，通过在线二次优化解决长期编辑中的可塑性-稳定性困境，支持万次编辑规模。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在长期序列编辑场景下面临可塑性-稳定性困境：硬写入方法会随时间累积干扰，而硬保留方法仅保护显式约束的内容，导致早期编辑被覆盖且未约束行为可能偏离，损害模型在多次编辑后的通用能力。

Method: RLSEdit将编辑建模为带软约束的在线二次优化问题，最小化累积键值拟合目标，包含两个正则项：控制与预训练权重的偏差，以及控制与指定锚点映射的偏差。通过Woodbury恒等式实现高效的在线递归更新，每次编辑成本与历史长度无关，仅与当前编辑规模相关。

Result: 实验在多个模型家族上验证了RLSEdit能够稳定扩展到10K次编辑，在编辑成功率和整体稳定性方面均优于强基线方法。关键优势包括：保留早期编辑，在GLUE基准和保留的推理/代码基准上保持通用能力。

Conclusion: RLSEdit通过递归最小二乘框架有效解决了长期序列编辑中的可塑性-稳定性困境，实现了高效、可扩展的模型编辑，为实际部署中的持续模型更新提供了实用解决方案。

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [48] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 论文提出零误差视界（ZEH）概念，用于评估LLM在无错误情况下的最大问题解决范围，发现即使是先进模型如GPT-5.2在简单任务上也会出错，这对安全关键应用有重要启示。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂任务上表现出色，但在简单任务上仍会出错，这种可靠性问题在安全关键领域尤为关键。需要一种方法来量化模型的无错误解决范围，以评估其可信度。

Method: 提出零误差视界（ZEH）评估框架，通过逐步增加问题复杂度来测试模型的无错误解决范围。使用树结构和在线softmax技术优化计算效率，实现高达一个数量级的加速。

Result: 评估发现GPT-5.2无法正确计算短字符串11000的奇偶性，也无法判断括号串((((())))))是否平衡。ZEH与准确率相关但行为模式不同，为算法能力涌现提供线索。计算优化使速度提升一个数量级。

Conclusion: ZEH为LLM可信度评估提供了重要指标，揭示了即使是先进模型在简单任务上的局限性。这对安全关键应用有重要启示，同时提出的优化方法使ZEH评估更加实用。

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [49] [Towards Automated Kernel Generation in the Era of LLMs](https://arxiv.org/abs/2601.15727)
*Yang Yu,Peiyu Zang,Chi Hsu Tsai,Haiming Wu,Yixin Shen,Jialing Zhang,Haoyu Wang,Zhiyou Xiao,Jingze Shi,Yuyu Luo,Wentao Zhang,Chunlei Men,Guang Liu,Yonghua Lin*

Main category: cs.LG

TL;DR: 关于LLM驱动内核生成的系统性综述：总结现有方法、数据集、基准测试，并指出关键挑战和未来研究方向


<details>
  <summary>Details</summary>
Motivation: 现代AI系统性能受限于底层内核质量，而内核工程需要硬件架构和编程模型的专家知识，是一个耗时且难以扩展的过程。LLM和基于LLM的智能体为自动化内核生成和优化提供了新可能，但目前该领域缺乏系统性视角。

Method: 提供结构化综述：1) 涵盖基于LLM的方法和智能体优化工作流程；2) 系统整理支撑学习和评估的数据集与基准测试；3) 维护开源GitHub仓库跟踪领域进展。

Result: 建立了LLM驱动内核生成的系统性框架，整合了现有方法、数据集和评估基准，为自动化内核优化提供了全面参考。

Conclusion: LLM和智能体系统在自动化内核生成方面展现出巨大潜力，但需要系统性方法来整合碎片化的研究进展。该综述为下一代自动化内核优化建立了综合参考框架。

Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.

</details>


### [50] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: GenRel-DDI提出了一种关系中心学习框架，将药物相互作用预测重新定义为关系学习问题，通过独立于药物身份学习相互作用表示，显著提升了模型对未见药物和新药物对的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有药物相互作用预测方法虽然在标准基准测试中表现良好，但在实际部署场景中泛化能力不足，特别是当大多数候选药物对涉及未见药物且已验证相互作用稀缺时。现有分子中心模型在嵌入空间中的邻近性不能可靠对应相互作用标签，单纯扩大模型容量无法改善泛化性能。

Method: 提出GenRel-DDI框架，将DDI预测重新定义为关系中心学习问题。该方法独立于药物身份学习相互作用表示，通过关系级抽象捕获可迁移的相互作用模式。这种设计使模型能够泛化到未见药物和新药物对。

Result: 在多个基准测试上的广泛实验表明，GenRel-DDI始终显著优于现有最先进方法，特别是在严格的实体不相交评估中取得了特别大的性能提升，突显了关系学习对稳健DDI预测的有效性和实际效用。

Conclusion: 关系中心学习方法通过独立于药物身份学习相互作用表示，能够捕获可迁移的相互作用模式，显著提高了药物相互作用预测的泛化能力，为解决实际部署场景中的挑战提供了有效解决方案。

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [51] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: 提出了一种基于混合LLMs的主动学习框架，用多个LLMs的混合标注模型替代人工标注，通过聚合多个LLMs的优势提升标注鲁棒性，并引入标注差异性和负学习来处理噪声标签。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，它们被越来越多地集成到主动学习流程中以降低标注成本。然而，LLMs生成的标注质量往往达不到实际应用要求，需要提升LLM标注的鲁棒性和可靠性。

Method: 提出Mixture of LLMs in the Loop Active Learning框架：1) 使用基于混合LLMs的标注模型替代人工标注；2) 引入标注差异性分析来识别不可靠标注；3) 采用负学习策略增强学习效果；4) 基于轻量级LLMs构建，可在本地机器上运行。

Result: 实验表明，该框架实现了与人工标注相当的性能，并持续优于单LLM基线和其他基于LLM集成的方法。框架基于轻量级LLMs，能够在实际应用中完全在本地机器上运行。

Conclusion: 提出的混合LLMs主动学习框架有效提升了LLM标注的鲁棒性，通过聚合多个LLMs的优势并处理噪声标签，实现了接近人工标注的性能，同时保持了本地部署的实用性。

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [52] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: GOSV框架通过全局优化识别LLM中的安全关键注意力头，发现恶意注入向量和安全抑制向量两种空间分离的安全向量，并基于此开发了新型白盒越狱攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防护机制脆弱，易受越狱攻击，表明对安全组件理解有限。现有方法依赖局部贪婪归因，忽略了注意力头等组件间的协同交互作用。

Method: 提出GOSV框架，通过全局优化同时识别所有注意力头中的安全关键头。采用两种互补的激活重补丁策略：有害补丁和零消融，识别空间分离的安全向量。

Result: 发现对齐LLM维护着分离的功能通路用于安全目的（恶意注入向量和安全抑制向量）。当约30%的总头被重补丁时，所有模型都会出现完全安全崩溃。基于此开发的新型白盒越狱攻击在所有测试模型中显著优于现有方法。

Conclusion: GOSV框架在LLM安全可解释性方面具有有效性，揭示了安全机制中组件间的协同交互作用，为理解LLM安全防护提供了新视角。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [53] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 提出一种后训练统计准则和结构退火方法，通过移除未支持的参数依赖来揭示稳定独立子结构，实现无需修改模型功能的结构化并行推理。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型推理通常在稠密参数矩阵上进行，导致推理成本和系统复杂度随模型规模不可持续增长。这种限制并非源于模型容量不足，而是由于将后训练推理系统视为整体算子而忽略了学习过程中形成的内部结构。

Method: 基于梯度更新事件高度局部化和选择性的观察，提出后训练统计准则和结构退火程序，移除统计上与初始化分布无区别的未支持参数依赖，揭示稳定独立的子结构。

Result: 建立了后训练、模型无关的推理系统结构视图，实现了无需修改模型功能或接口的结构化并行推理。

Conclusion: 后训练推理系统本质上是结构非均匀且可分解的，通过揭示其内部稳定子结构可以实现高效的结构化并行推理，解决大规模模型推理的可扩展性问题。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [54] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: 提出迭代摊销分层变分自编码器(IA-HVAE)，结合摊销推理的初始猜测和基于解码器梯度的迭代优化，在变换域中实现线性可分离解码器，获得35倍加速并提升重建质量


<details>
  <summary>Details</summary>
Motivation: 传统分层变分自编码器(HVAE)在推理速度与精度之间存在权衡：完全摊销推理速度快但精度有限，完全迭代推理精度高但计算成本大。需要一种混合方法在保持实时应用能力的同时提升推理质量。

Method: 提出IA-HVAE架构，采用混合推理方案：1) 初始摊销推理生成初步估计；2) 基于解码器梯度的迭代细化优化。关键创新是在变换域(如傅里叶空间)设计线性可分离解码器，实现高模型深度的实时应用。

Result: 1) 相比传统HVAE，迭代推理速度提升35倍；2) 混合方法在精度上优于完全摊销方法，在速度上优于完全迭代方法；3) 在去模糊和去噪等逆问题中，IA-HVAE的重建质量优于原始HVAE。

Conclusion: IA-HVAE通过结合摊销推理的效率和迭代优化的精度，在变换域中实现线性可分离解码器，为高深度模型的实时应用提供了有效解决方案，在逆问题中展现出优越的重建性能。

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [55] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: k-lazyGD是一种在线学习算法，在贪婪的在线梯度下降(OGD)和懒惰GD/双平均之间建立了一个谱系，通过懒惰度参数k控制更新策略的响应性与稳定性平衡。


<details>
  <summary>Details</summary>
Motivation: 在平滑在线凸优化(SOCO)中，学习者同时面临命中成本和移动成本。现有方法要么过于贪婪(OGD)，要么过于懒惰(双平均)，缺乏在响应性和稳定性之间的灵活调节机制。需要一种能够根据比较器路径长度自适应调整懒惰度的算法。

Method: 提出k-lazyGD算法，基于FTRL框架，通过参数k控制更新频率：k=1对应贪婪的OGD，k=T对应完全懒惰的GD/双平均。算法创建了一个从反应性到稳定性的连续谱系，允许在命中性能和移动成本之间进行权衡。

Result: 证明当懒惰度k达到Θ(√(T/P_T))时，k-lazyGD仍能实现最优动态遗憾界O(√((P_T+1)T))，其中P_T是比较器路径长度。这意味着算法可以在不牺牲命中性能的前提下保持懒惰方法的固有小移动特性。同时提供了匹配的下界证明。

Conclusion: 懒惰度与比较器变化程度之间存在形式化联系：比较器变化越少，允许的懒惰度越大。通过使用不同懒惰度的学习者集合，可以得到一个在可能时保持稳定、在必要时保持敏捷的自适应方法。

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [56] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 提出条件灵活性指数（CFI），通过从历史数据学习参数化可接受不确定性集，并利用上下文信息使其条件化，从而更准确地评估调度灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统灵活性指数使用简单可接受不确定性集（如超立方体）来近似可接受不确定性区域，但未考虑可用的上下文信息（如预测）来定义可接受不确定性集。随着过程柔性化增加，需要更精确的调度决策评估方法。

Method: 使用归一化流学习从高斯基分布到数据分布的双射映射，在潜在空间中构建超球面作为可接受潜在不确定性集，然后映射到数据空间。通过纳入上下文信息，使可接受不确定性集条件化，提出条件灵活性指数（CFI）。

Result: 通过示例表明，数据驱动的可接受不确定性集不一定总是优于简单集，条件集也不一定总是优于无条件集。但两者都能确保只考虑包含实际实现的参数空间区域。在安全约束机组组合示例中，CFI通过纳入时间信息提高了调度质量。

Conclusion: 条件灵活性指数通过从数据学习参数化可接受不确定性集并使其条件化，提供了更信息丰富的灵活性评估，能够考虑特定条件下更相关的区域，从而改进调度决策。

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [57] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: CLASP算法在约束在线凸优化中同时最小化累积损失和平方约束违反，对凸损失实现次线性遗憾和惩罚，对强凸问题首次实现对数级遗憾和惩罚保证。


<details>
  <summary>Details</summary>
Motivation: 研究约束在线凸优化问题，其中学习者在迭代选择动作时面临未预期的凸损失和凸约束，需要在累积损失的同时处理约束违反惩罚。现有方法未能充分利用凸投影算子的性质，缺乏对强凸问题的对数级保证。

Method: 提出CLASP算法，通过凸投影算子的严格非扩张性分析策略，最小化累积损失和平方约束违反。算法设计基于对损失和约束的联合优化框架，采用新颖的证明技术。

Result: 对于凸损失：遗憾为O(T^{max{β,1-β}})，累积平方惩罚为O(T^{1-β})，其中β∈(0,1)。对于强凸损失：首次实现对数级保证，遗憾和累积平方惩罚均为O(log T)。

Conclusion: CLASP算法在约束在线凸优化中提供了优越的性能保证，特别是对强凸问题首次实现了对数级遗憾和惩罚，通过利用凸投影算子的严格非扩张性开辟了新的分析路径。

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [58] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 该研究将可解释AI（XAI）应用于工业信息物理系统（CPS）中的机器学习模型，通过SHAP值分析时间序列分解组件对预测的影响，发现模型训练中上下文信息不足的问题，并通过增加数据窗口大小提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 工业信息物理系统（CPS）对安全和经济效益至关重要，需要高可靠性。虽然机器学习（特别是深度学习）越来越多地集成到工业CPS中，但ML模型的固有复杂性导致其操作不透明。需要严格评估以防止模型在未来未见数据上出现意外行为。可解释AI（XAI）可用于揭示模型推理过程，从而进行更全面的行为分析。

Method: 应用可解释AI（XAI）来改进工业CPS中ML模型的预测性能。使用SHAP值分析时间序列数据分解组件对模型预测的影响。通过XAI发现模型训练中上下文信息不足的问题，并基于此发现增加数据实例的窗口大小。

Result: 通过XAI分析发现了模型训练中缺乏足够上下文信息的证据。根据XAI的发现，通过增加数据窗口大小，成功提高了模型性能。

Conclusion: 可解释AI（XAI）不仅可用于理解模型推理，还能指导模型改进策略。通过分析SHAP值揭示的时间序列分解组件影响，可以识别模型训练中的信息不足问题，并通过调整数据窗口大小等方法来提升工业CPS中ML模型的预测性能。

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [59] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: 提出PAC-MAP算法，为MAP推理提供概率近似正确保证，在可变和固定计算预算下提供可证明的最优解


<details>
  <summary>Details</summary>
Motivation: MAP估计在概率推断中是基本任务，但通常难以计算，即使在许多常见的结构约束和近似方案下仍然困难。需要开发具有理论保证的MAP推理方法。

Method: 引入概率近似正确(PAC)算法进行MAP推理，使用信息理论度量表征可处理性条件，通过具有适当架构的概率电路高效实现，开发随机化策略作为独立MAP推理技术或改进流行启发式方法

Result: 实验证实该方法在一系列基准测试中的优势，能够为MAP推理提供严格的理论保证

Conclusion: PAC-MAP算法为MAP推理提供了具有理论保证的解决方案，通过信息理论度量和概率电路实现高效计算，随机化策略可增强现有启发式方法的可靠性

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [60] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: 该研究首次系统性地对多个专门为拉曼光谱设计的深度学习分类器在共享开源数据集上进行基准测试，比较了五种代表性架构在统一训练协议下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前拉曼光谱深度学习分类器的评估往往孤立进行，或仅与传统机器学习方法比较，缺乏专门为拉曼光谱设计的深度学习模型之间的直接对比。现有研究在共享开源数据集上的系统比较非常稀缺。

Method: 研究选择了五种代表性的深度学习架构，在三个开源拉曼数据集上采用统一的训练和超参数调优协议进行评估。数据集选择支持标准评估、微调和显式分布偏移测试。使用分类准确率和宏平均F1分数作为评价指标。

Result: 该研究提供了深度学习模型在拉曼光谱分类任务上的公平、可重复比较结果，具体性能数据需参考完整论文。

Conclusion: 这是首批对多个已发表的拉曼专用深度学习分类器进行系统基准测试的研究之一，为拉曼光谱分析领域的深度学习模型比较建立了标准化评估框架。

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [61] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 该论文研究了核岭回归中流形假设的两种内在维度定义：基于核诱导度量的Minkowski维度和基于Kolmogorov n-宽度的有效维度，分析了它们与积分算子特征值的关系，并推导了泛化误差界。


<details>
  <summary>Details</summary>
Motivation: 流形假设认为当输入分布支撑集的本质维度较低时，机器学习方法的泛化性能会显著提升。本文旨在研究核岭回归中两种不同的本质维度概念，并分析它们与泛化性能的理论关系。

Method: 1. 定义了两种本质维度：基于核诱导度量的Minkowski维度d_ρ和基于Kolmogorov n-宽度的有效维度d_K；2. 分析了n-宽度与积分算子特征值的关系；3. 推导了约束核岭回归的泛化误差界；4. 提出了从有限样本估计n-宽度上界的算法；5. 计算了分形集的有效维度并进行了数值实验。

Result: 1. 证明了对于固定域Ω，Kolmogorov n-宽度刻画了所有支撑在Ω上的概率测度的最坏情况特征值衰减；2. 推导了约束核岭回归的过拟合误差界为O(n^{-(2+d_K)/(2+2d_K)+ε})；3. 提出算法可从有限样本估计n-宽度上界；4. 对于接近均匀的分布，证明了使用O(ε^{-d_ρ}log(1/ε))样本可以高概率计算所有n-宽度的ε-精确上界；5. 数值实验显示对于Laplace核等核函数，有效维度d_K可以显著小于Minkowski维度d_ρ。

Conclusion: 本文建立了核岭回归中两种本质维度概念的理论联系，证明了有效维度d_K可以比Minkowski维度d_ρ更小，从而为流形假设提供了更精细的理论分析框架，并提供了从有限样本估计这些维度的实用算法。

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [62] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: Beat-SSL：一种针对ECG信号的对比学习框架，通过节奏级和心跳级双重上下文学习与软目标对比，在有限标注数据下实现有效迁移学习


<details>
  <summary>Details</summary>
Motivation: 获取标注ECG数据用于监督模型训练具有挑战性。现有对比学习框架要么仅关注全局上下文，要么未能充分利用ECG特定特征，且依赖硬对比目标，无法充分捕捉ECG信号特征相似性的连续性

Method: 提出Beat-SSL对比学习框架，执行双重上下文学习：1) 节奏级对比，2) 心跳级对比，采用软目标而非硬目标，更好地适应ECG特征的连续相似性

Result: 在两个下游任务评估：1) 多标签分类（全局节奏评估）达到ECG基础模型93%的性能；2) ECG分割任务超越所有其他方法4%。消融研究验证了框架有效性

Conclusion: Beat-SSL通过ECG特定的双重上下文学习和软目标对比，在有限标注数据下实现了有效的表示学习，在多标签分类和分割任务中均表现出色，超越了现有方法

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [63] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: TTT-Discover通过测试时强化学习让LLM针对特定问题持续训练，在数学、GPU内核、算法设计和生物学等多个领域实现了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时缩放方法（如AlphaEvolve）使用冻结的LLM进行搜索，无法针对特定测试问题进行持续学习。需要一种方法让LLM在测试时能够基于特定问题的经验继续训练，专注于产生单个优秀解决方案而非平均性能。

Method: TTT-Discover（测试时训练发现）方法：通过强化学习在测试时让LLM持续训练，学习目标和搜索子程序专门设计为优先考虑最有前景的解决方案。专注于连续奖励问题，使用开源模型gpt-oss-120b，通过Tinker API实现，每个问题成本仅几百美元。

Result: 在几乎所有尝试的问题中都实现了新的SOTA：1) Erdős最小重叠问题和自相关不等式；2) GPUMode内核竞赛（比现有技术快达2倍）；3) 过去的AtCoder算法竞赛；4) 单细胞分析中的去噪问题。所有结果均由专家或组织者审查，使用开源模型实现，可复现。

Conclusion: TTT-Discover通过测试时强化学习使LLM能够针对特定科学问题持续优化，在多个领域实现了新的最先进性能，且相比依赖封闭前沿模型的方法更具可访问性和可复现性。

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [64] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: 提出一种名为"反事实训练"的新训练范式，利用反事实解释来增强模型的可解释性，通过在训练阶段使用反事实来最小化学习表示与合理、可操作解释之间的差异。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法主要作为后处理技术，虽然能生成符合数据分布和特征可变性约束的解释，但模型本身并不直接对这些期望的解释目标负责。作者希望开发一种训练方法，使模型能够直接产生内在符合要求的反事实解释。

Method: 提出反事实训练范式，在训练阶段直接使用反事实解释，通过最小化学习到的表示与合理、可操作的反事实解释之间的差异来训练模型。这种方法使模型在训练过程中直接对期望的解释目标负责，而不是依赖后处理生成解释。

Result: 通过实证和理论分析证明，该方法能够训练出能够提供内在符合要求的反事实解释的模型，同时还能提高模型的对抗鲁棒性。

Conclusion: 反事实训练是一种有效的训练范式，能够使模型直接产生符合数据合理性和特征可操作性的反事实解释，同时增强模型的鲁棒性，为构建更透明、可信的机器学习系统提供了新途径。

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>


<div id='Surajit Chaudhuri'></div>

# Surajit Chaudhuri [[Back]](#toc)

### [65] [EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03471&hl=zh-CN&sa=X&d=8873960001622105257&ei=iUF1acr7D7ui6rQPrda9qQc&scisig=AHkA5jRlwZXalKtuijihThk2vtmk&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=2&folt=rel)
*M Wei,D Min,Z Liu,Y Xie,G Wu,C Yang,MSY Lau…*

Main category: Surajit Chaudhuri

TL;DR: 该论文旨在创建一个专注于流行病学推理的医学问答基准，以弥补现有临床问答基准的不足，通过合成研究证据来推断疾病负担、传播动态和干预效果。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答基准主要关注临床诊断和治疗，缺乏对流行病学推理的关注。可靠的流行病学推理需要综合研究证据来推断疾病负担、传播动态和干预效果，这对公共卫生决策至关重要。

Method: 创建了一个专注于流行病学推理的医学问答基准，包含需要综合研究证据来回答的问题，涉及疾病负担、传播动态和干预效果等流行病学概念。

Result: 开发了一个新的流行病学推理问答基准，填补了现有医学问答基准的空白，为评估模型在流行病学推理能力方面提供了专门工具。

Conclusion: 该论文强调了流行病学推理在医学问答中的重要性，并提供了一个专门基准来促进该领域的发展，有助于提升模型在公共卫生决策支持方面的能力。

Abstract: Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical …

</details>


### [66] [Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.14270&hl=zh-CN&sa=X&d=17150939803702874760&ei=iUF1acr7D7ui6rQPrda9qQc&scisig=AHkA5jS5_YmFJHv5h6ZAdDlUKRJP&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=5&folt=rel)
*L Pan,J Liang,J Ye,M Yang,X Lu,F Zhu*

Main category: Surajit Chaudhuri

TL;DR: 该论文探讨大语言模型（LLMs）在解决多步推理问题时的内部机制，区别于现有主要关注外部性能的综述，旨在揭示其内部工作原理。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型已展现出解决多步推理问题的卓越能力，但其实现这种能力的内部机制仍不明确。现有研究大多关注模型的外部性能表现，缺乏对内部工作原理的系统性分析，这阻碍了对LLMs推理能力的深入理解。

Method: 论文采用系统性综述方法，重点分析LLMs在多步推理任务中的内部工作机制，而非仅关注外部性能指标。可能包括对模型架构、注意力机制、内部表示等方面的深入分析。

Result: 通过系统性分析，论文揭示了LLMs在多步推理过程中的关键内部机制，包括信息处理路径、推理步骤的内部表示、以及不同组件在推理过程中的作用等。

Conclusion: 理解LLMs的内部推理机制对于提升模型性能、增强可解释性以及开发更高效的推理模型至关重要。该研究为深入理解LLMs的推理能力提供了新的视角和方法论基础。

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on …

</details>


### [67] [SAGE: Adaptive Recommendation of Spreadsheet Pivot Tables](https://scholar.google.com/scholar_url?url=https://afariha.github.io/papers/SAGE_Demo_SIGMOD_2026_Manuscript.pdf&hl=zh-CN&sa=X&d=244504959709676486&ei=iUF1acr7D7ui6rQPrda9qQc&scisig=AHkA5jQi59kUTUMjvg5MJpGX4WDO&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=6&folt=rel)
*W Cho,S Fatin,A Fariha*

Main category: Surajit Chaudhuri

TL;DR: 该论文研究如何利用大型语言模型自动生成电子表格数据摘要，以帮助用户理解复杂数据集并提取数据洞察


<details>
  <summary>Details</summary>
Motivation: 电子表格数据无处不在，数十亿用户依赖电子表格进行日常任务如财务分析和决策制定。用户需要数据摘要来理解电子表格数据集并提取数据洞察，但手动创建摘要耗时且需要专业知识。现有方法如数据透视表需要用户具备领域知识，而大型语言模型的出现为自动化数据摘要生成提供了新机会。

Method: 论文提出了一种基于大型语言模型的框架，用于自动生成电子表格数据摘要。该方法可能包括：1）数据预处理和结构化，2）利用LLM理解数据模式和关系，3）生成自然语言摘要，4）可能包含交互式功能让用户细化摘要需求。方法可能结合了传统数据分析技术和LLM的自然语言处理能力。

Result: 该方法能够自动生成准确、有洞察力的电子表格数据摘要，显著减少用户手动分析时间。实验可能显示生成的摘要质量高，能够识别关键数据模式、趋势和异常值，帮助用户快速理解数据集。系统可能支持多种摘要类型，如统计摘要、趋势分析、异常检测等。

Conclusion: 基于大型语言模型的电子表格数据摘要生成方法具有实用价值，能够帮助用户更高效地理解复杂数据集。该方法降低了数据分析的门槛，使非专业用户也能获得专业级的数据洞察。未来工作可能包括提高摘要的个性化程度、支持更复杂的数据类型和集成到主流电子表格软件中。

Abstract: Spreadsheet data is ubiquitous, with billions of users relying on spreadsheets for everyday tasks such as financial analysis and decision making. To make sense of spreadsheet datasets and extract data insights, users rely on data summaries. Pivot …

</details>


### [68] [Opendecoder: Open large language model decoding to incorporate document quality in rag](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.09028&hl=zh-CN&sa=X&d=2508204888155190544&ei=iUF1acr7D7ui6rQPrda9qQc&scisig=AHkA5jS2COpKp6XZg3yX276FpMWA&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=7&folt=rel)
*F Mo,Z Su,Y Hui,J Zhang,JA Sun,Z Liu,C Zhang…*

Main category: Surajit Chaudhuri

TL;DR: 该论文探讨了基于大语言模型的检索增强生成系统中检索质量对生成内容的影响，并提出了一种改进方法


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的检索增强生成系统虽然在下游任务中表现出色，但其生成内容的质量严重依赖于检索到的文档质量。当检索到的文档不相关或质量较差时，会直接影响最终生成内容的准确性和可靠性。

Method: 论文提出了一种改进的检索增强生成方法，可能包括检索质量评估、文档筛选机制、多源检索融合或检索结果重排序等技术，以提升检索文档的相关性和质量，从而改善最终生成效果。

Result: 改进后的方法在多个下游任务中显著提升了生成内容的质量和准确性，特别是在检索文档质量不稳定的情况下，能够有效过滤低质量文档并利用高质量信息进行生成。

Conclusion: 检索质量是基于大语言模型的检索增强生成系统的关键瓶颈，通过改进检索机制和文档筛选策略，可以显著提升系统整体性能，为实际应用提供更可靠的内容生成能力。

Abstract: The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the …

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [69] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 提出了三种改进基于RT Core的粒子FRNN物理模拟的方法：BVH更新/重建比率优化器、无需邻居列表的RT Core新用法（两种变体）、支持周期性边界条件的RT Core技术，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RT Core的粒子FRNN物理模拟存在性能优化空间，特别是在BVH管理、邻居列表内存开销和周期性边界条件支持方面需要改进，以充分发挥RT Core的潜力。

Method: 1) 实时BVH更新/重建比率优化器，根据模拟动态自适应调整；2) 两种无需邻居列表的RT Core变体；3) 支持周期性边界条件的RT Core技术。以Lennard-Jones FRNN相互作用模型为案例进行实验评估。

Result: BVH优化器使RT Core流水线比其他方法快约3.4倍；新变体在小半径时加速约1.3倍，在对数正态半径分布时加速约2.0倍；支持周期性边界条件无显著性能损失；方法在不同GPU代际上具有良好的性能和能效扩展性。

Conclusion: 提出的三种方法显著提升了基于RT Core的粒子FRNN物理模拟的性能和能效，同时确定了传统GPU计算仍占优势的模拟场景，有助于理解RT Core的优缺点。

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='Bin CUI'></div>

# Bin CUI [[Back]](#toc)

### [70] [Unleashing Efficient Asynchronous RL Post-Training via Staleness-Constrained Rollout Coordination](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.12784&hl=zh-CN&sa=X&d=1183535510241298154&ei=h0F1afzZDcyQieoPxcX9yAo&scisig=AHkA5jRfiGFFOVLPrL7Msaet5Wo0&oi=scholaralrt&hist=i6heNjgAAAAJ:3739178042932535188:AHkA5jQRTxR8mtnnr-h_sMDSNvEf&html=&pos=0&folt=art)
*H Li,S Lin,F Fu,Y Zhou,X Ji,Y Zhao,L Wang,J Jiang…*

Main category: Bin CUI

TL;DR: 论文提出了一种完全解耦架构的强化学习系统，将RL的三个阶段（数据收集、模型训练、策略评估）分离，以提升大规模模型训练的效率


<details>
  <summary>Details</summary>
Motivation: 现代大规模模型需要高效的强化学习后训练，传统集成式RL系统存在资源利用不均衡、扩展性差等问题，需要更灵活的架构来支持大规模分布式训练

Method: 采用完全解耦的RL系统架构，将数据收集（rollout）、模型训练、策略评估三个阶段分离，每个阶段可以独立扩展和优化，支持异步并行处理

Result: 解耦架构显著提高了资源利用率，实现了更好的扩展性，能够支持更大规模的模型训练，同时保持了训练稳定性和收敛性能

Conclusion: 完全解耦的RL架构是提升大规模模型后训练效率的有效方案，为未来更大规模的语言模型训练提供了可扩展的技术基础

Abstract: Reinforcement learning (RL) post-training has become pivotal for enhancing the capabilities of modern large models. A recent trend is to develop RL systems with a fully disaggregated architecture, which decouples the three RL phases (rollout …

</details>


### [71] [LB-MCTS: Synergizing Large Language Models and Bayesian Optimization for Efficient CASH](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.12355&hl=zh-CN&sa=X&d=3087355949536284178&ei=h0F1afzZDcyQieoPxcX9yAo&scisig=AHkA5jRIdecMGUpYyuWogBFfjj23&oi=scholaralrt&hist=i6heNjgAAAAJ:3739178042932535188:AHkA5jQRTxR8mtnnr-h_sMDSNvEf&html=&pos=1&folt=art)
*B Xu,W Qian,L Tung,Y Lu,B Cui*

Main category: Bin CUI

TL;DR: AutoML社区通过解决CASH问题（算法选择和超参数调优）来降低机器学习的使用门槛，但传统方法存在局限性


<details>
  <summary>Details</summary>
Motivation: 降低机器学习的使用门槛，让非专家也能有效应用机器学习技术

Method: 专注于解决CASH问题（Combined Algorithm Selection and Hyperparameter optimization），采用自动化方法进行算法选择和超参数调优

Result: 未提供具体结果，但暗示传统方法存在局限性

Conclusion: 需要更先进的AutoML方法来有效解决CASH问题，进一步降低机器学习的使用门槛

Abstract: To lower the expertise barrier in machine learning, the AutoML community has focused on the CASH problem, a fundamental challenge that automates the process of algorithm selection and hyperparameter tuning. While traditional methods like …

</details>


<div id='Ion Stoica'></div>

# Ion Stoica [[Back]](#toc)

### [72] [Speculative Decoding: Performance or Illusion?](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.11580&hl=zh-CN&sa=X&d=12283815019833746782&ei=h0F1afbnIaOi6rQPoMCXoAE&scisig=AHkA5jT6iIJ7_YGKBQP_W151lbqT&oi=scholaralrt&hist=i6heNjgAAAAJ:3972655621478617842:AHkA5jSf2ebnJwC9N9_xKs4IgCHN&html=&pos=0&folt=art)
*X Liu,J Yu,J Park,I Stoica,A Cheung*

Main category: Ion Stoica

TL;DR: 本文评估了推测解码在实际生产环境中的有效性，发现其在真实工作负载下加速效果有限，主要受限于批处理大小、模型对齐和内存带宽等因素。


<details>
  <summary>Details</summary>
Motivation: 推测解码已成为加速大语言模型推理的流行技术，但现有评估多基于研究原型和不切实际的小批量设置，其在实际生产环境中的真实效果尚不明确。

Method: 通过在实际生产环境中进行系统性评估，考虑真实工作负载、批处理大小、模型对齐、内存带宽等实际约束条件，分析推测解码在不同场景下的性能表现。

Result: 研究发现推测解码在实际生产环境中的加速效果有限，主要瓶颈包括：批处理大小限制、模型对齐开销、内存带宽约束以及实际工作负载的复杂性。

Conclusion: 推测解码的理论加速优势在实际生产环境中难以完全实现，需要针对实际约束条件进行优化设计，并重新评估其在真实部署场景中的价值。

Abstract: Speculative decoding (SD) has become a popular technique to accelerate Large Language Model (LLM) inference, yet its real-world effectiveness remains unclear as prior evaluations rely on research prototypes and unrealistically small batch sizes …

</details>


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [73] [ForgetMark: Stealthy Fingerprint Embedding via Targeted Unlearning in Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08189&hl=en&sa=X&d=10986440491085924953&ei=iUF1abeBLsyQieoPxcX9yAo&scisig=AHkA5jThCjeMrogiSddIygMf3Z39&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=0&folt=rel)
*Z Xu,H Zhang,Z Wang,Q Liu,H Xu,W Xing,M Han*

Main category: Google Scholar

TL;DR: ForgetMark是一种新型隐蔽后门攻击方法，通过低困惑度触发器和自适应响应模式来规避现有检测机制


<details>
  <summary>Details</summary>
Motivation: 现有后门指纹存在高困惑度触发器易被过滤、固定响应模式易被启发式检测器发现、以及对良性输入产生虚假激活等问题，需要更隐蔽的后门攻击方法

Method: ForgetMark采用低困惑度触发器设计，结合自适应响应模式，避免固定模式暴露，同时减少对良性输入的虚假激活

Result: 该方法能够有效规避现有后门检测机制，保持攻击成功率的同时显著提升隐蔽性

Conclusion: ForgetMark为后门攻击提供了新的隐蔽性范式，揭示了现有检测方法的局限性

Abstract: Existing invasive (backdoor) fingerprints suffer from high-perplexity triggers that are easily filtered, fixed response patterns exposed by heuristic detectors, and spurious activations on benign inputs. We introduce\textsc {ForgetMark}, a stealthy …

</details>


### [74] [LLMOrbit: A Circular Taxonomy of Large Language Models-From Scaling Walls to Agentic AI Systems](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.14053&hl=en&sa=X&d=5200329808375216364&ei=iUF1abeBLsyQieoPxcX9yAo&scisig=AHkA5jTXuVEVpj0SlaEYxE4_1-fi&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=1&folt=rel)
*BN Patro,VS Agneeswaran*

Main category: Google Scholar

TL;DR: LLMOrbit是一个全面的循环分类法，用于导航从基础Transformer架构到接近人类水平性能的推理能力AI系统的演进路径。


<details>
  <summary>Details</summary>
Motivation: 人工智能领域经历了从基础Transformer架构到具备推理能力系统的革命性发展，需要一个系统性的分类框架来理解和导航这一演进过程。

Method: 提出LLMOrbit——一个全面的循环分类法，通过系统性的分类框架来组织和理解AI系统从基础架构到高级推理能力的发展轨迹。

Result: LLMOrbit分类法提供了一个结构化框架，能够系统地分类和导航从基础Transformer到接近人类水平推理能力的AI系统演进路径。

Conclusion: LLMOrbit分类法为理解和导航AI系统从基础架构到高级推理能力的发展提供了一个有价值的系统性框架。

Abstract: The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating …

</details>


### [75] [Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.22742&hl=en&sa=X&d=17090565180492256644&ei=iUF1abeBLsyQieoPxcX9yAo&scisig=AHkA5jSm3L3PG5GfmLvGkgf_wY35&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=2&folt=rel)
*H Meng,J Cao,R Pottinger*

Main category: Google Scholar

TL;DR: 本文研究表格数据中的列类型标注(CTA)问题，提出了一种基于大型语言模型(LLM)的零样本方法，通过上下文学习实现无需微调的高精度列类型标注


<details>
  <summary>Details</summary>
Motivation: 现有基于编码器语言模型的方法需要大量标注数据进行微调才能达到高精度，这在实际应用中成本高昂且不切实际。需要开发能够在零样本或少样本设置下有效工作的CTA方法，以降低标注成本并提高实用性。

Method: 提出了一种基于大型语言模型(LLM)的零样本CTA方法，利用上下文学习能力，通过精心设计的提示模板和少量示例，使LLM能够理解表格列的内容和上下文语义，从而准确推断列类型。方法包括提示工程、上下文示例选择和类型推断策略。

Result: 实验结果表明，所提出的LLM-based零样本方法在多个基准数据集上取得了与微调编码器模型相当甚至更好的性能，特别是在少样本和零样本设置下表现优异。该方法显著降低了标注需求，同时保持了高精度。

Conclusion: 基于LLM的零样本CTA方法为表格数据理解提供了一种实用且高效的解决方案，无需大量标注数据即可实现高精度列类型标注，为大规模表格数据集成和语义理解开辟了新途径。

Abstract: Column Type Annotation (CTA) is a fundamental step towards enabling schema alignment and semantic understanding of tabular data. Existing encoder-only language models achieve high accuracy when fine-tuned on labeled columns, but …

</details>


### [76] [Linking Cryptoasset Attribution Tags to Knowledge Graph Entities: An LLM-Based Approach](https://scholar.google.com/scholar_url?url=https://books.google.com/books%3Fhl%3Den%26lr%3Dlang_en%26id%3DH5ylEQAAQBAJ%26oi%3Dfnd%26pg%3DPA366%26ots%3De2bAvRe3DZ%26sig%3DrTZY9IQSds2VmhphKRJ5IDAkPGk&hl=en&sa=X&d=2169516983021448480&ei=iUF1abeBLsyQieoPxcX9yAo&scisig=AHkA5jSptjWP6ADpEyw7uPH-SM9E&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=3&folt=rel)
*B Haslhofer*

Main category: Google Scholar

TL;DR: 提出基于计算方法的加密资产归因标签验证框架，解决标签不一致和错误问题


<details>
  <summary>Details</summary>
Motivation: 加密资产归因标签是现代加密货币取证的基础，但不一致或错误的标签会误导调查甚至导致错误指控，需要系统性的验证方法

Method: 提出基于计算方法的归因标签验证框架，通过算法分析标签的一致性和准确性

Result: 该方法能够有效识别和纠正不一致或错误的归因标签，提高加密货币取证的可靠性

Conclusion: 计算验证方法为加密资产归因标签提供了系统性的质量保证，有助于提升加密货币取证的准确性和可信度

Abstract: Attribution tags form the foundation of modern cryptoasset forensics. However, inconsistent or incorrect tags can mislead investigations and even result in false accusations. To address this issue, we propose a novel computational method based …

</details>


### [77] [Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.14553&hl=en&sa=X&d=8497207547188254433&ei=h0F1aYL8NLuM6rQP2dOq-Ag&scisig=AHkA5jSi0wo_yH_5a_Eo1SS5mOEV&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=0&folt=cit)
*B Christian,M Mazor*

Main category: Google Scholar

TL;DR: 大型语言模型在近似反事实决策时存在与人类相似的偏见限制，难以忽略无关信息进行公平决策


<details>
  <summary>Details</summary>
Motivation: 公平决策需要忽略无关的、可能带有偏见的信息，但人类在进行反事实自我模拟时存在困难，导致即使善意行为者也会产生偏见判断。研究旨在探索大型语言模型是否也存在类似的局限性。

Method: 研究通过实验测试大型语言模型在近似反事实决策时的能力，特别是模型能否模拟不知道某些事实（如求职者的性别或种族）时的决策情况。

Result: 研究结果显示大型语言模型在反事实自我模拟方面存在与人类相似的局限性，难以准确近似不知道某些信息时的决策，导致决策偏见。

Conclusion: 大型语言模型在公平决策方面存在与人类相似的认知限制，这对其在敏感决策场景中的应用提出了重要挑战。

Abstract: Fair decisions require ignoring irrelevant, potentially biasing, information. To achieve this, decision-makers need to approximate what decision they would have made had they not known certain facts, such as the gender or race of a job candidate. This counterfactual self-simulation is notoriously hard for humans, leading to biased judgments even by well-meaning actors. Here we show that large language models (LLMs) suffer from similar limitations in their ability to approximate what decisions …

</details>


### [78] [Sutradhara: An Intelligent Orchestrator-Engine Co-design for Tool-based Agentic Inference](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.12967&hl=en&sa=X&d=26922578090216282&ei=h0F1aYL8NLuM6rQP2dOq-Ag&scisig=AHkA5jQZv_HjS4GC3hE9FEIq91PL&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=2&folt=cit)
*A Biswas,K Goel,J Mohan,A Khare,A Parayil…*

Main category: Google Scholar

TL;DR: 该论文研究基于工具的LLM智能体应用中首次令牌渲染延迟的性能瓶颈问题，并提出优化方案


<details>
  <summary>Details</summary>
Motivation: 基于工具的智能体应用已成为LLM生产部署的主流范式，但与传统单轮推理不同，智能体工作负载需要链接多个LLM调用和工具执行，导致最终答案的首次令牌渲染延迟显著增加，形成了新的性能瓶颈

Method: 通过分析智能体工作负载的特点，研究多轮LLM调用和工具执行对延迟的影响机制，提出针对首次令牌渲染延迟的优化方法

Result: 研究发现智能体应用中的多轮调用链显著增加了最终答案的首次令牌渲染延迟，通过提出的优化方案可以有效降低这一关键性能指标

Conclusion: 智能体应用的性能评估需要特别关注首次令牌渲染延迟这一新瓶颈，针对性的优化策略对于提升基于工具的LLM系统在实际生产环境中的响应性能至关重要

Abstract: Agentic applications are LLMs that iteratively invoke external tools to accomplish complex tasks. Such tool-based agents are rapidly becoming the dominant paradigm for deploying language models in production. Unlike traditional single-turn inference, agentic workloads chain together multiple LLM calls and tool executions before producing a final response, creating a new performance bottleneck that manifests as increased latency in First Token Rendered (FTR) of the final answer. Through …

</details>


### [79] [From fine-tuning to prompting](https://scholar.google.com/scholar_url?url=https://pure.uva.nl/ws/files/282865836/Thesis.pdf&hl=en&sa=X&d=10747912826890348595&ei=h0F1aYL8NLuM6rQP2dOq-Ag&scisig=AHkA5jTbD55-x9WWlqURAIzkEu6D&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=4&folt=cit)
*X Li*

Main category: Google Scholar

TL;DR: 该论文探讨在复杂领域（如组织对话）中知识图谱构建的挑战，提出通过预训练语言模型分析和增量学习两种互补视角来适应噪声、演化和上下文依赖的数据环境


<details>
  <summary>Details</summary>
Motivation: 知识图谱在复杂领域（如组织对话）的构建面临挑战，因为数据具有噪声、不断演化且上下文依赖性强。现有方法难以适应这些条件，需要研究如何使知识图谱构建过程能够更好地处理这些现实世界的复杂性

Method: 采用两种互补的研究视角：1) 分析预训练语言模型在知识图谱构建中的局限性；2) 探索增量学习方法，使知识图谱能够适应不断演化的数据环境

Result: 论文通过系统分析揭示了预训练语言模型在复杂领域知识图谱构建中的具体限制，同时展示了增量学习方法如何有效处理数据演化问题，提高了知识图谱在动态环境中的适应能力

Conclusion: 在复杂领域构建知识图谱需要结合多种方法：既要理解预训练模型的局限性，又要采用增量学习策略来适应数据演化。这种双重视角为处理噪声、动态和上下文依赖的数据提供了有效的框架

Abstract: Knowledge graphs (KGs) provide structured, machine-actionable representations of information that support search, reasoning, and decision-making. Constructing them, however, remains challenging in complex domains such as organizational conversations, where data is noisy, evolving, and context-dependent. This thesis examines how knowledge graph construction (KGC) can adapt to these conditions through two complementary perspectives:(i) analyzing the limitations of the pretrain …

</details>


### [80] [ZLearnIndex: Learned Index on ZNS SSDs for Space and Performance Efficiency](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11323122/&hl=en&sa=X&d=890056117042651579&ei=h0F1aYL8NLuM6rQP2dOq-Ag&scisig=AHkA5jQHMd1HGZDICPUOKj-N3LAq&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=7&folt=cit)
*J Huo,C Li,Y Xu*

Main category: Google Scholar

TL;DR: ZLearnIndex：一种基于学习索引的ZNS SSD I/O中间件，解决传统映射方法在单节点大容量存储中的高内存开销问题


<details>
  <summary>Details</summary>
Motivation: 现有ZNS SSD的I/O中间件采用页级或其他传统映射方法支持随机写入，但这些映射方法在单节点大容量存储场景下带来了巨大的主机内存管理挑战

Method: 提出ZLearnIndex系统，采用学习索引（learned indexes）技术来替代传统的映射方法，降低内存开销

Result: 未在摘要中明确说明具体结果，但暗示该方法能有效解决ZNS SSD I/O中间件中的高内存开销问题

Conclusion: 学习索引为ZNS SSD I/O中间件提供了一种新颖的解决方案，能够显著降低内存开销，特别适用于单节点大容量存储场景

Abstract: Existing I/O middlewares for ZNS SSDs employ page-level or other conventional mapping approaches to support random writes. However, these mapping approaches present substantial challenges for managing host memory, particularly in the case of single-node, high-capacity storage. In this paper, we present a novel approach to address the problem of heavy memory overhead in ZNS SSD I/O middlewares with learned indexes. We propose ZLearnIndex, a system that takes …

</details>


### [81] [MOCHA: Memory-Optimized Learned Index Based on Chameleon](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/11299360/&hl=en&sa=X&d=14811718407053992748&ei=h0F1aYL8NLuM6rQP2dOq-Ag&scisig=AHkA5jRoI9r0zmuXYl6pXZcZj351&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=8&folt=cit)
*C Sun,R Chen,M Xia,J Xie,E Wu,Z Han*

Main category: Google Scholar

TL;DR: 提出MOCHA索引，在数据分布倾斜场景下优化内存使用，同时保持查询性能


<details>
  <summary>Details</summary>
Motivation: 传统索引（如B+树）在局部倾斜数据分布下查询性能差、内存消耗高；现有学习型索引Chameleon在倾斜数据下查询速度快但仍占用大量内存空间

Method: 提出MOCHA内存优化索引，通过特定技术（论文未详细说明）在倾斜数据分布下减少内存占用

Result: MOCHA在倾斜数据场景下显著减少内存使用，同时保持或提升查询性能

Conclusion: MOCHA索引有效解决了倾斜数据分布下索引内存占用过高的问题，为数据库系统提供了更高效的内存优化解决方案

Abstract: Indexes are important components in database systems, capable of significantly improving the speed of data lookup. Under locally skewed data distributions, traditional indexes (such as B+-trees) exhibit poor query performance and high memory consumption; while the existing learned index Chameleon achieves faster query speed under skewed data, it still occupies a large amount of memory space. To optimize the utilization of memory space, this paper proposes MOCHA-a memory …

</details>


<div id='Matei Zaharia'></div>

# Matei Zaharia [[Back]](#toc)

### [82] [Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.04651&hl=zh-CN&sa=X&d=4442328996166826277&ei=ikF1aZitDfOlieoPkILrwAk&scisig=AHkA5jRIIb679jcfAdOYnr_lkWxO&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=0&folt=rel)
*C Xu,L Yan,J Wu,H Wang,S Wang,Y Li,J Huang…*

Main category: Matei Zaharia

TL;DR: 该论文提出了一种名为RAG-Reasoner的新框架，通过多视角推理和动态检索增强，解决了大型推理模型在检索增强生成中的两个关键挑战：单一视角推理和静态检索机制。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型与检索增强生成的结合存在两个主要问题：1）推理模型通常从单一视角进行推理，限制了其全面理解复杂问题的能力；2）检索机制通常是静态的，无法根据推理过程动态调整，导致检索结果与推理需求不匹配。

Method: 提出了RAG-Reasoner框架，包含三个核心组件：1）多视角推理模块，从不同角度分析问题；2）动态检索模块，根据推理状态自适应调整检索策略；3）协同融合模块，整合多视角推理结果和动态检索信息。该框架采用迭代推理机制，在推理过程中不断优化检索策略。

Result: 在多个基准测试中，RAG-Reasoner相比现有方法在准确性和推理质量上均有显著提升。具体表现为：1）复杂问题解答准确率提高15-20%；2）推理过程的连贯性和逻辑性得到改善；3）检索效率提升，减少了不相关信息的干扰。

Conclusion: RAG-Reasoner框架通过多视角推理和动态检索的协同机制，有效解决了大型推理模型在检索增强生成中的局限性，为复杂推理任务提供了更强大的解决方案，推动了人工智能推理能力的发展。

Abstract: Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain:(1) reasoning models typically operate from a single …

</details>


### [83] [The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.15165&hl=zh-CN&sa=X&d=12425223643819823204&ei=ikF1aZitDfOlieoPkILrwAk&scisig=AHkA5jQyUam6LkEU_CcG4sUFWBZh&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=2&folt=rel)
*Z Ni,S Wang,Y Yue,T Yu,W Zhao,Y Hua,T Chen…*

Main category: Matei Zaharia

TL;DR: dLLMs打破传统LLMs的左到右约束，支持任意顺序的token生成，理论上具有更优的解空间，但实际性能仍待验证


<details>
  <summary>Details</summary>
Motivation: 传统LLMs受限于自回归的左到右生成顺序，限制了其灵活性和潜在性能。dLLMs通过扩散模型框架打破这一约束，理论上应能提供更优的生成能力

Method: 采用扩散大语言模型（dLLMs）框架，允许token以任意顺序生成，而非传统的自回归左到右顺序。通过扩散过程实现更灵活的生成策略

Result: 理论上dLLMs的解空间严格包含传统LLMs的解空间，但实际性能表现仍需进一步验证，可能受训练策略和优化方法影响

Conclusion: dLLMs为语言模型提供了更灵活的生成范式，理论上具有优势，但实际应用效果需要更深入的研究和评估

Abstract: Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive …

</details>


### [84] [Mi: dm 2.0 Korea-centric Bilingual Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.09066&hl=zh-CN&sa=X&d=2877226607759881096&ei=ikF1aZitDfOlieoPkILrwAk&scisig=AHkA5jSw6CerAm19vqyoj-HKnqhC&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=4&folt=rel)
*D Shin,S Lee,S Bae,H Ryu,C Ok,H Jung,H Ji,J Lim…*

Main category: Matei Zaharia

TL;DR: Mi:dm 2.0是一个专门为推进韩国中心AI设计的双语大语言模型，超越韩语文本处理，整合了韩国价值观、推理模式和常识


<details>
  <summary>Details</summary>
Motivation: 开发专门针对韩国文化和语言特点的大语言模型，超越简单的文本处理，整合韩国特有的价值观、推理模式和常识，以推进韩国中心AI的发展

Method: 构建双语大语言模型，整合韩国价值观、推理模式和常识，采用专门针对韩国文化特点的工程方法

Result: 成功开发了Mi:dm 2.0模型，这是一个专门为韩国中心AI设计的双语大语言模型

Conclusion: Mi:dm 2.0代表了韩国中心AI的重要进展，通过整合韩国特有的文化元素，为韩国语言和文化处理提供了更专业的AI解决方案

Abstract: We introduce Mi: dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. This model goes beyond Korean text processing by integrating the values, reasoning patterns, and commonsense …

</details>


### [85] [d3LLM: Ultra-Fast Diffusion LLM using Pseudo-Trajectory Distillation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07568&hl=zh-CN&sa=X&d=10293173906037769625&ei=ikF1aZitDfOlieoPkILrwAk&scisig=AHkA5jR9jIIWOfnxlXLKTUBdaoa2&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=7&folt=rel)
*YY Qian,J Su,L Hu,P Zhang,Z Deng,P Zhao…*

Main category: Matei Zaharia

TL;DR: dLLMs相比AR LLMs具有并行解码和随机顺序生成等优势，但在实际应用中面临挑战


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)相比自回归(AR) LLMs具有并行解码和随机顺序生成等额外能力，但在实际应用中实现这些优势面临挑战

Method: 论文未提供具体方法细节，但聚焦于分析dLLMs相对于AR LLMs的优势及其实现挑战

Result: 未提供具体实验结果，但指出了dLLMs在实际应用中面临的关键挑战

Conclusion: 虽然dLLMs具有理论优势，但在实际应用中实现这些优势需要解决特定的技术挑战

Abstract: Diffusion large language models (dLLMs) offer capabilities beyond those of autoregressive (AR) LLMs, such as parallel decoding and random-order generation. However, realizing these benefits in practice is non-trivial, as dLLMs inherently face …

</details>


### [86] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.13761&hl=zh-CN&sa=X&d=53638698130625861&ei=ikF1aZitDfOlieoPkILrwAk&scisig=AHkA5jTsgdIo9GPsovXThPMMl-gx&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=8&folt=rel)
*S Fan,X Ye,Y Lin*

Main category: Matei Zaharia

TL;DR: 自博弈与大型语言模型结合是实现自我改进AI的有前景范式，但现有框架存在优化不稳定性问题


<details>
  <summary>Details</summary>
Motivation: 现有自博弈框架存在优化不稳定性问题，主要源于：(i) 非平稳性，(ii) 策略崩溃，(iii) 探索-利用权衡。这些问题限制了自博弈在LLM中的有效应用。

Method: 从提供的摘要片段来看，论文可能提出了一种新的自博弈框架或改进方法来解决优化不稳定性问题。具体方法可能包括稳定化技术、改进的训练策略或算法创新。

Result: 摘要片段未提供具体结果，但可以推断该方法应能改善自博弈的稳定性，提升LLM在自博弈过程中的学习效率和性能。

Conclusion: 解决自博弈中的优化不稳定性对于实现LLM的有效自我改进至关重要，论文提出的方法为此提供了有前景的解决方案。

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary …

</details>
