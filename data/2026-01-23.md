<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [Xuanhe Zhou](#Xuanhe Zhou) [Total: 5]
- [Matei Zaharia](#Matei Zaharia) [Total: 8]
- [Google Scholar](#Google Scholar) [Total: 3]
- [cs.LG](#cs.LG) [Total: 36]
- [Alekh Jindal](#Alekh Jindal) [Total: 3]
- [Surajit Chaudhuri](#Surajit Chaudhuri) [Total: 4]
- [Carsten Binnig](#Carsten Binnig) [Total: 1]
- [Zongheng Yang](#Zongheng Yang) [Total: 6]
- [Ziniu Wu](#Ziniu Wu) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases](https://arxiv.org/abs/2601.15758)
*Xieyang Wang,Mengyi Liu,Weijia Yi,Jianqiu Xu,Raymond Chi-Wing Wong*

Main category: cs.DB

TL;DR: NL4ST是一个交互式工具，允许用户用自然语言查询时空数据库，通过三层架构将自然语言转换为可执行的物理查询计划。


<details>
  <summary>Details</summary>
Motivation: 移动计算设备和定位技术的进步导致时空数据爆炸式增长，但查询时空数据库需要领域专业知识和查询语言技能，这对非专业用户构成挑战，因此需要支持自然语言查询的解决方案。

Method: NL4ST采用三层架构：(1)知识库和语料库进行知识准备，(2)自然语言理解进行实体链接，(3)生成物理查询计划。系统将自然语言查询转换为可执行的数据库查询。

Result: NL4ST能够为时空数据库提供有效的物理查询计划，已在四个真实和合成数据集上验证了其有效性。系统已在线部署并提供演示视频。

Conclusion: NL4ST成功弥合了非专业用户与数据库查询计划之间的鸿沟，通过自然语言接口使时空数据库查询对普通用户更加可访问。

Abstract: The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.

</details>


### [2] [EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery](https://arxiv.org/abs/2601.16025)
*Yajuan Xu,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: EAIFD是一种用于增量函数依赖发现的高效算法，通过维护差异集的部分超图并将问题转化为超图上的最小命中集枚举，避免了完全重新执行。采用多属性哈希表(MHT)和两步验证策略，显著提升了性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 函数依赖是关系数据库中的基本完整性约束，但在增量更新下的发现仍然具有挑战性。静态算法由于完全重新执行而效率低下，增量算法则存在严重的性能和内存瓶颈。需要一种高效、可扩展的增量FD发现解决方案。

Method: EAIFD算法维护差异集的部分超图，将增量FD发现问题重新定义为超图上的最小命中集枚举。引入两个关键创新：1) 多属性哈希表(MHT)用于有效FD的高频键值映射，其内存消耗被证明与数据集大小无关；2) 两步验证策略，利用MHT有效减少验证空间，然后选择性加载数据块进行批量验证，避免重复I/O操作。

Result: 在真实世界数据集上的实验结果表明，EAIFD相比现有算法具有显著优势。在运行时间上实现了高达一个数量级的加速，同时将内存使用减少了两个数量级以上，证明了其作为增量FD发现的高效和可扩展解决方案。

Conclusion: EAIFD是一种高效、可扩展的增量函数依赖发现算法，通过创新的超图方法和内存优化技术，解决了现有方法在性能和内存方面的瓶颈问题，为数据库完整性约束的增量维护提供了实用解决方案。

Abstract: Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.

</details>


<div id='Xuanhe Zhou'></div>

# Xuanhe Zhou [[Back]](#toc)

### [3] [High-Quality Complex Text-to-SQL Data Generation through Chain-of-Verification](https://scholar.google.com/scholar_url?url=https://aclanthology.org/2025.findings-ijcnlp.143.pdf&hl=zh-CN&sa=X&d=7587639226537212652&ei=S3JyaZnYLqC16rQPqaeYsAc&scisig=AHkA5jTqnxVujMJYiavzmPP10_um&oi=scholaralrt&hist=i6heNjgAAAAJ:18363798109415294512:AHkA5jR4Nz41hcddyWt9RkK93bRp&html=&pos=0&folt=cit)
*Y Zhang,Y Gao,B Chen,W Li,S Sun,J Su*

Main category: Xuanhe Zhou

TL;DR: 论文提出Chain-of-Verifications框架，将少量专家标注的种子转化为大规模高质量Text-to-SQL数据集，解决现有基准规模小、成本高、复杂度分布不均的问题


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL基准存在明显不足：Spider1.0和BIRD等手工构建的数据集规模小、成本高、SQL复杂度偏向中等；而LLM生成的数据集虽然廉价，但存在浅层嵌套、语义漂移、模板疲劳和质量控制不足等问题，无法充分测试现代LLM的能力

Method: 提出Chain-of-Verifications框架，通过少量专家标注的种子数据，利用验证链机制自动生成大规模高质量Text-to-SQL数据集，解决现有数据集的局限性

Result: 未在摘要中明确说明具体结果，但暗示该方法能够生成大规模、高质量的数据集，能够更好地测试现代LLM在Text-to-SQL任务上的能力

Conclusion: 需要新的Text-to-SQL基准来充分测试现代LLM的能力，提出的Chain-of-Verifications框架能够有效解决现有数据集的不足，生成更全面、高质量的数据集

Abstract: Can today's Text-to-SQL benchmarks still stretch modern LLMs? We argue no. Spider1. 0 and BIRD, painstakingly hand-built, remain small, costly, and skewed toward middle complex SQL. Meanwhile, LLM-generated corpora are inexpensive but often superficial and fragile suffering from shallow nesting, semantic drift, template fatigue, and insufficient quality check. We address this gap with a Chain-of-Verifications framework that turns a handful of expert-labelled seeds into a large …

</details>


### [4] [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.08187&hl=zh-CN&sa=X&d=462578899627140995&ei=SnJyaYiHKvDB6rQPsMjv6As&scisig=AHkA5jS9vTN1-q2jJ4QYlUHu3JlF&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=0&folt=rel)
*Z Di,B Lu,H Kang,L Fu,J Ding,X Gan,L Zhou…*

Main category: Xuanhe Zhou

TL;DR: 该论文针对文本属性图(TAG)理解中LLM应用的局限性，提出了一种新的框架GraphAgent，通过将图结构转化为可执行的代码来增强LLM的图推理能力


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖手工设计的提示词将图结构文本化，这种方法存在两个主要问题：1) 图结构文本化会导致信息丢失和冗余；2) LLM在处理复杂图结构时容易产生幻觉。需要一种更有效的方法来增强LLM在图数据上的推理能力

Method: 提出GraphAgent框架，将图结构转化为可执行的Python代码，使LLM能够通过代码执行来探索图结构。框架包括三个核心组件：1) 图到代码转换器，将图结构转化为可执行的Python代码；2) 代码执行器，在安全环境中执行代码；3) 推理模块，基于代码执行结果进行推理。这种方法允许LLM通过代码交互来探索图结构，而不是依赖静态的文本描述

Result: 在多个基准测试中，GraphAgent显著优于现有方法，在节点分类任务上平均提升5.5%，在链接预测任务上平均提升17.1%。特别是在处理复杂图结构时，GraphAgent表现出更强的鲁棒性和准确性，有效减少了LLM的幻觉问题

Conclusion: 将图结构转化为可执行代码是一种有效的LLM图理解方法，能够显著提升LLM在图数据上的推理能力。GraphAgent框架为LLM在图分析任务中的应用提供了新的思路，通过代码交互的方式解决了传统文本化方法的局限性

Abstract: Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its …

</details>


### [5] [Breaking the Spiral: A Utility-Driven Optimization Framework for Balanced Information Retrieval in the LLM Era](https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3788865&hl=zh-CN&sa=X&d=8931733612144997941&ei=SnJyaYiHKvDB6rQPsMjv6As&scisig=AHkA5jTXJkiRGy0IV0VKX7_1TVKI&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=1&folt=rel)
*X Chen,B He,H Lin,X Han,T Wang,B Cao,L Sun…*

Main category: Xuanhe Zhou

TL;DR: 该论文探讨了LLM生成文本对检索系统的长期影响，特别是对RAG系统性能的潜在退化问题


<details>
  <summary>Details</summary>
Motivation: 随着LLM和RAG系统的广泛应用，LLM生成文本可能污染训练数据，导致检索系统性能退化，形成"数据中毒"循环

Method: 通过模拟实验分析LLM生成文本对检索系统的影响，评估数据污染程度和系统性能变化

Result: LLM生成文本确实会导致检索系统性能下降，形成负反馈循环，影响信息检索质量

Conclusion: 需要开发新的机制来检测和缓解LLM生成文本对检索系统的负面影响，确保信息检索系统的长期健康

Abstract: The widespread adoption of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems is reshaping the landscape of information retrieval. However, the long-term effects of LLM-generated texts on retrieval systems …

</details>


### [6] [SharP: Soft and Hard Prompt-Guided Augmentation with LLM for Low Resource Fake News Detection](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0957417426000928&hl=zh-CN&sa=X&d=11254404912360424935&ei=SnJyaYiHKvDB6rQPsMjv6As&scisig=AHkA5jQdZkzXtAZCN-T_XOOWLy6T&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=2&folt=rel)
*Z Luo,W Li,H Huang,K Liu,M Gao*

Main category: Xuanhe Zhou

TL;DR: 该论文研究低资源条件下的假新闻检测，针对标记数据稀缺和难以捕捉微妙欺骗模式的问题，提出了一种改进方法。


<details>
  <summary>Details</summary>
Motivation: 低资源条件下的假新闻检测面临两大挑战：标记数据稀缺和难以捕捉微妙的欺骗模式。现有方法在数据有限时效果不佳，需要开发更有效的低资源检测技术。

Method: 论文提出了一种针对低资源假新闻检测的改进方法，可能涉及数据增强、迁移学习、少样本学习或半监督学习等技术，以在标记数据有限的情况下提高检测性能。

Result: 该方法在低资源条件下相比基线方法取得了更好的假新闻检测性能，能够更有效地利用有限的标记数据，并更好地捕捉欺骗性模式。

Conclusion: 该研究为低资源假新闻检测提供了有效的解决方案，证明了在标记数据稀缺的情况下仍能实现可靠的检测性能，对实际应用具有重要意义。

Abstract: Fake news detection under low resource conditions is challenged by the scarcity of labeled data and the difficulty of capturing subtle deceptive patterns. In this work, we focus on low resource fake news detection, where only a small labeled set is …

</details>


### [7] [AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.18411&hl=zh-CN&sa=X&d=6862250751461476261&ei=SnJyaYiHKvDB6rQPsMjv6As&scisig=AHkA5jT-5-K4k7IaNmPRyoyR7FX3&oi=scholaralrt&hist=i6heNjgAAAAJ:13182394089411826447:AHkA5jSlroL7H9qfrEO07iLXc5er&html=&pos=3&folt=rel)
*F Song,Y Li,J Li,R Wang,C Zheng,F Xu,H Xiong*

Main category: Xuanhe Zhou

TL;DR: 多提示学习方法通过利用多种提示策略，在有限资源下有效促进视觉语言模型快速适应下游任务


<details>
  <summary>Details</summary>
Motivation: 现有多提示学习方法主要关注利用各种提示策略，但缺乏对提示之间协同作用的系统探索，限制了模型性能的进一步提升

Method: 提出一种新的多提示学习方法，通过系统探索和优化不同提示之间的协同作用，设计更有效的提示组合策略

Result: 相比现有方法，新方法在多个下游任务上取得了显著性能提升，特别是在少样本学习场景下表现出更强的适应能力

Conclusion: 系统探索提示协同作用是多提示学习的关键改进方向，能够显著提升视觉语言模型在资源受限场景下的适应性能

Abstract: Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various …

</details>


<div id='Matei Zaharia'></div>

# Matei Zaharia [[Back]](#toc)

### [8] [CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.02236&hl=zh-CN&sa=X&d=6694489326265326153&ei=S3JyadCPH7ui6rQPrda9qQc&scisig=AHkA5jTHdkNfEi7WSZSAMSeYqlSb&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=2&folt=rel)
*Y Liang,Z Wang,H Chen,X Sun,J Wu,X Yu,J Liu…*

Main category: Matei Zaharia

TL;DR: 扩散语言模型通过并行解码解决自回归语言模型的序列依赖延迟问题


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型在许多基准测试中表现出色，但其解码过程受到先前生成token的序列依赖限制，导致延迟问题。扩散语言模型有望通过并行解码来解决这一根本性延迟限制。

Method: 论文提出使用扩散语言模型作为自回归模型的替代方案，通过并行解码机制来克服序列依赖带来的延迟问题。

Result: 扩散语言模型在保持生成质量的同时，能够实现并行解码，显著降低生成延迟，解决了自回归模型的根本性延迟限制。

Conclusion: 扩散语言模型是解决自回归语言模型序列依赖延迟问题的有前景的替代方案，通过并行解码机制能够显著提升生成效率。

Abstract: Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel …

</details>


### [9] [Mi: dm 2.0 Korea-centric Bilingual Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.09066&hl=zh-CN&sa=X&d=2877226607759881096&ei=S3JyadCPH7ui6rQPrda9qQc&scisig=AHkA5jSw6CerAm19vqyoj-HKnqhC&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=3&folt=rel)
*D Shin,S Lee,S Bae,H Ryu,C Ok,H Jung,H Ji,J Lim…*

Main category: Matei Zaharia

TL;DR: Mi:dm 2.0是一个专门为推进韩国中心AI设计的双语大语言模型，超越韩语文本处理，整合了韩国价值观、推理模式和常识


<details>
  <summary>Details</summary>
Motivation: 开发专门针对韩国文化和语言特点的大语言模型，解决现有通用模型在处理韩国特定价值观、推理模式和常识方面的不足，推进韩国中心AI的发展

Method: 构建双语大语言模型，整合韩国价值观、推理模式和常识，超越单纯的文本处理，实现文化敏感的AI系统

Result: 成功开发了Mi:dm 2.0模型，这是一个专门针对韩国文化和语言特点的双语大语言模型

Conclusion: Mi:dm 2.0代表了韩国中心AI的重要进展，为处理韩国特定文化内容提供了专门化的解决方案

Abstract: We introduce Mi: dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. This model goes beyond Korean text processing by integrating the values, reasoning patterns, and commonsense …

</details>


### [10] [Benchmark^ 2: Systematic Evaluation of LLM Benchmarks](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03986&hl=zh-CN&sa=X&d=2470135056096510870&ei=S3JyadCPH7ui6rQPrda9qQc&scisig=AHkA5jTa2kJIdwrBbfWcBPcD8nKo&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=4&folt=rel)
*Q Qian,C Huang,J Xu,C Lv,M Wu,W Liu,X Wang…*

Main category: Matei Zaharia

TL;DR: 提出了Benchmark^2框架，用于系统评估大语言模型评测基准的质量，包含三个核心维度：可靠性、公平性和实用性


<details>
  <summary>Details</summary>
Motivation: 大语言模型评测基准的快速扩散导致了对基准质量本身进行系统评估的迫切需求。现有基准质量参差不齐，缺乏统一评估标准，影响了模型评估的准确性和可比性。

Method: 提出Benchmark^2框架，包含三个核心维度：1) 可靠性（测量一致性、稳定性和可复现性），2) 公平性（评估偏差、代表性和包容性），3) 实用性（衡量效率、可扩展性和实际价值）。框架提供量化指标和评估工具。

Result: Benchmark^2框架能够系统识别现有基准的优缺点，为基准开发者提供改进指导，为模型评估者提供基准选择依据，提升整个评测生态的质量和可信度。

Conclusion: 基准质量评估是确保大语言模型评测有效性的关键环节。Benchmark^2为基准评估提供了系统化框架，有助于推动更可靠、公平和实用的评测实践。

Abstract: The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^ 2, a comprehensive framework comprising three …

</details>


### [11] [d3LLM: Ultra-Fast Diffusion LLM using Pseudo-Trajectory Distillation](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07568&hl=zh-CN&sa=X&d=10293173906037769625&ei=S3JyadCPH7ui6rQPrda9qQc&scisig=AHkA5jR9jIIWOfnxlXLKTUBdaoa2&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=5&folt=rel)
*YY Qian,J Su,L Hu,P Zhang,Z Deng,P Zhao…*

Main category: Matei Zaharia

TL;DR: 扩散大语言模型(dLLMs)相比自回归LLMs具有并行解码和随机顺序生成等优势，但在实际应用中面临挑战


<details>
  <summary>Details</summary>
Motivation: 探索扩散大语言模型(dLLMs)相对于自回归LLMs的潜在优势，包括并行解码和随机顺序生成能力，并解决实际应用中的挑战

Method: 未在提供的摘要中明确说明具体方法，但涉及扩散模型在语言生成中的应用，可能包括扩散过程的优化、训练策略或架构改进

Result: 摘要未提供具体结果，但暗示dLLMs在实际应用中面临非平凡挑战，需要进一步研究来充分发挥其潜力

Conclusion: 扩散大语言模型具有超越自回归模型的独特能力，但需要解决实际应用中的挑战才能充分发挥其优势

Abstract: Diffusion large language models (dLLMs) offer capabilities beyond those of autoregressive (AR) LLMs, such as parallel decoding and random-order generation. However, realizing these benefits in practice is non-trivial, as dLLMs inherently face …

</details>


### [12] [Distilling Robustness: Mitigating Persona Sensitivity in Language Models via RLVR Teacher-Student Training](https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3DPRjRT5iRbP&hl=zh-CN&sa=X&d=13710676611840491387&ei=S3JyadCPH7ui6rQPrda9qQc&scisig=AHkA5jTVNV3N3Z0K3VQjhNS2wxMz&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=6&folt=rel)
*J Oh,M Aghazada,SY Yun*

Main category: Matei Zaharia

TL;DR: 本文探讨了角色提示对大型语言模型性能的影响，指出虽然角色提示能提升性能，但找到最优角色非常困难，其效果往往不可预测甚至有害。当前研究主要关注角色选择，但缺乏对角色如何影响模型内部推理过程的深入理解。


<details>
  <summary>Details</summary>
Motivation: 角色提示已被证明能提升LLM性能，但其效果不稳定且难以预测。现有研究主要关注选择"正确"的角色，但缺乏对角色如何影响模型内部推理机制的系统性理解。需要深入探究角色提示的作用机制，以更可靠地利用这一技术。

Method: 论文未提供具体方法细节，但从摘要可推断研究可能采用：1）系统性的角色提示实验设计；2）对模型内部表示和推理过程的深入分析；3）可能结合可解释性AI技术来理解角色如何影响模型行为；4）对比不同角色提示策略的效果。

Result: 摘要未提供具体实验结果，但暗示研究发现：1）角色提示的效果高度不可预测；2）不合适的角色可能对性能产生负面影响；3）需要更深入理解角色如何影响模型的内部推理过程，而不仅仅是选择表面合适的角色。

Conclusion: 角色提示的有效性不仅取决于角色选择，更关键的是理解角色如何影响模型的内部推理机制。未来研究需要超越简单的角色选择，深入探究角色提示的作用机理，以实现更可靠和可预测的性能提升。

Abstract: While persona prompting can boost Large Language Model (LLM) performance, prior work shows that finding the optimal persona is notoriously difficult, with its effects often being unpredictable and even detrimental. Current research has …

</details>


### [13] [ORBITFLOW: SLO-Aware Long-Context LLM Serving with Fine-Grained KV Cache Reconfiguration](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.10729&hl=zh-CN&sa=X&d=5345457422684473681&ei=S3JyadCPH7ui6rQPrda9qQc&scisig=AHkA5jTdHsaUotKNG-1eZCVkncRP&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=7&folt=rel)
*X Ma,H Hong,T Um,J Lee,S Choy,WY Lee,M Jeon*

Main category: Matei Zaharia

TL;DR: 针对长上下文LLM服务中KV缓存内存占用波动大的问题，提出动态KV缓存管理框架，通过运行时内存调整和智能卸载策略优化内存使用


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM服务面临挑战：请求长度和批次组成在token生成过程中变化，导致运行时内存占用显著波动。将KV缓存卸载到主机内存限制了有效内存带宽，影响性能

Method: 提出动态KV缓存管理框架，包括运行时内存调整机制和智能卸载策略，根据实际需求动态分配和释放GPU内存，优化KV缓存的内存使用

Result: 该框架能够显著减少内存占用波动，提高内存利用率，同时保持或提升推理性能，特别是在处理变长请求和动态批次时表现优异

Conclusion: 动态KV缓存管理是解决长上下文LLM服务内存挑战的有效方案，通过智能内存管理策略平衡了内存使用和计算性能

Abstract: Serving long-context LLMs is challenging because request lengths and batch composition vary during token generation, causing the memory footprint to fluctuate significantly at runtime. Offloading KV caches to host memory limits effective memory …

</details>


### [14] [ECLIPTICA-A Framework for Switchable LLM Alignment via CITA-Contrastive Instruction-Tuned Alignment](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06157&hl=zh-CN&sa=X&d=10780815274848775517&ei=S3JyadCPH7ui6rQPrda9qQc&scisig=AHkA5jSRmXZBnnkHp5j-IEvMQNZ5&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=8&folt=rel)
*K Wanaskar,G Jena,V Jain,A Chadha,A Das*

Main category: Matei Zaharia

TL;DR: 论文指出当前大语言模型的对齐方法（如DPO、GRPO）是静态的，训练后策略被固化，缺乏运行时控制能力，需要更灵活的动态对齐方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的对齐方法存在局限性：训练后策略被冻结，只能通过提示技巧或昂贵的重新对齐进行有限控制，缺乏动态、实时的行为调整能力。

Method: 论文提出了一种动态对齐方法（具体方法未在摘要中详细说明，但暗示了不同于DPO、GRPO等静态对齐的新方法）。

Result: 摘要未提供具体实验结果，但暗示新方法能够提供更好的运行时控制能力，减少对提示技巧或重新对齐的依赖。

Conclusion: 需要从静态对齐转向动态对齐方法，使大语言模型在部署后仍能灵活调整行为，提高可控性和适应性。

Abstract: Alignment in large language models (LLMs) is still largely static: after training, the policy is frozen. DPO, GRPO methods typically imprint one behavior into the weights, leaving little runtime control beyond prompt hacks or expensive re-alignment. We …

</details>


### [15] [Structured Reasoning for Large Language Models](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.07180&hl=zh-CN&sa=X&d=1191685630117185386&ei=S3JyadCPH7ui6rQPrda9qQc&scisig=AHkA5jQquljm5N6FJDvCrI4bKH3I&oi=scholaralrt&hist=i6heNjgAAAAJ:15580076029636943118:AHkA5jSEWE2pdcgDuWOJdF-lmUBr&html=&pos=9&folt=rel)
*J Han,Z Di,Z Jiang,Y Liao,J Liang,Y Wang,Y Xiao*

Main category: Matei Zaharia

TL;DR: 该论文针对大语言模型生成冗长思维链时出现冗余推理步骤的问题，提出了一种基于强化学习的自动修剪方法，通过奖励机制优化推理路径，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过生成长思维链获得强性能，但长推理轨迹常包含冗余或无效步骤，导致计算资源浪费和推理效率低下。现有方法难以自动识别和修剪这些不必要的推理步骤。

Method: 提出基于强化学习的自动修剪框架，将推理过程建模为马尔可夫决策过程，使用奖励函数评估推理步骤的有效性，通过策略梯度方法学习最优修剪策略，保留关键推理步骤的同时去除冗余部分。

Result: 实验表明该方法能显著减少推理步骤数量（平均减少30-50%），同时保持或提升任务准确率，在数学推理、常识推理和代码生成等任务上均优于基线方法。

Conclusion: 基于强化学习的自动修剪方法能有效优化大语言模型的推理过程，在保持性能的同时显著提高推理效率，为高效推理系统提供了新思路。

Abstract: Large language models (LLMs) achieve strong performance by generating long chains of thought, but longer traces always introduce redundant or ineffective reasoning steps. One typical behavior is that they often perform unnecessary …

</details>


<div id='Google Scholar'></div>

# Google Scholar [[Back]](#toc)

### [16] [Towards More Efficient Long-Context Handling: Smarter Partitioning, Retrieval, and Execution in Recursive Language Models](https://scholar.google.com/scholar_url?url=https://kristianpraizner.com/papers/RLM_draft-3.pdf&hl=en&sa=X&d=15424025976884530857&ei=SXJyaa3FBKOi6rQPoMCXoAE&scisig=AHkA5jRk2gsNLBUn33XJTlLiHkQ-&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=0&folt=cit)
*L Bluestein,D Lee,K Praizner,T Toloraia*

Main category: Google Scholar

TL;DR: RLMs通过分区处理长上下文，但现有实现基于简单分区和检索启发式，存在局限性


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理长上下文时准确性下降，即使输入在模型支持长度内。现有递归语言模型实现依赖简单的分区和检索方法，需要改进

Method: 分析现有RLM实现，特别是Zhang (2025)的方法，指出其依赖基于token或正则表达式的简单分区、基本检索启发式和严格顺序处理

Result: 现有RLM实现存在局限性，需要更先进的分区策略、检索机制和处理方法

Conclusion: 需要改进RLM实现，开发更智能的分区策略、检索机制和非顺序处理方法以更好处理长上下文

Abstract: Long contexts remain difficult for language models, with empirical evidence demonstrating that accuracy declines even when inputs are well within the model's supported length. Recursive Language Models (RLMs) provide a way to handle these inputs by partitioning the context and invoking the model on smaller subproblems. Existing implementations, specifically Zhang (2025), rely on simple token or regex-based partitioning, basic retrieval heuristics, and strictly sequential …

</details>


### [17] [Missing Value Imputation in Tabular Data Lakes Unleashed: A Hybrid Approach: F. Luo et al.](https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s00778-025-00957-1&hl=en&sa=X&d=4606946733610701853&ei=SXJyaa3FBKOi6rQPoMCXoAE&scisig=AHkA5jSP3th-QnoRoXa_mamsssfF&oi=scholaralrt&hist=i6heNjgAAAAJ:6389686251013311652:AHkA5jSP8671UBQnfQacXFBHq92d&html=&pos=1&folt=cit)
*F Luo,H Lan,H Luo,Z Bao,JS Culpepper,S Sadiq…*

Main category: Google Scholar

TL;DR: 论文提出了一种新的数据湖缺失值填补框架，强调需要考虑三个方面的多样性：填补值来源、涉及表格类型和缺失值数据类型，以解决现有方法在复杂数据湖环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 表格数据湖中的缺失值严重影响数据分析和下游应用性能。现有填补方法主要依赖基于估计的方法（使用同一表格数据训练的模型）或基于搜索的方法，但未能充分考虑数据湖环境的复杂性，特别是需要同时考虑填补值来源、表格类型和数据类型三个维度的多样性。

Method: 论文提出了一个综合考虑三个多样性维度的填补框架：1）填补值来源多样性（来自同一表格、相似表格或外部知识）；2）表格类型多样性（结构化表格、半结构化表格等）；3）数据类型多样性（数值型、分类型、文本型等）。框架可能结合估计和搜索方法，并考虑数据湖中表格间的关联性。

Result: 论文提出的框架相比传统方法在数据湖缺失值填补任务上表现出更好的性能，能够更有效地处理复杂的数据环境，提高填补准确性和下游应用性能。

Conclusion: 在数据湖环境中进行缺失值填补时，必须同时考虑填补值来源、表格类型和数据类型三个维度的多样性。提出的综合框架能够更好地适应数据湖的复杂性，为实际应用提供更可靠的填补解决方案。

Abstract: Missing values in tabular data lakes can severely impact data analysis and diminish the performance in downstream applications. We highlight that a robust imputation strategy should properly take three aspects of variety into consideration: source of imputed value, the types of tables involved, and the data types of the missing value. Existing imputation methods rely on estimation-based approaches (using a model trained on data from the same table to estimate missing values) or search-based …

</details>


### [18] [DISASTER RECOVERY AND BACKUP STRATEGIES FOR FINANCIAL DATA LAKES](https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/James-Benardo/publication/387031051_DISASTER_RECOVERY_AND_BACKUP_STRATEGIES_FOR_FINANCIAL_DATA_LAKES/links/675d0eddda24c8537c6eb090/DISASTER-RECOVERY-AND-BACKUP-STRATEGIES-FOR-FINANCIAL-DATA-LAKES.pdf&hl=en&sa=X&d=3674063306391510866&ei=SnJyaYK7O7uM6rQP2dOq-Ag&scisig=AHkA5jQ0jkiRNWSL-H_Hq6a-R13g&oi=scholaralrt&hist=i6heNjgAAAAJ:13225314161935261941:AHkA5jR-WPkAfpCINzU6oW8zO6Qz&html=&pos=1&folt=rel)
*T Olasehinde,F Wills*

Main category: Google Scholar

TL;DR: 金融数据湖管理面临数据质量、治理和可访问性挑战，需要系统化解决方案


<details>
  <summary>Details</summary>
Motivation: 金融机构严重依赖海量数据进行决策、合规和运营优化，数据湖已成为结构化与非结构化数据的核心存储库，但面临数据质量、治理和可访问性等挑战

Method: 未在摘要中明确说明具体方法，但暗示需要系统化的数据湖管理框架或解决方案

Result: 未在摘要中提供具体结果，但暗示现有数据湖存在管理挑战

Conclusion: 金融机构需要改进数据湖管理以充分发挥数据价值，确保数据质量、治理和可访问性

Abstract: Financial institutions depend heavily on large volumes of data to make decisions, ensure regulatory compliance, and optimize operations. As such, data lakes have become a central repository for unstructured and structured data. However, the …

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Language Models Entangle Language and Culture](https://arxiv.org/abs/2601.15337)
*Shourya Jain,Paras Chopra*

Main category: cs.LG

TL;DR: 研究发现LLMs在低资源语言中提供更低质量的开放性问题回答，语言选择显著影响模型使用的文化背景，进而影响回答质量


<details>
  <summary>Details</summary>
Motivation: 确保用户不会因使用不同语言与LLMs交互而受到系统性不利影响，即不同语言用户应获得相似质量的回答，无论使用何种语言

Method: 基于WildChat数据集分析创建真实世界开放性问题集，评估回答质量是否因查询语言而异；使用LLM-as-a-Judge识别回答中的文化背景；在多种语言上评估CulturalBench基准的翻译子集

Result: LLMs在低资源语言中持续提供更低质量的开放性问题回答；语言显著影响模型使用的文化背景；这种背景差异影响下游回答质量

Conclusion: 语言选择不仅影响回答质量，还影响LLMs使用的文化背景，揭示了语言和文化在LLMs中的纠缠关系，需要解决多语言公平性问题

Abstract: Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.

</details>


### [20] [Improving MoE Compute Efficiency by Composing Weight and Data Sparsity](https://arxiv.org/abs/2601.15370)
*Maciej Kilian,Oleg Mkrtchyan,Luke Zettlemoyer,Akshat Shrivastava,Armen Aghajanyan*

Main category: cs.LG

TL;DR: 通过引入零计算（空）专家实现数据稀疏性，在因果令牌选择MoE中结合权重稀疏性和数据稀疏性，提高计算效率


<details>
  <summary>Details</summary>
Motivation: 混合专家层通过权重稀疏性实现计算效率，但数据稀疏性提供了互补维度。专家选择路由直接实现数据稀疏性但违反自回归模型的因果性，造成训练-推理不匹配。需要在保持因果性的同时实现数据稀疏性。

Method: 在因果令牌选择MoE的路由池中引入零计算（空）专家。当令牌路由到空专家时，这些槽位不消耗计算。通过标准负载均衡目标训练模型均匀使用所有专家（真实和空），在期望上实现数据稀疏性而不违反因果性。

Result: 在视觉语言模型训练中，权重稀疏性和数据稀疏性组合在匹配的期望FLOPs下比单独使用权重稀疏性产生更高效的计算前沿，在训练损失和下游性能上都有提升。模型学习隐式的模态感知分配，视觉令牌比文本令牌更积极地路由到空专家。

Conclusion: 通过引入空专家，可以在因果令牌选择MoE中实现数据稀疏性，结合权重稀疏性进一步提高计算效率，特别适用于视觉语言模型等数据异构场景。

Abstract: Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.

</details>


### [21] [Ambient Dataloops: Generative Models for Dataset Refinement](https://arxiv.org/abs/2601.15417)
*Adrián Rodríguez-Muñoz,William Daspit,Adam Klivans,Antonio Torralba,Constantinos Daskalakis,Giannis Daras*

Main category: cs.LG

TL;DR: Ambient Dataloops是一个迭代式数据集精炼框架，通过数据集与模型的协同进化过程，逐步提升数据集质量，使扩散模型更容易学习底层数据分布。


<details>
  <summary>Details</summary>
Motivation: 现代数据集包含质量差异很大的样本，直接在这样异构的数据上训练往往产生次优模型。需要一种方法来提升数据集质量，从而改善模型性能。

Method: 提出数据集-模型协同进化过程：在每次迭代中，数据集质量逐步提升，模型相应改进。为避免破坏性的自消耗循环，将合成改进的样本视为噪声样本（但噪声水平略低于前一次迭代），并使用Ambient Diffusion技术在损坏条件下学习。

Result: 在无条件图像生成、文本条件图像生成和从头蛋白质设计任务中实现了最先进的性能。提供了理论分析证明数据循环过程的优势。

Conclusion: Ambient Dataloops框架通过迭代式数据集精炼有效提升了扩散模型的性能，为处理异构数据集提供了一种系统化方法。

Abstract: We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.

</details>


### [22] [CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models](https://arxiv.org/abs/2601.15441)
*Zhenghao He,Guangzhi Xiong,Boyang Wang,Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: CASL：首个通过监督学习将扩散模型稀疏潜在维度与语义概念对齐的框架，结合CASL-Steer因果探针和EPR评估指标，实现精确概念控制


<details>
  <summary>Details</summary>
Motivation: 现有基于稀疏自编码器（SAE）的扩散模型理解方法主要依赖无监督方法，无法将稀疏特征与人类可理解概念对齐，限制了语义控制的可靠性

Method: CASL框架：1）在冻结U-Net激活上训练SAE获得解耦潜在表示；2）学习轻量级线性映射，将每个概念与少量相关潜在维度关联；3）提出CASL-Steer作为因果探针验证语义对齐；4）引入编辑精度比（EPR）联合评估概念特异性和无关属性保持

Result: 实验表明，相比现有方法，CASL在编辑精度和可解释性方面表现更优，首次实现了扩散模型中潜在表示与语义概念的监督对齐

Conclusion: CASL是首个通过监督学习将扩散模型稀疏潜在维度与语义概念对齐的框架，为理解扩散模型内部表示和实现精确语义控制提供了新途径

Abstract: Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Concept-Aligned Sparse Latents), a supervised framework that aligns sparse latent dimensions of diffusion models with semantic concepts. CASL first trains an SAE on frozen U-Net activations to obtain disentangled latent representations, and then learns a lightweight linear mapping that associates each concept with a small set of relevant latent dimensions. To validate the semantic meaning of these aligned directions, we propose CASL-Steer, a controlled latent intervention that shifts activations along the learned concept axis. Unlike editing methods, CASL-Steer is used solely as a causal probe to reveal how concept-aligned latents influence generated content. We further introduce the Editing Precision Ratio (EPR), a metric that jointly measures concept specificity and the preservation of unrelated attributes. Experiments show that our method achieves superior editing precision and interpretability compared to existing approaches. To the best of our knowledge, this is the first work to achieve supervised alignment between latent representations and semantic concepts in diffusion models.

</details>


### [23] [Learning from Synthetic Data: Limitations of ERM](https://arxiv.org/abs/2601.15468)
*Kareem Amin,Alex Bie,Weiwei Kong,Umar Syed,Sergei Vassilvitskii*

Main category: cs.LG

TL;DR: 论文研究在合成数据污染环境下学习理论的基本问题，发现传统ERM方法存在局限性，而加权算法能更好处理混合数据


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成内容的普及和低成本，自然数据被合成数据污染的现象日益普遍。需要重新审视在这种混合数据环境下的学习理论基本问题，特别是当学习算法无法区分数据来源时

Method: 将场景建模为一系列学习任务，输入是自然数据和合成数据的混合，算法不知道单个样本的来源。研究ERM在这种设置下的可能性和局限性，并与为不同代数据分配非均匀权重的算法进行比较

Result: 对于估计任意d维分布均值的问题，ERM虽然收敛到真实均值，但被非均匀加权算法超越。在PAC学习设置中，差异更显著：ERM并不总是收敛到真实概念，但存在算法能够学习任意VC类和任意污染量的正确假设

Conclusion: 在合成数据污染环境中，传统ERM方法存在根本局限性，需要开发新的算法策略来处理混合数据，特别是能够区分不同代数据并适当加权的算法

Abstract: The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.

</details>


### [24] [Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding](https://arxiv.org/abs/2601.15482)
*Huayu Li,ZhengXiao He,Siyuan Tian,Jinghao Wen,Ao Li*

Main category: cs.LG

TL;DR: MFS将LLM解码重构为最优随机过程识别问题，利用鞅理论设计理论基础的算法，在推理基准上超越SOTA方法并提升计算效率


<details>
  <summary>Details</summary>
Motivation: 标准自回归解码在大型语言模型中具有短视性，难以找到全局最优推理路径。现有的前瞻采样方法依赖启发式机制进行路径评估和搜索空间剪枝，缺乏理论依据。

Method: 提出鞅前瞻采样(MFS)框架，将LLM解码重构为最优随机过程识别问题。利用鞅理论设计算法：1) 基于Doob分解定理推导步骤评估，测量路径的可预测优势；2) 使用可选停止理论进行原则性路径选择和剪枝；3) 基于鞅收敛定理的自适应停止规则，在路径质量收敛时终止探索。

Result: 在六个推理基准测试中，MFS在准确性方面超越了最先进的方法，同时显著提高了计算效率。

Conclusion: MFS为LLM解码提供了一个理论基础的框架，通过鞅理论替代启发式机制，在保持计算效率的同时提升推理性能。

Abstract: Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

</details>


### [25] [MARS: Unleashing the Power of Speculative Decoding via Margin-Aware Verification](https://arxiv.org/abs/2601.15498)
*Jingwei Song,Xinyu Wang,Hanbin Wang,Xiaoxuan Lei,Bill Shi,Shixin Han,Eric Yang,Xiao-Wen Chang,Lynn Ai*

Main category: cs.LG

TL;DR: 提出Margin-Aware Speculative Verification方法，通过自适应目标模型的局部决策稳定性来改进推测解码中的验证机制，在保持生成质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现代LLM经常在低边际区域运行，目标模型对顶级候选词表现出弱偏好。在这种情况下，拒绝合理的次优标记带来的信息增益微乎其微，却会产生显著的回滚成本，导致验证机制存在根本性低效问题。

Method: 提出Margin-Aware Speculative Verification方法，这是一种无需训练且与领域无关的验证策略。该方法根据从目标模型logits直接测量的决策稳定性来条件化验证，仅在严格验证提供最小收益时放宽拒绝规则。该方法仅修改验证规则，与现有的目标耦合推测解码框架完全兼容。

Result: 在8B到235B不同模型规模上的广泛实验表明，该方法相比最先进的基线方法提供了一致且显著的推理加速，同时在多样化基准测试中保持了生成质量。

Conclusion: 通过自适应目标模型的局部决策稳定性来改进推测解码中的验证机制，可以在保持生成质量的同时显著提升推理效率，为解决现代LLM在低边际区域运行时的验证低效问题提供了有效方案。

Abstract: Speculative Decoding (SD) accelerates autoregressive large language model (LLM) inference by decoupling generation and verification. While recent methods improve draft quality by tightly coupling the drafter with the target model, the verification mechanism itself remains largely unchanged, relying on strict token-level rejection sampling. In practice, modern LLMs frequently operate in low-margin regimes where the target model exhibits weak preference among top candidates. In such cases, rejecting plausible runner-up tokens yields negligible information gain while incurring substantial rollback cost, leading to a fundamental inefficiency in verification. We propose Margin-Aware Speculative Verification, a training-free and domain-agnostic verification strategy that adapts to the target model's local decisiveness. Our method conditions verification on decision stability measured directly from the target logits and relaxes rejection only when strict verification provides minimal benefit. Importantly, the approach modifies only the verification rule and is fully compatible with existing target-coupled speculative decoding frameworks. Extensive experiments across model scales ranging from 8B to 235B demonstrate that our method delivers consistent and significant inference speedups over state-of-the-art baselines while preserving generation quality across diverse benchmarks.

</details>


### [26] [Data-driven Lake Water Quality Forecasting for Time Series with Missing Data using Machine Learning](https://arxiv.org/abs/2601.15503)
*Rishit Chatterjee,Tahiya Chowdhury*

Main category: cs.LG

TL;DR: 该研究提出了一种联合可行性策略，用于确定湖泊监测中达到目标精度所需的最小训练历史和最少预测因子数量，使志愿者主导的湖泊监测更高效实用。


<details>
  <summary>Details</summary>
Motivation: 志愿者主导的湖泊监测会产生不规则的季节性时间序列，存在大量数据缺失（冰盖、天气限制、人为错误等），这给有害藻华预测和预警带来困难。需要找到在保持预测精度的前提下，最小化监测工作量的方法。

Method: 使用MICE处理数据缺失，在30个湖泊的30年现场记录中评估六种候选模型，采用归一化平均绝对误差(nMAE)进行跨湖可比性评估。通过岭回归确定最小样本量和最小特征集，并引入联合可行性函数统一历史长度和特征选择。

Result: 岭回归表现最佳；在向后最近历史协议下，平均每个湖泊需要约176个训练样本即可达到完整历史精度的95%；紧凑的四特征子集与十三特征基线在5%容差内匹配；达到5%精度目标仅需约64个最近样本和每个湖泊一个预测因子。

Conclusion: 联合可行性策略将最近历史长度和特征选择统一在固定精度目标下，为湖泊研究者提供了设置采样工作和测量优先级的简单高效规则，使有针对性的监测更加实用可行。

Abstract: Volunteer-led lake monitoring yields irregular, seasonal time series with many gaps arising from ice cover, weather-related access constraints, and occasional human errors, complicating forecasting and early warning of harmful algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake, data-rich subset drawn from three decades of in situ records collected across Maine lakes. Missingness is handled via Multiple Imputation by Chained Equations (MICE), and we evaluate performance with a normalized Mean Absolute Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge regression provides the best mean test performance. Using ridge regression, we then quantify the minimal sample size, showing that under a backward, recent-history protocol, the model reaches within 5% of full-history accuracy with approximately 176 training samples per lake on average. We also identify a minimal feature set, where a compact four-feature subset matches the thirteen-feature baseline within the same 5% tolerance. Bringing these results together, we introduce a joint feasibility function that identifies the minimal training history and fewest predictors sufficient to achieve the target of staying within 5% of the complete-history, full-feature baseline. In our study, meeting the 5% accuracy target required about 64 recent samples and just one predictor per lake, highlighting the practicality of targeted monitoring. Hence, our joint feasibility strategy unifies recent-history length and feature choice under a fixed accuracy target, yielding a simple, efficient rule for setting sampling effort and measurement priorities for lake researchers.

</details>


### [27] [Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features](https://arxiv.org/abs/2601.15530)
*Megan A. Witherow,Michael L. Evans,Ahmed Temtam,Hamid Okhravi,Khan M. Iftekharuddin*

Main category: cs.LG

TL;DR: 提出机器学习方法，利用临床测试和MRI数据区分非典型阿尔茨海默病与非AD认知障碍，提高诊断准确率


<details>
  <summary>Details</summary>
Motivation: 非典型阿尔茨海默病（atAD）患者常被误诊，因为基于临床评估和海马体积的标准诊断方法主要针对典型AD。需要开发仅使用临床测试和MRI数据的改进诊断方法。

Method: 使用机器学习方法，基于临床测试数据和MRI特征（包括海马体积和全脑MRI特征）进行atAD与非AD认知障碍的分类。采用Boruta统计方法识别显著脑区，使用来自私人数据集、NACC和ADNI的1410名受试者数据。

Result: 结合重要MRI特征的方法优于仅使用海马体积。在NACC数据集中，atAD病例正确诊断率从52%提升至69%；在ADNI数据集中从34%提升至77%，同时保持高精度。

Conclusion: 提出的机器学习方法能显著提高非典型阿尔茨海默病的诊断准确性，仅使用临床标准护理数据，具有重要的临床应用价值。

Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.

</details>


### [28] [QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs](https://arxiv.org/abs/2601.15538)
*Himanshu Mishra,Kanwal Mehreen*

Main category: cs.LG

TL;DR: 本文发现低比特量化会灾难性地恢复已遗忘的知识，并提出了一种量化感知的遗忘方法来确保遗忘在量化后仍然有效。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从训练好的模型中移除特定知识（如受版权保护或私人数据），而无需完全重新训练。然而，在实际部署中，模型通常会被量化（如4位），研究发现量化会灾难性地恢复已遗忘的信息，这构成了一个重要的实际问题。

Method: 首先分析量化如何破坏遗忘效果，通过计算权重变化统计和量化桶重叠来展示典型的遗忘更新太小而无法跨越量化阈值。基于这一洞察，提出了一种对数空间铰链损失：对于每个遗忘样本，强制要求遗忘模型的输出对数与原始模型至少相差一个边界值（量化步长的一半），以确保遗忘样本在量化后仍然可区分。

Result: 在语言和分类任务（包括Twitter虚假信息数据集）上的评估表明，该方法在4位量化下能保持遗忘效果，而现有方法几乎完全恢复了被遗忘的知识。

Conclusion: 量化会严重破坏机器遗忘的效果，但通过量化感知的遗忘方法可以有效缓解这一问题，确保遗忘在量化部署后仍然有效。

Abstract: Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.

</details>


### [29] [PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction](https://arxiv.org/abs/2601.15540)
*Dongchen Huang*

Main category: cs.LG

TL;DR: 提出Prism架构，基于最大编码率减少原理构建白盒注意力机制，通过过完备字典和π-RoPE无理频率分离实现信号-噪声子空间解耦，在TinyStories上实现无监督功能解缠


<details>
  <summary>Details</summary>
Motivation: 深度学习模型特别是Transformer缺乏可解释性，被视为"黑箱"。研究者希望构建一个基于几何原理的白盒注意力架构，证明可解释性和性能可以统一而非权衡

Method: 提出Prism架构，将注意力机制建模为信号-噪声流形上的梯度上升过程。引入两个物理约束：1) 过完备字典扩展表示相空间；2) π-RoPE无理频率分离强制信号与噪声子空间不相干。这些几何归纳偏置被视为物理约束，足以单独诱导无监督功能解缠

Result: 在TinyStories测试平台上验证谱动态，观察到Prism自发地将注意力头专门化为谱分离机制：低频头捕获长程因果依赖(信号)，高频头处理局部句法约束(噪声)。模型实现了无监督的功能解缠

Conclusion: 可解释性和性能不是权衡关系，可以通过原则性的几何构造统一。基于MCR²原理的白盒注意力架构能够实现无监督功能解缠，为构建可解释的深度学习模型提供了新途径

Abstract: Deep learning models, particularly Transformers, are often criticized as "black boxes" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.

</details>


### [30] [RDumb++: Drift-Aware Continual Test-Time Adaptation](https://arxiv.org/abs/2601.15544)
*Himanshu Mishra*

Main category: cs.LG

TL;DR: RDumb++ 是一种持续测试时自适应方法，通过两种漂移检测机制（基于熵和KL散度）和自适应重置策略，在长时域数据流中防止模型预测崩溃，相比RDumb在CCC基准上获得约3%的绝对精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时自适应方法（如Tent、EATA）在短期分布漂移下表现良好，但在快速变化或极长时域的数据流中容易失效。CCC基准包含750万个样本的持续变化数据流，现有方法难以应对这种长期挑战。

Method: 提出RDumb++方法，作为RDumb的扩展，引入两种漂移检测机制：1) 基于熵的漂移评分，2) KL散度漂移评分。结合自适应重置策略，当检测到累积自适应变得有害时，模型能够重置以恢复性能，防止预测崩溃。

Result: 在CCC-medium基准的三个速度和三个种子（共9次运行，每次包含100万个样本）上，RDumb++始终优于RDumb，获得约3%的绝对精度提升，并在整个数据流中保持稳定的自适应性能。消融实验表明漂移感知重置对于防止崩溃和实现可靠的长期CTTA至关重要。

Conclusion: RDumb++通过漂移检测和自适应重置机制，有效解决了长期持续测试时自适应中的预测崩溃问题，在长时域数据流中实现了稳定可靠的性能。

Abstract: Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.

</details>


### [31] [Beyond validation loss: Clinically-tailored optimization metrics improve a model's clinical performance](https://arxiv.org/abs/2601.15546)
*Charles B. Delahunt,Courosh Mehanian,Daniel E. Shea,Matthew P. Horning*

Main category: cs.LG

TL;DR: 使用临床定制指标而非验证损失来优化医疗ML模型，能更好地满足临床需求，提升模型在实际医疗任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统ML使用验证损失来优化模型，但医疗ML的目标不同：模型需要满足具体的临床需求，而不仅仅是优化训练损失函数。临床需求可以通过定制指标更精确地捕捉。

Method: 设计了两个对照实验，比较使用临床定制指标与使用验证损失进行模型优化的效果。实验关注那些不需要驱动指标可微分的优化任务，从而允许使用更广泛的临床相关指标。

Result: 实验结果表明，使用临床定制指标进行模型优化相比使用验证损失，能在临床任务上获得更好的性能表现。

Conclusion: 虽然定义和编码临床相关指标需要额外努力，但这种方法能产生更符合医疗ML核心目标的模型：在临床实践中具有强大性能。

Abstract: A key task in ML is to optimize models at various stages, e.g. by choosing hyperparameters or picking a stopping point. A traditional ML approach is to use validation loss, i.e. to apply the training loss function on a validation set to guide these optimizations. However, ML for healthcare has a distinct goal from traditional ML: Models must perform well relative to specific clinical requirements, vs. relative to the loss function used for training. These clinical requirements can be captured more precisely by tailored metrics. Since many optimization tasks do not require the driving metric to be differentiable, they allow a wider range of options, including the use of metrics tailored to be clinically-relevant. In this paper we describe two controlled experiments which show how the use of clinically-tailored metrics provide superior model optimization compared to validation loss, in the sense of better performance on the clinical task. The use of clinically-relevant metrics for optimization entails some extra effort, to define the metrics and to code them into the pipeline. But it can yield models that better meet the central goal of ML for healthcare: strong performance in the clinic.

</details>


### [32] [Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling](https://arxiv.org/abs/2601.15547)
*Jingren Hou,Hong Wang,Pengyu Xu,Chang Gao,Huafeng Liu,Liping Jing*

Main category: cs.LG

TL;DR: 提出了首个从部分观测数据学习神经算子的系统框架，通过掩码预测训练策略和物理感知潜在传播器解决监督缺失和空间不匹配问题，在PDE任务中显著降低误差


<details>
  <summary>Details</summary>
Motivation: 现实科学应用中经常遇到不完整的观测数据，而现有神经算子方法假设完全观测空间输入，严重限制了在实际应用中的适用性，需要解决部分观测条件下的学习问题

Method: 提出了Latent Autoregressive Neural Operator (LANO)框架，包含两个核心组件：(1) 掩码预测训练策略，通过战略性地掩码观测区域创建人工监督；(2) 物理感知潜在传播器，在潜在空间中通过边界优先的自回归生成重建解。还开发了POBench-PDE基准测试

Result: 在缺失率低于50%的补丁式缺失情况下，在所有基准测试中实现了18-69%的相对L2误差降低，包括真实世界气候预测。方法有效处理了高达75%缺失率的实际场景

Conclusion: 该框架首次系统解决了从部分观测学习神经算子的问题，通过创新的训练策略和生成机制，在一定程度上弥合了理想化研究设置与现实科学计算复杂性之间的差距

Abstract: Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.

</details>


### [33] [Deep Learning for Perishable Inventory Systems with Human Knowledge](https://arxiv.org/abs/2601.15589)
*Xuan Liao,Zhenkang Peng,Ying Rong*

Main category: cs.LG

TL;DR: 该研究提出了一种基于深度学习的端到端易腐品库存管理方法，通过边际成本核算方案和启发式策略结构嵌入，在需求过程和提前期分布未知的情况下，利用有限历史数据和协变量直接学习订购策略。


<details>
  <summary>Details</summary>
Motivation: 易腐品库存管理面临需求不确定和提前期随机性的挑战，传统方法需要已知需求过程和提前期分布。在实际应用中，这些信息通常是未知的，且历史数据有限。需要开发能够利用有限数据和协变量直接学习最优订购策略的方法，同时提高学习效率。

Method: 采用边际成本核算方案，为每个订单分配单一生命周期成本，形成统一的端到端学习损失函数。开发了两种端到端变体：1) E2E-BB：纯黑盒方法，直接输出订购量；2) E2E-PIL：结构引导方法，嵌入投影库存水平(PIL)策略，通过显式计算而非额外学习来捕捉库存效应。进一步利用E2E-PIL目标函数的一阶齐次性，应用操作数据分析(ODA)中的提升技术，得到增强策略E2E-BPIL。

Result: 在合成数据和真实数据上的实验确立了稳健的性能排序：E2E-BB被E2E-PIL主导，而E2E-PIL又被E2E-BPIL进一步改进。通过超额风险分解分析表明，嵌入启发式策略结构降低了有效模型复杂度，仅以适度的灵活性损失为代价提高了学习效率。

Conclusion: 深度学习决策工具在人类知识引导下更加有效和稳健，强调了将高级分析与库存理论相结合的价值。结构引导方法通过减少模型复杂度和提高学习效率，在有限数据环境下表现出优越性能。

Abstract: Managing perishable products with limited lifetimes is a fundamental challenge in inventory management, as poor ordering decisions can quickly lead to stockouts or excessive waste. We study a perishable inventory system with random lead times in which both the demand process and the lead time distribution are unknown. We consider a practical setting where orders are placed using limited historical data together with observed covariates and current system states. To improve learning efficiency under limited data, we adopt a marginal cost accounting scheme that assigns each order a single lifetime cost and yields a unified loss function for end-to-end learning. This enables training a deep learning-based policy that maps observed covariates and system states directly to order quantities. We develop two end-to-end variants: a purely black-box approach that outputs order quantities directly (E2E-BB), and a structure-guided approach that embeds the projected inventory level (PIL) policy, capturing inventory effects through explicit computation rather than additional learning (E2E-PIL). We further show that the objective induced by E2E-PIL is homogeneous of degree one, enabling a boosting technique from operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL). Experiments on synthetic and real data establish a robust performance ordering: E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an excess-risk decomposition, we show that embedding heuristic policy structure reduces effective model complexity and improves learning efficiency with only a modest loss of flexibility. More broadly, our results suggest that deep learning-based decision tools are more effective and robust when guided by human knowledge, highlighting the value of integrating advanced analytics with inventory theory.

</details>


### [34] [Closing the Gap on the Sample Complexity of 1-Identification](https://arxiv.org/abs/2601.15620)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 本文研究了1-identification多臂老虎机问题，提出了新的下界和算法，解决了历史文献中多个合格臂情况下的期望停止时间分析问题。


<details>
  <summary>Details</summary>
Motivation: 1-identification是多臂老虎机中的一个基本探索问题，目标是确定是否存在平均奖励不低于已知阈值μ₀的合格臂。现有文献在多个合格臂情况下的期望停止时间分析存在空白，本文旨在填补这一理论缺口。

Method: 采用优化公式推导新的下界，并设计新算法获得紧上界。通过理论分析证明算法在所有问题实例上的性能与下界差距最多为对数因子的多项式。

Result: 推导出当至少存在一个合格臂时期望停止时间的新下界，设计了算法获得紧上界，上下界差距最多为对数因子的多项式，解决了多个合格臂情况下的理论分析问题。

Conclusion: 本文为1-identification问题提供了完整的理论分析框架，特别是解决了多个合格臂情况下的期望停止时间分析这一历史遗留问题，算法性能接近理论最优。

Abstract: 1-identification is a fundamental multi-armed bandit formulation on pure exploration. An agent aims to determine whether there exists a qualified arm whose mean reward is not less than a known threshold $μ_0$, or to output \textsf{None} if it believes such an arm does not exist. The agent needs to guarantee its output is correct with probability at least $1-δ$, while making expected total pulling times $\mathbb{E}τ$ as small as possible. We work on 1-identification with two main contributions. (1) We utilize an optimization formulation to derive a new lower bound of $\mathbb{E}τ$, when there is at least one qualified arm. (2) We design a new algorithm, deriving tight upper bounds whose gap to lower bounds are up to a polynomial of logarithm factor across all problem instance. Our result complements the analysis of $\mathbb{E}τ$ when there are multiple qualified arms, which is an open problem left by history literature.

</details>


### [35] [Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors](https://arxiv.org/abs/2601.15625)
*Zhiwei Zhang,Fei Zhao,Rui Wang,Zezhong Wang,Bin Liang,Jiakang Wang,Yao Hu,Shaosheng Cao,Kam-Fai Wong*

Main category: cs.LG

TL;DR: Fission-GRPO：一种将执行错误转化为纠正监督的强化学习框架，通过错误模拟器诊断反馈和在线重采样恢复轨迹，提升LLM在多轮工具调用中的错误恢复能力


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多轮工具调用中存在脆弱性：遇到工具调用错误时，小模型往往陷入重复无效调用，无法解释错误反馈并进行自我纠正。现有方法存在局限：标准RL将错误视为稀疏负奖励，不提供恢复指导；预收集的合成错误纠正数据集与模型在线错误模式存在分布不匹配问题

Method: 提出Fission-GRPO框架，核心机制是将每个失败轨迹"裂变"为新的训练实例：1）通过微调的错误模拟器生成诊断反馈；2）在策略上重采样恢复轨迹。这使得模型能够从探索过程中产生的具体错误中学习，而不是从静态的预收集错误案例中学习

Result: 在BFCL v4 Multi-Turn基准测试中，Fission-GRPO将Qwen3-8B的错误恢复率提升了5.7%绝对值，整体准确率从42.75%提升到46.75%（相对GRPO提升4%），优于专门的工具使用智能体

Conclusion: Fission-GRPO通过将执行错误转化为纠正监督，有效解决了LLM在多轮工具调用中的错误恢复问题，为实际部署提供了更可靠的错误处理机制

Abstract: Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.

</details>


### [36] [Dualformer: Time-Frequency Dual Domain Learning for Long-term Time Series Forecasting](https://arxiv.org/abs/2601.15669)
*Jingjing Bai,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Dualformer是一种用于长期时间序列预测的双域Transformer框架，通过分层频率采样和周期性感知加权机制解决传统Transformer的低通滤波效应，有效保留高频信息。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在长期时间序列预测中存在固有的低通滤波效应，这是由于各层间频率分量无差别传播导致高频信息逐渐衰减，限制了模型捕捉细粒度时间变化的能力。

Method: 提出Dualformer框架，包含三个关键组件：1）双分支架构同时建模时域和频域的互补时间模式；2）分层频率采样模块为不同层分配不同频段，在低层保留高频细节，在深层建模低频趋势；3）周期性感知加权机制基于输入的谐波能量比动态平衡双分支贡献。

Result: 在八个广泛使用的基准数据集上进行的大量实验表明，Dualformer具有鲁棒性和优越性能，特别是在异构或弱周期性数据上表现突出。

Conclusion: Dualformer通过结构化的频率建模和时频特征的自适应集成，有效解决了Transformer的低通滤波问题，增强了模型的泛化能力，为长期时间序列预测提供了更有效的解决方案。

Abstract: Transformer-based models, despite their promise for long-term time series forecasting (LTSF), suffer from an inherent low-pass filtering effect that limits their effectiveness. This issue arises due to undifferentiated propagation of frequency components across layers, causing a progressive attenuation of high-frequency information crucial for capturing fine-grained temporal variations. To address this limitation, we propose Dualformer, a principled dual-domain framework that rethinks frequency modeling from a layer-wise perspective. Dualformer introduces three key components: (1) a dual-branch architecture that concurrently models complementary temporal patterns in both time and frequency domains; (2) a hierarchical frequency sampling module that allocates distinct frequency bands to different layers, preserving high-frequency details in lower layers while modeling low-frequency trends in deeper layers; and (3) a periodicity-aware weighting mechanism that dynamically balances contributions from the dual branches based on the harmonic energy ratio of inputs, supported theoretically by a derived lower bound. This design enables structured frequency modeling and adaptive integration of time-frequency features, effectively preserving high-frequency information and enhancing generalization. Extensive experiments conducted on eight widely used benchmarks demonstrate Dualformer's robustness and superior performance, particularly on heterogeneous or weakly periodic data. Our code is publicly available at https://github.com/Akira-221/Dualformer.

</details>


### [37] [Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing](https://arxiv.org/abs/2601.15686)
*Xinyu Wang,Sicheng Lyu,Yu Gu,Jerry Huang,Peng Lu,Yufei Cui,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: RLSEdit：基于递归最小二乘的LLM顺序编辑方法，通过在线二次优化解决长期编辑中的可塑性-稳定性困境，支持万次编辑规模


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在长期顺序编辑中面临可塑性-稳定性困境：硬写入方法会积累干扰，硬保护方法只能保护显式约束部分，导致早期编辑被覆盖或未约束行为偏离，在多次编辑场景下会降低模型通用能力

Method: RLSEdit将编辑建模为带有软约束的在线二次优化问题，最小化累积键值拟合目标，包含两个正则项：控制与预训练权重的偏差，以及控制与指定锚映射的偏差。通过Woodbury恒等式实现高效在线递归，每次编辑成本与历史长度无关，仅与当前编辑规模相关

Result: 实验表明RLSEdit能够稳定扩展到10,000次编辑，在多个模型家族上优于基线方法，在编辑成功率和整体稳定性方面表现优异，能够保留早期编辑，并在GLUE和保留的推理/代码基准上保持通用能力

Conclusion: RLSEdit通过递归最小二乘框架有效解决了长期顺序编辑中的可塑性-稳定性困境，为大规模模型编辑提供了可扩展且稳定的解决方案

Abstract: Model editing updates a pre-trained LLM with new facts or rules without re-training, while preserving unrelated behavior. In real deployment, edits arrive as long streams, and existing editors often face a plasticity-stability dilemma: locate-then-edit "hard writes" can accumulate interference over time, while null-space-style "hard preservation" preserves only what is explicitly constrained, so past edits can be overwritten and unconstrained behaviors may deviate, degrading general capabilities in the many-edits regime. We propose RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit formulates editing as an online quadratic optimization with soft constraints, minimizing a cumulative key-value fitting objective with two regularizers that control for both deviation from the pre-trained weights and from a designated anchor mapping. The resulting update admits an efficient online recursion via the Woodbury identity, with per-edit cost independent of history length and scaling only with the current edit size. We further provide deviation bounds and an asymptotic characterization of the adherence-preservation trade-off in the many-edits regime. Experiments on multiple model families demonstrate stable scaling to 10K edits, outperforming strong baselines in both edit success and holistic stability -- crucially retaining early edits, and preserving general capabilities on GLUE and held-out reasoning/code benchmarks.

</details>


### [38] [Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs](https://arxiv.org/abs/2601.15714)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 论文提出了零误差范围（ZEH）概念来评估LLM的可信度，发现即使是先进模型如GPT-5.2在简单任务上也会出错，为安全关键应用提供重要警示。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂任务上表现出色，但在简单任务上仍会出错，这对安全关键应用构成风险。需要一种方法来量化模型的无错误解决范围，以评估其可信度。

Method: 提出零误差范围（ZEH）概念，定义为模型能够无错误解决的最大问题范围。通过评估GPT-5.2和Qwen2.5等先进模型的ZEH，分析其错误模式。使用树结构和在线softmax技术来降低计算成本。

Result: 发现GPT-5.2无法计算短字符串11000的奇偶性，也无法判断括号串((((())))))是否平衡。ZEH与准确率相关但行为模式不同，为算法能力涌现提供线索。通过优化技术实现了最多一个数量级的加速。

Conclusion: ZEH是评估LLM可信度的有效指标，揭示了即使是先进模型在简单任务上的局限性。这对安全关键应用有重要启示，同时提出的优化方法使ZEH评估更加实用。

Abstract: We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.

</details>


### [39] [Towards Automated Kernel Generation in the Era of LLMs](https://arxiv.org/abs/2601.15727)
*Yang Yu,Peiyu Zang,Chi Hsu Tsai,Haiming Wu,Yixin Shen,Jialing Zhang,Haoyu Wang,Zhiyou Xiao,Jingze Shi,Yuyu Luo,Wentao Zhang,Chunlei Men,Guang Liu,Yonghua Lin*

Main category: cs.LG

TL;DR: 关于LLM驱动内核生成与优化的系统性综述，涵盖LLM方法和智能体优化流程，提供数据集、基准测试、开放挑战和未来方向


<details>
  <summary>Details</summary>
Motivation: 现代AI系统性能受限于底层内核质量，内核工程需要硬件架构和编程模型的专家知识，过程耗时且难以扩展。LLM和LLM智能体为自动化内核生成和优化提供了新可能，但该领域缺乏系统性视角

Method: 提供结构化综述方法：1) 系统梳理现有LLM驱动内核生成方法；2) 涵盖LLM方法和智能体优化工作流程；3) 系统整理支撑学习和评估的数据集与基准测试；4) 维护开源GitHub仓库跟踪领域进展

Result: 建立了LLM驱动内核生成领域的系统性框架，包括方法分类、数据集整理、基准测试汇编，为自动化内核优化提供了全面参考

Conclusion: LLM和智能体系统在自动化内核生成和优化方面展现出巨大潜力，但需要系统性框架来整合碎片化进展。该综述填补了这一空白，为下一代自动化内核优化建立了全面参考

Abstract: The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.

</details>


### [40] [Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning](https://arxiv.org/abs/2601.15771)
*Dong Xu,Jiantao Wu,Qihua Pan,Sisi Yuan,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: GenRel-DDI是一个用于药物相互作用预测的关系学习框架，通过将DDI预测重新定义为关系中心学习问题，学习独立于药物身份的相互作用表示，从而显著提升对未见药物和新药物对的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于分子的DDI预测方法虽然在标准基准上表现良好，但在实际部署场景中泛化能力不足，特别是当大多数候选药物对涉及未见药物且已验证相互作用稀缺时。现有方法在嵌入空间中的邻近性并不能可靠地对应相互作用标签，单纯扩大模型容量无法改善泛化性能。

Method: 提出GenRel-DDI框架，将DDI预测重新定义为关系中心学习问题。该方法学习独立于药物身份的相互作用表示，通过关系级抽象捕获可转移的相互作用模式。这种设计使得模型能够泛化到未见药物和新药物对。

Result: 在多个基准测试中，GenRel-DDI一致且显著优于最先进的方法，特别是在严格的实体不相交评估中获得了特别大的性能提升，证明了关系学习对于稳健DDI预测的有效性和实际效用。

Conclusion: 关系中心学习方法能够有效解决DDI预测中的泛化问题，通过独立于药物身份学习相互作用表示，可以捕获可转移的相互作用模式，显著提升对未见药物和新药物对的预测性能。

Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.

</details>


### [41] [Next Generation Active Learning: Mixture of LLMs in the Loop](https://arxiv.org/abs/2601.15773)
*Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Guoxiang Guo,Joanne Enticott,Gang Liu,Lan Du*

Main category: cs.LG

TL;DR: 提出Mixture of LLMs in the Loop Active Learning框架，用多LLM混合标注模型替代人工标注，通过聚合多个LLM优势提升标注鲁棒性，结合标注差异性和负学习处理噪声标签，在轻量级LLM上实现与人工标注相当的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速发展和强大泛化能力，它们被越来越多地整合到主动学习流程中作为标注者以降低标注成本。然而，LLM生成的标注质量往往达不到实际应用要求，需要提升LLM-based标注的鲁棒性。

Method: 提出Mixture of LLMs in the Loop Active Learning框架：1）用基于多LLM混合的标注模型替代人工标注者；2）引入标注差异性（annotation discrepancy）识别不可靠标注；3）采用负学习（negative learning）增强学习效果；4）基于轻量级LLM构建，可在本地机器上运行。

Result: 实验表明：1）框架性能与人工标注相当；2）持续优于单LLM基线和其他LLM集成方法；3）基于轻量级LLM，可在实际应用中完全在本地机器上运行。

Conclusion: 提出的Mixture of LLMs in the Loop Active Learning框架通过聚合多个LLM优势、识别不可靠标注和增强学习效果，有效提升了LLM-based标注的鲁棒性，在降低标注成本的同时保持了与人工标注相当的性能。

Abstract: With the rapid advancement and strong generalization capabilities of large language models (LLMs), they have been increasingly incorporated into the active learning pipelines as annotators to reduce annotation costs. However, considering the annotation quality, labels generated by LLMs often fall short of real-world applicability. To address this, we propose a novel active learning framework, Mixture of LLMs in the Loop Active Learning, replacing human annotators with labels generated through a Mixture-of-LLMs-based annotation model, aimed at enhancing LLM-based annotation robustness by aggregating the strengths of multiple LLMs. To further mitigate the impact of the noisy labels, we introduce annotation discrepancy and negative learning to identify the unreliable annotations and enhance learning effectiveness. Extensive experiments demonstrate that our framework achieves performance comparable to human annotation and consistently outperforms single-LLM baselines and other LLM-ensemble-based approaches. Moreover, our framework is built on lightweight LLMs, enabling it to operate fully on local machines in real-world applications.

</details>


### [42] [Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models](https://arxiv.org/abs/2601.15801)
*Fengheng Chu,Jiahao Chen,Yuhong Wang,Jun Wang,Zhihui Fu,Shouling Ji,Songze Li*

Main category: cs.LG

TL;DR: GOSV框架通过全局优化识别LLM中的安全关键注意力头，发现恶意注入向量和安全抑制向量两种空间分离的安全向量，并基于此开发了新型白盒越狱攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全护栏在越狱攻击下仍显脆弱，表明对安全机制组件的理解有限。现有方法依赖局部贪婪归因，忽略了注意力头等组件间的协同交互作用。

Method: 提出GOSV框架，通过全局优化同时识别所有注意力头中的安全关键头。采用两种互补的激活重补丁策略：有害补丁和零消融，识别空间分离的安全向量。

Result: 发现对齐LLM维护着分离的功能通路用于安全目的，识别出恶意注入向量和安全抑制向量。当约30%的总头被重补丁时，所有模型都会出现完全安全崩溃。基于此开发的新型白盒越狱攻击在所有测试模型上显著优于现有方法。

Conclusion: GOSV框架在LLM安全可解释性方面具有有效性，揭示了安全机制中组件协同作用的重要性，并为理解LLM安全脆弱性提供了新视角。

Abstract: While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for \textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies safety-critical attention heads through global optimization over all heads simultaneously. We employ two complementary activation repatching strategies: Harmful Patching and Zero Ablation. These strategies identify two spatially distinct sets of safety vectors with consistently low overlap, termed Malicious Injection Vectors and Safety Suppression Vectors, demonstrating that aligned LLMs maintain separate functional pathways for safety purposes. Through systematic analyses, we find that complete safety breakdown occurs when approximately 30\% of total heads are repatched across all models. Building on these insights, we develop a novel inference-time white-box jailbreak method that exploits the identified safety vectors through activation repatching. Our attack substantially outperforms existing white-box attacks across all test models, providing strong evidence for the effectiveness of the proposed GOSV framework on LLM safety interpretability.

</details>


### [43] [Why Inference in Large Models Becomes Decomposable After Training](https://arxiv.org/abs/2601.15871)
*Jidong Jin*

Main category: cs.LG

TL;DR: 提出了一种后训练统计准则和结构退火方法，通过移除未支持的参数依赖关系，揭示稳定独立的子结构，实现无需修改模型功能或接口的结构化并行推理。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的推理通常在密集参数矩阵上进行，导致推理成本和系统复杂度随模型规模不可持续增长。这种限制并非源于模型容量不足，而是由于将后训练推理系统视为单一算子，忽略了学习过程中形成的内部结构。

Method: 研究发现大型模型中的梯度更新事件高度局部化和选择性，许多参数依赖关系在训练后与其初始化分布在统计上无法区分。基于此，提出了后训练统计准则和结构退火程序，移除未支持的依赖关系，揭示稳定独立的子结构。

Result: 建立了后训练、模型无关的推理系统结构视图，实现了结构化并行推理，无需修改模型功能或接口。

Conclusion: 通过识别和利用大型模型中的内在结构可分解性，可以显著降低推理成本，同时保持模型功能不变，为大规模AI模型的可持续部署提供了新途径。

Abstract: Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.

</details>


### [44] [Iterative Amortized Hierarchical VAE](https://arxiv.org/abs/2601.15894)
*Simon W. Penninga,Ruud J. G. van Sloun*

Main category: cs.LG

TL;DR: IA-HVAE结合摊销推理与迭代优化，通过可分离解码器实现35倍加速，在逆问题中表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统分层变分自编码器（HVAE）在推理时面临效率与精度的权衡：完全摊销推理速度快但精度有限，完全迭代推理精度高但计算成本大。需要一种混合方案来平衡速度与精度。

Method: 提出迭代摊销分层变分自编码器（IA-HVAE），采用混合推理方案：先进行摊销推理生成初始猜测，然后利用解码器梯度进行迭代优化。关键创新是设计在变换域（如傅里叶空间）中线性可分离的解码器，支持实时应用和高模型深度。

Result: IA-HVAE相比传统HVAE实现35倍迭代推理加速。在精度和速度上分别优于完全摊销和完全迭代方法。在去模糊和去噪等逆问题中，重建质量优于原始HVAE。

Conclusion: IA-HVAE通过混合摊销-迭代推理方案和可分离解码器设计，有效平衡了推理速度与精度，为高深度模型的实时应用提供了可行方案。

Abstract: In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.

</details>


### [45] [Partially Lazy Gradient Descent for Smoothed Online Learning](https://arxiv.org/abs/2601.15984)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: k-lazyGD算法在平滑在线凸优化中建立了懒惰性与动态后悔之间的权衡，允许在保持最优追踪性能的同时实现不同程度的稳定性


<details>
  <summary>Details</summary>
Motivation: 传统在线梯度下降（OGD）具有反应性更新但稳定性差，而懒惰GD/双重平均方法稳定但追踪能力有限。需要一种能在这两个极端之间连续调节的算法，在保持最优动态后悔的同时实现不同程度的懒惰性

Method: 提出k-lazyGD算法，通过参数k在贪婪OGD（k=1）和懒惰GD（k=T）之间创建连续谱。基于FTRL框架进行分析，证明算法能在保持最优动态后悔的前提下实现不同程度的懒惰性

Result: k-lazyGD对任何懒惰性松弛k ≤ Θ(√T/P_T)都能达到最优动态后悔O(√(P_T+1)T)，其中P_T是比较器路径长度。建立了懒惰性与比较器变化之间的形式化联系，并给出了匹配的下界

Conclusion: 懒惰性可以在不牺牲命中性能的情况下实现，懒惰性松弛k与比较器路径长度P_T相关。通过使用不同松弛参数的集成学习器，可以得到在可能时稳定、在必要时敏捷的自适应方法

Abstract: We introduce $k$-lazyGD, an online learning algorithm that bridges the gap between greedy Online Gradient Descent (OGD, for $k=1$) and lazy GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable updates. We analyze this spectrum in Smoothed Online Convex Optimization (SOCO), where the learner incurs both hitting and movement costs. Our main contribution is establishing that laziness is possible without sacrificing hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to $Θ(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result formally connects the allowable laziness to the comparator's shifts, showing that $k$-lazyGD can retain the inherently small movements of lazy methods without compromising tracking ability. We base our analysis on the Follow the Regularized Leader (FTRL) framework, and derive a matching lower bound. Since the slack depends on $P_T$, an ensemble of learners with various slacks is used, yielding a method that is provably stable when it can be, and agile when it must be.

</details>


### [46] [Data-Driven Conditional Flexibility Index](https://arxiv.org/abs/2601.16028)
*Moritz Wedemeyer,Eike Cramer,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 提出条件灵活性指数（CFI），通过从历史数据学习参数化可容许不确定性集，并利用上下文信息使其条件化，从而更准确地评估调度灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统灵活性指数使用简单几何形状（如超立方体）近似可容许不确定性区域，未考虑可用的上下文信息（如预测数据），导致灵活性评估不够精确。需要开发能够利用历史数据和条件信息的方法来定义更相关的可容许不确定性集。

Method: 提出条件灵活性指数（CFI），使用归一化流学习从高斯基分布到数据分布的双射映射。在潜在空间中构建超球体作为可容许潜在不确定性集，然后映射到数据空间。通过纳入上下文信息，使可容许不确定性集条件化，从而在给定条件下定义更可能相关的区域。

Result: 通过示例表明，数据驱动的可容许不确定性集不一定总是优于简单集合，条件集也不一定总是优于无条件集。但两者都能确保只考虑包含实际实现的参数空间区域。在安全约束机组组合案例中，CFI通过纳入时间信息提高了调度质量。

Conclusion: 条件灵活性指数通过从历史数据学习参数化可容许不确定性集并利用上下文信息使其条件化，提供了更灵活的灵活性评估方法。虽然不能一概而论地认为数据驱动或条件方法总是更优，但它们能确保只考虑实际相关的参数区域，在特定应用（如机组组合）中能显著提高调度质量。

Abstract: With the increasing flexibilization of processes, determining robust scheduling decisions has become an important goal. Traditionally, the flexibility index has been used to identify safe operating schedules by approximating the admissible uncertainty region using simple admissible uncertainty sets, such as hypercubes. Presently, available contextual information, such as forecasts, has not been considered to define the admissible uncertainty set when determining the flexibility index. We propose the conditional flexibility index (CFI), which extends the traditional flexibility index in two ways: by learning the parametrized admissible uncertainty set from historical data and by using contextual information to make the admissible uncertainty set conditional. This is achieved using a normalizing flow that learns a bijective mapping from a Gaussian base distribution to the data distribution. The admissible latent uncertainty set is constructed as a hypersphere in the latent space and mapped to the data space. By incorporating contextual information, the CFI provides a more informative estimate of flexibility by defining admissible uncertainty sets in regions that are more likely to be relevant under given conditions. Using an illustrative example, we show that no general statement can be made about data-driven admissible uncertainty sets outperforming simple sets, or conditional sets outperforming unconditional ones. However, both data-driven and conditional admissible uncertainty sets ensure that only regions of the uncertain parameter space containing realizations are considered. We apply the CFI to a security-constrained unit commitment example and demonstrate that the CFI can improve scheduling quality by incorporating temporal information.

</details>


### [47] [CLASP: An online learning algorithm for Convex Losses And Squared Penalties](https://arxiv.org/abs/2601.16072)
*Ricardo N. Ferreira,Cláudia Soares,João Xavier*

Main category: cs.LG

TL;DR: CLASP算法在约束在线凸优化中同时最小化累积损失和平方约束违反，首次为强凸问题提供了对数级别的遗憾和累积平方惩罚保证。


<details>
  <summary>Details</summary>
Motivation: 研究约束在线凸优化问题，其中学习者在每次迭代中面临未预期的凸损失和凸约束，需要最小化累积损失同时控制约束违反。现有方法未能充分利用凸投影算子的非扩张性，且缺乏对强凸问题的对数级别保证。

Method: 提出CLASP算法，通过凸投影算子的严格非扩张性这一新颖证明策略，同时最小化累积损失和平方约束违反。算法适用于任意β∈(0,1)参数选择，平衡遗憾和惩罚项。

Result: 对于凸损失，CLASP获得遗憾O(T^{max{β,1-β}})和累积平方惩罚O(T^{1-β})；对于强凸问题，首次实现遗憾O(log T)和累积平方惩罚O(log T)的对数级别保证。

Conclusion: CLASP算法通过利用凸投影算子的严格非扩张性，在约束在线凸优化中取得了理论突破，特别是为强凸问题提供了首个对数级别的遗憾和约束违反保证。

Abstract: We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.

</details>


### [48] [Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2601.16074)
*Annemarie Jutte,Uraz Odyurt*

Main category: cs.LG

TL;DR: 应用可解释AI（XAI）分析工业信息物理系统中机器学习模型的预测行为，通过SHAP值分析时间序列分解组件对预测的影响，发现训练数据上下文信息不足，通过增加数据窗口大小提升模型性能


<details>
  <summary>Details</summary>
Motivation: 工业信息物理系统（CPS）对安全和经济效益至关重要，需要高度可靠性。虽然机器学习（特别是深度学习）越来越多地集成到工业CPS中，但ML模型的固有复杂性导致其操作不透明。需要严格评估以防止模型在未来未见数据上表现出意外行为。可解释AI（XAI）可用于揭示模型推理过程，从而进行更全面的行为分析。

Method: 应用可解释AI（XAI）来改进工业CPS中ML模型的预测性能。使用SHAP值分析时间序列数据分解组件对模型预测的影响。通过这种方法，观察到模型训练期间缺乏足够上下文信息的证据。根据XAI发现，通过增加数据实例的窗口大小来改进模型性能。

Result: 通过XAI分析发现模型训练中上下文信息不足的问题，基于SHAP值的分析为模型改进提供了依据。通过增加数据窗口大小，成功提升了模型性能。

Conclusion: 可解释AI（XAI）是分析和改进工业CPS中机器学习模型性能的有效工具。通过SHAP值分析时间序列分解组件的影响，可以识别模型训练的局限性（如上下文信息不足），并据此采取针对性改进措施（如增加数据窗口大小），从而提升模型的预测性能和可靠性。

Abstract: Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.

</details>


### [49] [Probably Approximately Correct Maximum A Posteriori Inference](https://arxiv.org/abs/2601.16083)
*Matthew Shorvon,Frederik Mallmann-Trenn,David S. Watson*

Main category: cs.LG

TL;DR: 提出用于MAP推断的PAC算法，在可变和固定计算预算下提供可证明的最优解，通过概率电路实现，并为启发式方法提供严格保证


<details>
  <summary>Details</summary>
Motivation: MAP估计是概率推断中的基本任务，但通常难以计算，即使在许多常见的结构约束和近似方案下仍然困难。需要开发具有理论保证的高效MAP推断方法。

Method: 引入PAC-MAP算法，使用信息论度量表征可处理性条件，通过概率电路高效实现，开发随机化策略作为独立MAP推断技术或改进现有启发式方法

Result: 在多种基准测试中验证了方法的有效性，能够为MAP推断提供可证明的最优解，并强化启发式方法的解决方案

Conclusion: PAC-MAP算法为MAP推断提供了具有理论保证的实用解决方案，通过概率电路实现和随机化策略，能够在计算预算约束下获得最优解

Abstract: Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.

</details>


### [50] [Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets](https://arxiv.org/abs/2601.16107)
*Adithya Sineesh,Akshita Kamsali*

Main category: cs.LG

TL;DR: 首次系统性地对多个专门针对拉曼光谱设计的深度学习分类器在共享开源数据集上进行基准测试，比较了五种代表性架构在三种数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前拉曼光谱深度学习分类器的评估往往孤立进行，或仅与传统机器学习方法或简单适配的视觉架构比较，缺乏专门针对拉曼光谱设计的深度学习模型之间的直接对比。

Method: 采用统一的训练和超参数调优协议，在三种开源拉曼数据集上评估五种代表性深度学习架构，支持标准评估、微调和显式分布偏移测试。

Result: 报告了分类准确率和宏平均F1分数，为拉曼光谱分类的深度学习模型提供了公平且可复现的比较结果。

Conclusion: 该研究填补了拉曼光谱深度学习模型系统性比较的空白，为后续研究提供了可靠的基准参考。

Abstract: Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.

</details>


### [51] [On the Intrinsic Dimensions of Data in Kernel Learning](https://arxiv.org/abs/2601.16139)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: 该论文研究了流形假设下核岭回归的泛化性能，提出了两种内在维度定义：基于核诱导度量的Minkowski维度和基于Kolmogorov n-宽度的有效维度，并建立了它们与泛化误差的关系。


<details>
  <summary>Details</summary>
Motivation: 流形假设认为当输入分布支撑集的本质维度较低时，机器学习方法的泛化性能会显著提升。本文旨在在核岭回归框架下，研究两种不同的本质维度定义及其对泛化误差的影响，特别是对于不规则域（如分形集）的情况。

Method: 1. 定义两种本质维度：$d_ρ$（基于核诱导度量的Minkowski维度）和$d_K$（基于Kolmogorov n-宽度的有效维度）
2. 分析Kolmogorov n-宽度与积分算子特征值的关系
3. 证明Kolmogorov n-宽度刻画了所有支撑在Ω上的概率测度的最坏情况特征值衰减
4. 提出从有限样本估计n-宽度上界的算法
5. 计算各种分形集的有效维度$d_K$并进行数值实验

Result: 1. 建立了Kolmogorov n-宽度与积分算子特征值衰减的关系
2. 推导了约束核岭回归的过拟合误差上界$O(n^{-\frac{2+d_K}{2+2d_K} + ε})$
3. 证明了对于接近均匀的分布，使用$O\left(ε^{-d_ρ}\log\frac{1}ε\right)$个样本可以高概率计算所有n-宽度的ε-准确上界
4. 对于Laplace核等核函数，有效维度$d_K$可以显著小于Minkowski维度$d_ρ$，尽管在规则域上两者相等

Conclusion: 本文为核岭回归的泛化分析提供了基于本质维度的理论框架，证明了有效维度$d_K$比Minkowski维度$d_ρ$更能准确刻画泛化性能，特别是在不规则域上。提出的样本复杂度结果和估计算法为实际应用提供了理论指导。

Abstract: The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε> 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.

</details>


### [52] [Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets](https://arxiv.org/abs/2601.16147)
*Muhammad Ilham Rizqyawan,Peter Macfarlane,Stathis Hadjidemetriou,Fani Deligianni*

Main category: cs.LG

TL;DR: Beat-SSL是一种针对心电图的对比学习框架，通过节奏级和心跳级的双重上下文学习与软目标对比，解决了心电图数据标注困难和现有方法未能充分利用心电图特性的问题。


<details>
  <summary>Details</summary>
Motivation: 获取标注心电图数据用于监督模型训练具有挑战性。现有对比学习框架要么只关注全局上下文，要么未能充分利用心电图特定特性，且依赖硬对比目标，无法充分捕捉心电图信号中特征相似性的连续性。

Method: 提出Beat-SSL对比学习框架，通过节奏级和心跳级的双重上下文学习，采用软目标进行对比，以更好地捕捉心电图信号的连续特征相似性。

Result: 在多标签分类任务中，Beat-SSL达到了心电图基础模型93%的性能；在分割任务中，超越了所有其他方法4%。

Conclusion: Beat-SSL通过双重上下文学习和软目标对比，有效解决了心电图数据标注困难的问题，在有限标注数据下实现了有效的迁移学习，特别是在分割任务中表现优异。

Abstract: Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model's broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.

</details>


### [53] [Learning to Discover at Test Time](https://arxiv.org/abs/2601.16175)
*Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun*

Main category: cs.LG

TL;DR: TTT-Discover：通过测试时强化学习让LLM针对特定问题持续训练，在多个科学领域取得新的SOTA结果


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法（如AlphaEvolve）使用冻结的LLM进行搜索，无法针对特定测试问题进行持续学习。需要一种方法让LLM在测试时能够基于问题特定经验继续训练，专注于生成单个优秀解决方案而非平均性能。

Method: 提出TTT-Discover方法，在测试时进行强化学习，让LLM能够针对特定问题持续训练。设计专门的学习目标和搜索子程序，优先探索最有希望的解决方案。使用OpenAI gpt-oss-120b开源模型，通过Tinker API进行测试时训练。

Result: 在数学（Erdős最小重叠问题、自相关不等式）、GPU内核工程（比现有技术快2倍）、算法设计（AtCoder竞赛）、生物学（单细胞分析去噪问题）等多个领域几乎都取得了新的SOTA结果。所有结果均使用开源模型实现，成本仅为每个问题几百美元。

Conclusion: TTT-Discover通过测试时训练成功发现了多个科学问题的新SOTA解决方案，证明了在测试时进行持续学习的有效性，且使用开源模型即可超越之前需要闭源前沿模型才能达到的结果。

Abstract: How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

</details>


### [54] [Counterfactual Training: Teaching Models Plausible and Actionable Explanations](https://arxiv.org/abs/2601.16205)
*Patrick Altmeyer,Aleksander Buszydlik,Arie van Deursen,Cynthia C. S. Liem*

Main category: cs.LG

TL;DR: 提出一种新颖的反事实训练方法，利用反事实解释增强模型的可解释性，使模型在训练阶段就生成合理且可操作的反事实解释，同时提高对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法多为后处理技术，虽然能生成合理且可操作的反事实解释，但模型本身并未被训练来产生这些解释。研究旨在让模型在训练阶段就直接对期望的最终目标负责，即生成高质量的反事实解释。

Method: 提出反事实训练方法，在训练阶段使用反事实解释来最小化学得表示与合理、可操作解释之间的差异。该方法将反事实解释整合到训练过程中，使模型能够学习生成符合数据分布和特征可变性约束的反事实。

Result: 通过实证和理论分析证明，该方法能够训练出能够提供内在理想反事实解释的模型，同时模型表现出改进的对抗鲁棒性。

Conclusion: 反事实训练是一种有效的训练范式，能够提升模型的可解释性和鲁棒性，为构建更透明、可靠的决策系统提供了新途径。

Abstract: We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.

</details>


<div id='Alekh Jindal'></div>

# Alekh Jindal [[Back]](#toc)

### [55] [Konflux: Optimized Function Fusion for Serverless Applications](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.11156&hl=zh-CN&sa=X&d=17202262798157802628&ei=SHJyafrXKpTM6rQP2piuyQ0&scisig=AHkA5jQe7LZTOMwOKnwhKokvW3_o&oi=scholaralrt&hist=i6heNjgAAAAJ:4438704070979798767:AHkA5jShId-iWoaohWFlDvz-Dwie&html=&pos=0&folt=rel)
*N Kowallik,T Schirmer,D Bermbach*

Main category: Alekh Jindal

TL;DR: 论文探讨了在函数即服务(FaaS)环境中通过函数融合技术优化部署的方法，将多个函数组合成单一部署单元以提升性能


<details>
  <summary>Details</summary>
Motivation: 函数即服务已成为无服务器云计算的核心范式，但优化FaaS部署仍然具有挑战性。现有方法在函数间通信、冷启动延迟和资源利用效率方面存在不足，需要更有效的优化策略

Method: 提出函数融合方法，将多个相关函数组合成单一部署单元。该方法通过分析函数间的依赖关系和调用模式，智能地融合函数以减少通信开销和冷启动延迟

Result: 函数融合显著降低了函数间通信延迟，减少了冷启动次数，提高了资源利用效率。实验表明该方法在典型FaaS工作负载下能带来显著的性能提升

Conclusion: 函数融合是优化FaaS部署的有效方法，能够解决无服务器计算中的关键性能瓶颈。该方法为构建更高效的FaaS平台提供了新的技术路径

Abstract: Function-as-a-Service (FaaS) has become a central paradigm in serverless cloud computing, yet optimizing FaaS deployments remains challenging. Using function fusion, multiple functions can be combined into a single deployment unit, which can …

</details>


### [56] [Generating a detuned decision tree for entity selection](https://scholar.google.com/scholar_url?url=https://patents.google.com/patent/US12517881B2/en&hl=zh-CN&sa=X&d=16812681277632318420&ei=SnJyacGoBbuM6rQP2dOq-Ag&scisig=AHkA5jQM7M35z08JUMmBZiEX6mQl&oi=scholaralrt&hist=i6heNjgAAAAJ:9105223062713277753:AHkA5jRjsInJuQ8ojOPRhEA3nNaJ&html=&pos=0&folt=cit)
*VG Willett,DW Gold,GS Fidler*

Main category: Alekh Jindal

TL;DR: 基于特征元数据选择可训练特征子集，构建决策树预测目标特征，通过叶节点非目标实体数量筛选节点子集


<details>
  <summary>Details</summary>
Motivation: 解决从大量特征中选择有效子集进行预测的问题，提高目标特征预测的准确性和效率，减少计算复杂度

Method: 1. 基于特征元数据选择可训练特征子集；2. 训练决策树预测目标特征；3. 确定每个叶节点中不具目标特征的实体数量；4. 基于数量标准筛选叶节点子集

Result: 该方法能够有效识别具有预测能力的特征子集，通过叶节点筛选优化决策树结构，提高目标特征预测性能

Conclusion: 基于特征元数据的特征选择结合决策树训练和叶节点筛选，是一种有效的目标特征预测方法，可应用于实体分类和特征识别任务

Abstract: A subset of trainable features from a set of features is selected based at least in part on feature metadata. A trained decision tree is generated to predict which entities of a plurality of entities have a target feature. A corresponding quantity of entities of the plurality of entities that do not have the target feature is determined for each leaf node in the trained decision tree. A subset of the plurality of leaf nodes are selected wherein each leaf node in the subset of leaf nodes has a corresponding quantity of …

</details>


### [57] [Attribute-level access control for federated queries](https://scholar.google.com/scholar_url?url=https://patents.google.com/patent/US12505246B2/en&hl=zh-CN&sa=X&d=9840692633708553407&ei=SnJyacGoBbuM6rQP2dOq-Ag&scisig=AHkA5jSrcAC2Y4Z2Bp533r8DxrKg&oi=scholaralrt&hist=i6heNjgAAAAJ:9105223062713277753:AHkA5jRjsInJuQ8ojOPRhEA3nNaJ&html=&pos=1&folt=cit)
*S Srinivasan,P Natarajan,LF Joreteg…*

Main category: Alekh Jindal

TL;DR: 基于联邦查询属性实现数据访问控制的技术方案


<details>
  <summary>Details</summary>
Motivation: 解决联邦查询系统中多第三方数据源的访问控制问题，确保数据安全性和合规性

Method: 通过用户标识确定访问控制策略，生成包含可执行任务的执行计划来解析联邦查询

Result: 实现了基于属性的细粒度访问控制机制，能够安全地处理跨多个第三方数据源的联邦查询

Conclusion: 该技术为联邦查询系统提供了有效的访问控制框架，确保数据访问的安全性和合规性

Abstract: Various embodiments of the present disclosure provide access control for data provided by a federated query system based on attributes of a federated query. The techniques include determining a user identifier associated with a federated query, determining a set of access controls for a plurality of third-party data sources based on the user identifier, and generating an execution plan for resolving the federated query via one or more executable tasks with respect to the plurality of third-party data …

</details>


<div id='Surajit Chaudhuri'></div>

# Surajit Chaudhuri [[Back]](#toc)

### [58] [Budget-Aware Anytime Reasoning with LLM-Synthesized Preference Data](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.11038&hl=zh-CN&sa=X&d=640697535629122724&ei=SnJyaeKGGNDQ6rQPx77Y4A8&scisig=AHkA5jRnv2Bx3BU_qo5PVO8_sN3Q&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=1&folt=rel)
*X Zhang,S Ashrafi,A Mirsaidova,A Rezaeian…*

Main category: Surajit Chaudhuri

TL;DR: 研究大型语言模型在有限计算预算下的推理行为，强调快速生成有用部分解比高成本详尽推理更实用


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理成本高昂，实际应用中常面临有限计算预算，需要理解模型在资源受限时的推理行为模式

Method: 研究LLMs在有限计算设置下的推理行为，分析部分解生成与详尽推理的权衡

Result: 在计算受限场景中，快速生成有用部分解比追求完整详尽推理更具实用性

Conclusion: 实际部署中应重视LLMs在有限计算下的部分解生成能力，而非仅关注完美推理

Abstract: We study the reasoning behavior of large language models (LLMs) under limited computation budgets. In such settings, producing useful partial solutions quickly is often more practical than exhaustive reasoning, which incurs high inference costs …

</details>


### [59] [MITRA: A Large-Scale Parallel Corpus and Multilingual Pretrained Language Model for Machine Translation and Semantic Retrieval for P\= ali, Sanskrit, Buddhist …](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.06400&hl=zh-CN&sa=X&d=622635144511580027&ei=SnJyaeKGGNDQ6rQPx77Y4A8&scisig=AHkA5jQbj-_KQ2Dk3yzA9gP5Abc1&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=3&folt=rel)
*S Nehrdich,K Keutzer*

Main category: Surajit Chaudhuri

TL;DR: 开发跨语言文本并行检测系统，用于自动识别佛教文献中的文本平行段落


<details>
  <summary>Details</summary>
Motivation: 古代佛教文献中存在大量未标注的跨语言文本平行段落，涵盖梵语、巴利语、佛教汉语、藏语等多种语言，由于规模庞大，人工检查不可行

Method: 提出跨语言文本并行检测系统，利用计算方法自动识别佛教文献中的文本平行段落

Result: 开发出能够处理多语言佛教文献的自动检测系统，显著提高文本平行段落的识别效率

Conclusion: 计算方法为大规模佛教文献研究提供了可行工具，能够有效识别跨语言文本平行关系

Abstract: Ancient Buddhist literature features frequent, yet often unannotated, textual parallels spread across diverse languages: Sanskrit, P\= ali, Buddhist Chinese, Tibetan, and more. The scale of this material makes manual examination prohibitive. We present …

</details>


### [60] [BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.20403&hl=zh-CN&sa=X&d=11422369626604449533&ei=SnJyaeKGGNDQ6rQPx77Y4A8&scisig=AHkA5jR16Smh7NwCtLPWvNV4HsmV&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=4&folt=rel)
*XA Le,MN Tran,S Nguyen*

Main category: Surajit Chaudhuri

TL;DR: 大模型到小模型的知识蒸馏面临容量差距陷阱，需要新的蒸馏策略


<details>
  <summary>Details</summary>
Motivation: 大型专有模型（如GPT-4）到小型可部署模型（小于1B参数）的知识蒸馏面临1000倍容量差距的"容量-预算陷阱"，传统直接蒸馏方法无法有效工作

Method: 论文未在摘要中明确说明具体方法，但暗示需要新的蒸馏策略来解决容量差距问题

Result: 摘要未提供具体实验结果，但指出传统直接知识蒸馏方法在极端容量差距下失效

Conclusion: 需要开发新的知识蒸馏策略来弥合大型专有模型与小型可部署模型之间的巨大容量差距

Abstract: Distilling knowledge from large proprietary models (eg, GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while …

</details>


### [61] [Topic-guided debiased contrastive learning for neural topic modeling](https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0925231225031959&hl=zh-CN&sa=X&d=8332756107883873344&ei=SnJyaeKGGNDQ6rQPx77Y4A8&scisig=AHkA5jTt7G04DLWqitJj_34rrc42&oi=scholaralrt&hist=i6heNjgAAAAJ:10938430069730194209:AHkA5jT2_OB4x2gjQM46vAXf9pM1&html=&pos=5&folt=rel)
*Y Liu,Z Gong*

Main category: Surajit Chaudhuri

TL;DR: 对比学习被引入神经主题模型以提升主题连贯性，但现有方法受限于主题提取过程中的采样偏差，主要面临两个关键挑战


<details>
  <summary>Details</summary>
Motivation: 现有结合对比学习的神经主题模型存在采样偏差问题，这限制了主题提取的质量和效果，需要解决这一局限性以提升主题建模性能

Method: 论文提出了一种新的方法来克服主题提取中的采样偏差问题，但具体方法细节在提供的摘要中未完整展示

Result: 方法预期能够有效缓解采样偏差，提升主题连贯性和模型性能，但具体实验结果在提供的摘要片段中未提及

Conclusion: 解决对比学习神经主题模型中的采样偏差问题对于提升主题建模质量至关重要，提出的方法有望在这一方向上取得进展

Abstract: Recently, contrastive learning has been incorporated into neural topic models to enhance latent topic coherence. However, existing approaches are limited by sampling bias during topic extraction, primarily due to two key challenges: i) word …

</details>


<div id='Carsten Binnig'></div>

# Carsten Binnig [[Back]](#toc)

### [62] [Towards a Controllable and Realistic Entity Matching Benchmark](https://scholar.google.com/scholar_url?url=https://dbdbd2025.uantwerpen.be/wp-content/uploads/2025/12/dbdbd25_27.pdf&hl=zh-CN&sa=X&d=15955405778814568620&ei=S3JyacyCD9DQ6rQPx77Y4A8&scisig=AHkA5jSZVzJgbL2PD9vgQOl3McaH&oi=scholaralrt&hist=i6heNjgAAAAJ:15269883191641703195:AHkA5jTZCYhse3wIcZXHnO-iboY0&html=&pos=0&folt=cit)
*X Li,Z Zhang,S Schelter,P Groth,M Hulsebos*

Main category: Carsten Binnig

TL;DR: 该论文指出现有实体匹配基准存在三个主要局限性：静态不灵活、缺乏真实世界复杂性、以及难以评估模型鲁棒性，并提出需要更动态、可控的基准来支持鲁棒模型评估。


<details>
  <summary>Details</summary>
Motivation: 现有实体匹配基准存在三个关键问题：1) 静态且不灵活，无法适应不同领域或进行可控的数据扰动操作；2) 未能充分反映真实世界的复杂性；3) 难以有效评估模型的鲁棒性。这些问题阻碍了对实体匹配模型的稳健评估。

Method: 论文通过分析现有基准的局限性，提出了对更动态、可控的实体匹配基准的需求。虽然没有详细描述具体方法，但暗示需要能够适应不同领域、允许可控数据扰动操作、并能更好反映真实世界复杂性的基准设计。

Result: 分析揭示了现有实体匹配基准的三个核心缺陷：静态性、缺乏真实复杂性和鲁棒性评估不足。这为开发更先进的基准系统提供了理论基础和需求依据。

Conclusion: 需要开发新的实体匹配基准，这些基准应具备动态性、领域适应性、可控的数据扰动能力，并能更好地反映真实世界复杂性，以支持更鲁棒的模型评估。

Abstract: Entity Matching (EM), the task of identifying records that refer to the same real-world entities across heterogeneous data sources, is a fundamental component of data preparation. However, existing EM benchmarks [1, 2] suffer from three limitations that obstruct robust model evaluation. First, they are often static and inflexible, lacking the ability to adapt to different domains, or allow for controllable manipulation of data disturbances. Second, they often fail to reflect real-world complexity, especially in an …

</details>


<div id='Zongheng Yang'></div>

# Zongheng Yang [[Back]](#toc)

### [63] [SAM Translator: Self-Paced Learning And Mixture-of-Experts for Cross-Lingual Text-to-Speech Translation](https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/iel8/6287639/11323511/11300906.pdf&hl=zh-CN&sa=X&d=17271867320919848589&ei=SHJyaZ6dEsm4ieoPof3esQc&scisig=AHkA5jQhmEk_QtFOpkdweEJM60NE&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=0&folt=cit)
*C Tran,S Sakti*

Main category: Zongheng Yang

TL;DR: 提出SAM-Translator模型，解决文本到语音翻译任务中建模语言和声学多样性的挑战


<details>
  <summary>Details</summary>
Motivation: 现有翻译研究主要关注文本到文本或语音到文本任务，文本到语音翻译相对未被充分探索，尽管在文化融合和教育中具有核心作用。现有的T2ST系统在建模高资源和低资源语言的语言和声学多样性方面仍面临重大挑战。

Method: 提出SAM-Translator模型，具体方法未在摘要中详细说明，但旨在解决语言和声学多样性建模问题

Result: 摘要中未提供具体实验结果，但暗示该模型旨在改进现有T2ST系统的性能

Conclusion: SAM-Translator为解决文本到语音翻译中的语言和声学多样性挑战提供了新的解决方案

Abstract: Most existing translation studies have concentrated on text-to-text or speech-to-text tasks, whereas text-to-speech translation (T2ST) remains relatively underexplored, despite its central role in cultural integration and education. Among the few available T2ST systems, recent end-to-end approaches still face significant challenges in modeling the linguistic and acoustic diversity that exists in both high-resource and low-resource languages. To address these challenges, we propose SAM-Translator …

</details>


### [64] [Unifying Speech Recognition, Synthesis and Conversion with Autoregressive Transformers](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.10770&hl=zh-CN&sa=X&d=8652590826005105259&ei=SHJyaZ6dEsm4ieoPof3esQc&scisig=AHkA5jToENqyD0VLH0QMBKOc_BZg&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=1&folt=cit)
*R Cai,Y Lin,Y Wang,C Fu,X Zeng*

Main category: Zongheng Yang

TL;DR: GPA是一个统一的音频基础模型，将TTS、ASR、VC等多个语音任务集成到单个LLM架构中，使用共享的离散音频表示和统一的任务指令格式。


<details>
  <summary>Details</summary>
Motivation: 传统语音系统依赖独立的任务特定模型（TTS、ASR、VC），导致碎片化流程，限制了可扩展性、效率和跨任务泛化能力。

Method: 提出GPA（通用音频）模型，基于大型语言模型架构，使用共享的离散音频表示，通过统一的任务指令格式集成多个核心语音任务。

Result: GPA在多个语音任务上展示了强大的性能，实现了跨任务泛化，简化了语音处理流程，提高了效率和可扩展性。

Conclusion: GPA作为统一的音频基础模型，成功整合了多个语音任务，为构建更高效、可扩展的语音处理系统提供了新方向。

Abstract: Traditional speech systems typically rely on separate, task-specific models for text-to-speech (TTS), automatic speech recognition (ASR), and voice conversion (VC), resulting in fragmented pipelines that limit scalability, efficiency, and cross-task generalization. In this paper, we present General-Purpose Audio (GPA), a unified audio foundation model that integrates multiple core speech tasks within a single large language model (LLM) architecture. GPA operates on a shared discrete audio …

</details>


### [65] [The Anatomy of AI-Generated Disinformation from Deepfakes to Voice Cloning](https://scholar.google.com/scholar_url?url=https://link.springer.com/content/pdf/10.1007/978-981-95-4871-2.pdf%23page%3D125&hl=zh-CN&sa=X&d=12332161399726668932&ei=SHJyaZ6dEsm4ieoPof3esQc&scisig=AHkA5jRQs00hgqil3WN5NDaZrOJt&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=2&folt=cit)
*AS Ghai,S Jasola,RC Sharma,A de Bem Machado…*

Main category: Zongheng Yang

TL;DR: 本章探讨AI生成虚假信息的兴起，重点关注深度伪造和语音克隆技术，分析其技术原理、社会影响及应对策略。


<details>
  <summary>Details</summary>
Motivation: 人工智能在推动各行业发展的同时，也带来了AI生成虚假信息的新挑战。深度伪造和语音克隆技术的快速发展使得制造逼真虚假内容变得容易，这对社会信任、信息安全和个人隐私构成了严重威胁。本章旨在系统分析这一新兴威胁，为理解和管理AI生成虚假信息提供理论框架。

Method: 采用文献综述和案例分析的方法，重点研究深度伪造和语音克隆两种技术。分析这些技术的技术原理、生成机制、传播途径，以及它们在社会、政治、法律等领域的应用和影响。同时探讨检测技术和应对策略。

Result: AI生成的虚假信息（特别是深度伪造和语音克隆）具有高度逼真性和易传播性，能够有效欺骗人类感知系统。这些技术已被用于制造虚假新闻、政治宣传、身份欺诈等多种恶意用途，对社会信任体系造成严重破坏。同时，检测技术虽然有所发展，但仍面临技术挑战。

Conclusion: AI生成虚假信息是一个日益严重的全球性威胁，需要技术、法律、教育和社会治理等多方面的综合应对。未来需要加强技术检测能力、完善法律法规、提升公众媒体素养，并建立多方协作的治理框架来应对这一挑战。

Abstract: Artificial Intelligence has transformed multiple sectors, from healthcare and finance to entertainment and education, by enabling automation, improving efficiency, and enhancing decision-making. The advancements have also introduced challenges, particularly in AI-generated disinformation. This chapter explores the rise of AI-generated disinformation, focusing on two prominent technologies: deepfakes and voice cloning. Deepfakes manipulate visual and audio content, creating highly …

</details>


### [66] [RESCUE: Opportunistic Online Scheduling of Model Retraining on Underutilized Edges](https://scholar.google.com/scholar_url?url=https://fengshan.seu-netsi.net/papers/HLS-INFOCOM26.pdf&hl=zh-CN&sa=X&d=11179402278393287853&ei=SHJyaZ6dEsm4ieoPof3esQc&scisig=AHkA5jTlbsVC36hhp_tYkIdoywYC&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=3&folt=cit)
*J Huang,X Liu,F Shan*

Main category: Zongheng Yang

TL;DR: 利用边缘计算中未充分利用的资源进行AI模型重训练，避免与高优先级预留任务竞争专用服务器资源


<details>
  <summary>Details</summary>
Motivation: 边缘设备AI应用需要频繁的边缘辅助模型重训练来应对数据漂移，但边缘服务器资源紧张，高优先级预留任务加剧了资源竞争，而专用服务器成本高昂

Method: 识别并利用边缘计算中未充分利用的资源，这些资源源于预留任务的过度配置和碎片化

Result: 未在摘要中明确说明，但暗示该方法能有效利用现有资源进行模型重训练

Conclusion: 利用未充分利用的边缘计算资源是解决AI模型重训练资源需求的可行方案，避免了对昂贵专用服务器的投资

Abstract: The proliferation of Artificial Intelligence (AI) applications on edge devices requires frequent edge-assisted model retraining to mitigate data drift. This creates significant resource contention on edge servers, exacerbated by pre-committed highpriority reserved tasks. Instead of investing in costly dedicated servers, we identify a key and overlooked opportunity lying in Underutilized Edge Computing (UEC) resources, which arise from the over-provisioning and fragmentation of these reserved tasks …

</details>


### [67] [From Substitution to Complementarity: Leveraging BERT-VITS2 and Real Speech for Better Chinese Dysarthric Speech Recognition](https://scholar.google.com/scholar_url?url=https://www.researchsquare.com/article/rs-8505966/latest.pdf&hl=zh-CN&sa=X&d=9296264426672327408&ei=SHJyaZ6dEsm4ieoPof3esQc&scisig=AHkA5jQsBZ6Gc55VgM4P2uoABOJ-&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=4&folt=cit)
*K Feng,S Huang,M Li,K Qu*

Main category: Zongheng Yang

TL;DR: 首次将微调BERT-VITS2应用于中文构音障碍语音识别数据增强，提出结合合成与真实语音的互补训练策略，显著提升识别性能


<details>
  <summary>Details</summary>
Motivation: 中文构音障碍语音数据稀缺严重限制了DSR系统性能，需要有效的数据增强方法来克服数据不足问题

Method: 使用微调BERT-VITS2进行数据增强，提出结合合成与真实构音障碍语音的互补训练策略，包含时长调整、音高扰动和频谱增强等针对性修改

Result: 该方法显著提升了中文构音障碍语音识别性能，有效缓解了数据稀缺问题

Conclusion: BERT-VITS2数据增强与互补训练策略为中文构音障碍语音识别提供了有效解决方案，具有实际应用价值

Abstract: Abstract Dysarthric Speech Recognition (DSR) is essential for improving communication for individuals with dysarthria, yet the scarcity of Chinese dysarthric speech data significantly limits system performance. This paper presents the first application of fine-tuned BERT-VITS2 for data augmentation in Chinese DSR, proposing a complementary training strategy combining synthetic and authentic dysarthric speech. The framework incorporates targeted modifications:(1) duration …

</details>


### [68] [Voxia: A Virtual Voice Assistant](https://scholar.google.com/scholar_url?url=https://www.scitepress.org/Papers/2025/138854/138854.pdf&hl=zh-CN&sa=X&d=12554618970003800144&ei=SHJyaZ6dEsm4ieoPof3esQc&scisig=AHkA5jSV32miaWxBqqmxUfYXrAmK&oi=scholaralrt&hist=i6heNjgAAAAJ:289518505314468501:AHkA5jQJ09t5uyNUKDlpVNk33Ubd&html=&pos=5&folt=cit)
*FS Mahammad,CM Rani,AV Sree,GL Rani…*

Main category: Zongheng Yang

TL;DR: Voxia是一个基于AI的语音助手，集成了Whisper API、Hugging Face BERT和Google TTS/pyttsx3技术，为Web、移动和IoT设备提供准确即时的语音交互


<details>
  <summary>Details</summary>
Motivation: 开发一个能够准确理解用户意图并提供即时响应的语音助手，以改善用户与各种设备（Web、移动、IoT）的语音交互体验

Method: 采用模块化架构：使用Whisper API进行语音转文本，Hugging Face BERT进行意图识别，Google TTS/pyttsx3进行语音合成，整合为完整的语音交互系统

Result: 实现了准确且即时的语音交互功能，能够支持多种平台（Web、移动、IoT设备），提供流畅的用户体验

Conclusion: Voxia展示了将先进的AI技术（Whisper、BERT、TTS）集成到语音助手系统中的可行性，为跨平台语音交互提供了有效的解决方案

Abstract: Voxia is a voice assistant powered by artificial intelligence that facilitates interaction with users using speech recognition, natural language processing (NLP) and text-to-speech (TTS) technologies. With Whisper API for speech to text, Hugging Face BERT (Bidirectional Encoder Representations from Transformers) for intent recognition, and Google TTS/pyttsx3 for speech synthesis, Voxia propels accurate and instantaneous voice interactions for web, mobile, and IoT (internet of things) devices …

</details>


<div id='Ziniu Wu'></div>

# Ziniu Wu [[Back]](#toc)

### [69] [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.02076&hl=zh-CN&sa=X&d=1915482236404124982&ei=SXJyadiOGvDB6rQPsMjv6As&scisig=AHkA5jSIfa7a_J_3aEFHUin_0Jpv&oi=scholaralrt&hist=i6heNjgAAAAJ:8133205940682390297:AHkA5jQfTAaKkXi3kBQuJYouRHpx&html=&pos=1&folt=rel)
*Y Shu,Y Tian,C Xu,Y Wang,H Chen*

Main category: Ziniu Wu

TL;DR: 扩散语言模型作为自回归模型的替代方案，通过并行文本生成提升效率，但需要改进推理效率和KV缓存兼容性


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然能并行生成文本，但在推理效率和KV缓存兼容性方面存在不足，需要改进以更好地替代自回归模型

Method: 采用基于块的方法来改进推理效率和KV缓存兼容性

Result: 未提供具体结果数据，但暗示该方法能提升扩散语言模型的推理效率

Conclusion: 基于块的改进方法有助于提升扩散语言模型的推理效率和KV缓存兼容性，使其更实用

Abstract: Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based …

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [70] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 提出三种改进RT Core粒子FRNN物理模拟的方法：BVH更新/重建优化器、消除邻居列表的RT Core新用法、支持周期性边界条件的RT Core技术，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: RT Core在粒子物理模拟中已有应用，但仍有优化空间。需要解决BVH管理效率、邻居列表内存开销、周期性边界条件支持等问题，以进一步提升RT Core在FRNN模拟中的性能和适用性。

Method: 1) 实时BVH更新/重建比例优化器，根据模拟动态自适应调整；2) 两种消除邻居列表的RT Core变体；3) 支持周期性边界条件的RT Core技术。以Lennard-Jones FRNN相互作用模型为案例进行实验评估。

Result: BVH优化器使RT Core流水线比其他方法快约3.4倍；新变体在小半径时加速约1.3倍，在对数正态半径分布时加速约2.0倍；能模拟原本因邻居列表内存不足无法处理的案例；周期性边界条件技术无显著性能损失；方法在不同GPU代际上均能扩展性能和能效。

Conclusion: 提出的三种方法显著提升了RT Core在粒子FRNN物理模拟中的性能和能效，扩展了其适用场景，同时明确了RT Core与传统GPU计算的适用边界。

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>
